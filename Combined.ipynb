{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Anaconda\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports:\n",
    "import time;\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "id2word_wiki=gensim.corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking folder : ArtGeoHeaHisLitMatMusNatRelTec\n"
     ]
    }
   ],
   "source": [
    "# List Categories:\n",
    "#category_list = [\"Mathematics\",\"Technology\",\"Music\"]\n",
    "category_list=[\"Arts\", \"Geography\", \"Health\", \"History\", \"Literature\", \"Mathematics\", \"Music\", \"Nature\", \"Religion\", \"Technology\"]\n",
    "#category_list = [\"1977 introductions\", \"Language\" ,\"Arts\", \"Asia\", \"Asthma\", \"Automobiles\", \"BRICS nation\", \"Belief\", \"Climate\", \"Crime\", \"Culture\", \"Dance\", \"Deserts\", \"Dolls\", \"Earth\", \"Engines\", \"Fishing\", \"Folklore\", \"Games\", \"Geography\", \"Glass\", \"Government agencies\", \"Health\", \"History\", \"Humans\", \"Hygiene\", \"India\", \"Industry\", \"Internet\", \"Law\", \"Life\", \"Literature\", \"Mathematics\", \"Matter\", \"Millionaires\", \"Music\", \"Nature\", \"Nothing\", \"Parties\", \"Peace\", \"Politics\", \"Religion\", \"Sexology\", \"Society\", \"Songs\", \"Space\", \"Technology\", \"Television\", \"Transport\", \"Water sports\"]\n",
    "test_folder =  \"./test/\"\n",
    "root_folder = \"./Simplex/\"\n",
    "list.sort(category_list)\n",
    "folder_name=''.join([x[0]+x[1]+x[2] for x in category_list])\n",
    "root_folder='./'+folder_name+'/'\n",
    "\n",
    "print(\"Checking folder : \"+folder_name)\n",
    "if not os.path.exists(root_folder):\n",
    "    os.mkdir(root_folder)\n",
    "\n",
    "    \n",
    "\n",
    "wiki_bow_path = root_folder+'wiki_bow.mm'\n",
    "wiki_dict_path = root_folder+'wiki_dict.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning CSV\n"
     ]
    }
   ],
   "source": [
    "# Download Page Ids:\n",
    "#https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&categories=mathematics&doit=Do it!\n",
    "print (\"Scanning CSV\")\n",
    "for cat in category_list:\n",
    "    if not os.path.exists(root_folder+cat+\".csv\"):\n",
    "        url=\"https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=\"+cat\n",
    "        urllib.request.urlretrieve(url, root_folder+cat+\".csv\")\n",
    "        print(\"Downloading \",cat+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done downloading\n"
     ]
    }
   ],
   "source": [
    "# CSV to XML Download data:\n",
    "\n",
    "def get_data(ids,output_file):\n",
    "    url=\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=xml&pageids=\"+ids\n",
    "    req = urllib.request.urlopen(url)\n",
    "    if req.getcode() == 200:\n",
    "        soup = BeautifulSoup(req.read(), 'html.parser')\n",
    "        s = soup.find_all('page')\n",
    "        for si in s:\n",
    "            output_file.write(str(si))\n",
    "\n",
    "def batch_train(file):\n",
    "    output_file = open(file.replace(\".csv\",\".xml\"), 'a', encoding=\"utf8\")\n",
    "    output_file.write(\"<pages>\")\n",
    "                    \n",
    "    csvReader = csv.reader(open(file,'r'))\n",
    "    totalRecords = sum(1 for row in csv.reader(open(file,'r',encoding=\"UTF-8\")) )\n",
    "    print (file.replace(\".csv\",\".xml\"),totalRecords)\n",
    "    start = 0\n",
    "    end = start + 50\n",
    "   \n",
    "    while (start <= totalRecords):\n",
    "        pageIds = \"\"\n",
    "        for row in itertools.islice(csv.reader(open(file,'r',encoding=\"UTF-8\")),start,end):\n",
    "            pageIds = pageIds + row[2] + \"|\"\n",
    "        \n",
    "        get_data(pageIds,output_file)\n",
    "        start = end + 1\n",
    "        end = start + 50\n",
    "        if end> totalRecords:\n",
    "            end=totalRecords\n",
    "            \n",
    "    output_file.write(\"</pages>\")\n",
    "    \n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            if not os.path.exists(root_folder+file.replace(\".csv\",\".xml\")):\n",
    "                print(\"Downloading \",root_folder+file.replace(\".csv\",\".xml\"))\n",
    "                batch_train(path + file)\n",
    "\n",
    "print (\"Done downloading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading Dictionary object from ./ArtGeoHeaHisLitMatMusNatRelTec/wiki_dict.dict\n",
      "INFO : loaded ./ArtGeoHeaHisLitMatMusNatRelTec/wiki_dict.dict\n",
      "INFO : loaded corpus index from ./ArtGeoHeaHisLitMatMusNatRelTec/wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from ./ArtGeoHeaHisLitMatMusNatRelTec/wiki_bow.mm\n",
      "INFO : accepted corpus with 1968 documents, 37814 features, 419342 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Model\")\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "\n",
    "if not (os.path.exists(wiki_bow_path) and  os.path.exists(wiki_dict_path)):\n",
    "    for path, subdirs, files in os.walk(root_folder):\n",
    "        del subdirs[:]\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                doc_path = path + file\n",
    "                stream = iter_wiki(doc_path)\n",
    "                doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "                id2word_wiki.merge_with(gensim.corpora.Dictionary(doc_stream))\n",
    "\n",
    "    id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "    \n",
    "    # create a stream of bag-of-words vectors\n",
    "    wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "    \n",
    "    id2word_wiki.save(wiki_dict_path) \n",
    "    gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)\n",
    "print(len(category_list))\n",
    "id2word_wiki = gensim.corpora.Dictionary.load(wiki_dict_path)\n",
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using asymmetric alpha [0.20349776650601445, 0.15460680268266819, 0.12465746387131386, 0.10442834231175278, 0.089848016434062955, 0.078840302634700543, 0.070235422324486096, 0.063324036444942805, 0.05765099744942407, 0.052910849340634232]\n",
      "INFO : using symmetric eta at 2.6445231924683978e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 10 topics, 10 passes over the supplied corpus of 1968 documents, updating model once every 1968 documents, evaluating perplexity every 1968 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : -11.623 per-word bound, 3153.7 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 0, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.005*\"mobile\" + 0.003*\"digital\" + 0.003*\"engineering\" + 0.003*\"standards\" + 0.003*\"user\" + 0.002*\"users\" + 0.002*\"internet\" + 0.002*\"million\" + 0.002*\"metadata\" + 0.002*\"web\"\n",
      "INFO : topic #8 (0.058): 0.004*\"digital\" + 0.003*\"radar\" + 0.003*\"engineering\" + 0.003*\"software\" + 0.002*\"technical\" + 0.002*\"user\" + 0.002*\"mobile\" + 0.002*\"users\" + 0.002*\"communication\" + 0.002*\"internet\"\n",
      "INFO : topic #2 (0.125): 0.005*\"mobile\" + 0.003*\"engineering\" + 0.003*\"digital\" + 0.002*\"business\" + 0.002*\"zoom\" + 0.002*\"devices\" + 0.002*\"machine\" + 0.002*\"olympus\" + 0.002*\"caterpillar\" + 0.002*\"device\"\n",
      "INFO : topic #1 (0.155): 0.003*\"mobile\" + 0.003*\"technologies\" + 0.003*\"digital\" + 0.003*\"radar\" + 0.002*\"applications\" + 0.002*\"technological\" + 0.002*\"users\" + 0.002*\"engine\" + 0.002*\"internet\" + 0.002*\"user\"\n",
      "INFO : topic #0 (0.203): 0.004*\"mobile\" + 0.004*\"digital\" + 0.004*\"nuclear\" + 0.003*\"video\" + 0.003*\"observatory\" + 0.002*\"software\" + 0.002*\"web\" + 0.002*\"engineering\" + 0.002*\"technologies\" + 0.002*\"radar\"\n",
      "INFO : topic diff=4.503668, rho=1.000000\n",
      "INFO : -9.425 per-word bound, 687.5 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 1, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.006*\"mobile\" + 0.004*\"standards\" + 0.004*\"ontology\" + 0.004*\"users\" + 0.003*\"metadata\" + 0.003*\"user\" + 0.003*\"digital\" + 0.003*\"engineering\" + 0.003*\"internet\" + 0.002*\"web\"\n",
      "INFO : topic #8 (0.058): 0.006*\"radar\" + 0.004*\"engineering\" + 0.003*\"digital\" + 0.003*\"software\" + 0.003*\"technical\" + 0.002*\"user\" + 0.002*\"learning\" + 0.002*\"energy\" + 0.002*\"communication\" + 0.002*\"users\"\n",
      "INFO : topic #2 (0.125): 0.006*\"mobile\" + 0.003*\"zoom\" + 0.003*\"engineering\" + 0.003*\"business\" + 0.003*\"digital\" + 0.003*\"olympus\" + 0.002*\"devices\" + 0.002*\"card\" + 0.002*\"caterpillar\" + 0.002*\"location\"\n",
      "INFO : topic #1 (0.155): 0.004*\"technologies\" + 0.004*\"technological\" + 0.004*\"radar\" + 0.003*\"mobile\" + 0.003*\"digital\" + 0.002*\"innovation\" + 0.002*\"users\" + 0.002*\"applications\" + 0.002*\"internet\" + 0.002*\"product\"\n",
      "INFO : topic #0 (0.203): 0.006*\"mobile\" + 0.004*\"video\" + 0.004*\"digital\" + 0.003*\"software\" + 0.003*\"nuclear\" + 0.002*\"web\" + 0.002*\"tv\" + 0.002*\"radio\" + 0.002*\"iso\" + 0.002*\"devices\"\n",
      "INFO : topic diff=0.815681, rho=0.577350\n",
      "INFO : -9.189 per-word bound, 583.7 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 2, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.007*\"mobile\" + 0.005*\"standards\" + 0.005*\"ontology\" + 0.004*\"users\" + 0.004*\"metadata\" + 0.003*\"user\" + 0.003*\"web\" + 0.003*\"networking\" + 0.003*\"digital\" + 0.003*\"internet\"\n",
      "INFO : topic #8 (0.058): 0.008*\"radar\" + 0.004*\"engineering\" + 0.003*\"technical\" + 0.003*\"energy\" + 0.003*\"software\" + 0.003*\"materials\" + 0.003*\"user\" + 0.002*\"learning\" + 0.002*\"surface\" + 0.002*\"communication\"\n",
      "INFO : topic #2 (0.125): 0.006*\"mobile\" + 0.004*\"zoom\" + 0.003*\"business\" + 0.003*\"olympus\" + 0.003*\"digital\" + 0.003*\"signal\" + 0.003*\"engineering\" + 0.003*\"card\" + 0.003*\"location\" + 0.003*\"phone\"\n",
      "INFO : topic #1 (0.155): 0.006*\"technological\" + 0.005*\"technologies\" + 0.003*\"innovation\" + 0.002*\"digital\" + 0.002*\"product\" + 0.002*\"mobile\" + 0.002*\"slide\" + 0.002*\"industrial\" + 0.002*\"radar\" + 0.002*\"users\"\n",
      "INFO : topic #0 (0.203): 0.007*\"mobile\" + 0.005*\"video\" + 0.004*\"digital\" + 0.004*\"software\" + 0.003*\"tv\" + 0.003*\"lte\" + 0.003*\"devices\" + 0.002*\"radio\" + 0.002*\"web\" + 0.002*\"iso\"\n",
      "INFO : topic diff=0.653593, rho=0.500000\n",
      "INFO : -9.075 per-word bound, 539.4 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 3, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.006*\"mobile\" + 0.006*\"standards\" + 0.005*\"users\" + 0.005*\"ontology\" + 0.004*\"metadata\" + 0.004*\"user\" + 0.003*\"web\" + 0.003*\"networking\" + 0.003*\"internet\" + 0.003*\"software\"\n",
      "INFO : topic #8 (0.058): 0.009*\"radar\" + 0.005*\"engineering\" + 0.003*\"energy\" + 0.003*\"materials\" + 0.003*\"technical\" + 0.003*\"software\" + 0.003*\"user\" + 0.003*\"surface\" + 0.002*\"learning\" + 0.002*\"architecture\"\n",
      "INFO : topic #2 (0.125): 0.006*\"mobile\" + 0.004*\"zoom\" + 0.004*\"signal\" + 0.004*\"olympus\" + 0.003*\"business\" + 0.003*\"digital\" + 0.003*\"card\" + 0.003*\"location\" + 0.003*\"phone\" + 0.003*\"car\"\n",
      "INFO : topic #1 (0.155): 0.007*\"technological\" + 0.006*\"technologies\" + 0.004*\"innovation\" + 0.002*\"revolution\" + 0.002*\"industrial\" + 0.002*\"product\" + 0.002*\"digital\" + 0.002*\"slide\" + 0.002*\"economic\" + 0.002*\"rule\"\n",
      "INFO : topic #0 (0.203): 0.009*\"mobile\" + 0.006*\"video\" + 0.004*\"software\" + 0.004*\"digital\" + 0.004*\"lte\" + 0.003*\"tv\" + 0.003*\"devices\" + 0.003*\"radio\" + 0.003*\"iso\" + 0.002*\"wireless\"\n",
      "INFO : topic diff=0.505851, rho=0.447214\n",
      "INFO : -9.013 per-word bound, 516.8 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 4, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.007*\"standards\" + 0.006*\"mobile\" + 0.006*\"users\" + 0.005*\"ontology\" + 0.005*\"metadata\" + 0.004*\"web\" + 0.004*\"user\" + 0.003*\"networking\" + 0.003*\"software\" + 0.003*\"internet\"\n",
      "INFO : topic #8 (0.058): 0.010*\"radar\" + 0.006*\"engineering\" + 0.003*\"materials\" + 0.003*\"energy\" + 0.003*\"surface\" + 0.003*\"user\" + 0.003*\"software\" + 0.003*\"technical\" + 0.002*\"architecture\" + 0.002*\"engineers\"\n",
      "INFO : topic #2 (0.125): 0.006*\"mobile\" + 0.004*\"zoom\" + 0.004*\"signal\" + 0.004*\"olympus\" + 0.003*\"business\" + 0.003*\"card\" + 0.003*\"digital\" + 0.003*\"car\" + 0.003*\"location\" + 0.003*\"phone\"\n",
      "INFO : topic #1 (0.155): 0.009*\"technological\" + 0.006*\"technologies\" + 0.004*\"innovation\" + 0.003*\"revolution\" + 0.003*\"industrial\" + 0.002*\"economic\" + 0.002*\"product\" + 0.002*\"slide\" + 0.002*\"digital\" + 0.002*\"business\"\n",
      "INFO : topic #0 (0.203): 0.010*\"mobile\" + 0.006*\"video\" + 0.005*\"software\" + 0.004*\"lte\" + 0.004*\"digital\" + 0.003*\"devices\" + 0.003*\"tv\" + 0.003*\"radio\" + 0.003*\"wireless\" + 0.003*\"device\"\n",
      "INFO : topic diff=0.394805, rho=0.408248\n",
      "INFO : -8.974 per-word bound, 502.9 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 5, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.007*\"standards\" + 0.006*\"users\" + 0.006*\"mobile\" + 0.005*\"ontology\" + 0.005*\"metadata\" + 0.004*\"web\" + 0.004*\"user\" + 0.003*\"networking\" + 0.003*\"software\" + 0.003*\"content\"\n",
      "INFO : topic #8 (0.058): 0.011*\"radar\" + 0.007*\"engineering\" + 0.003*\"materials\" + 0.003*\"energy\" + 0.003*\"surface\" + 0.003*\"user\" + 0.003*\"software\" + 0.003*\"engineers\" + 0.003*\"technical\" + 0.003*\"architecture\"\n",
      "INFO : topic #2 (0.125): 0.006*\"mobile\" + 0.005*\"signal\" + 0.005*\"zoom\" + 0.004*\"olympus\" + 0.004*\"card\" + 0.004*\"car\" + 0.003*\"business\" + 0.003*\"digital\" + 0.003*\"location\" + 0.003*\"phone\"\n",
      "INFO : topic #1 (0.155): 0.009*\"technological\" + 0.006*\"technologies\" + 0.004*\"innovation\" + 0.003*\"revolution\" + 0.003*\"industrial\" + 0.003*\"economic\" + 0.002*\"product\" + 0.002*\"slide\" + 0.002*\"business\" + 0.002*\"artificial\"\n",
      "INFO : topic #0 (0.203): 0.011*\"mobile\" + 0.007*\"video\" + 0.005*\"software\" + 0.004*\"lte\" + 0.004*\"devices\" + 0.004*\"digital\" + 0.003*\"tv\" + 0.003*\"device\" + 0.003*\"wireless\" + 0.003*\"radio\"\n",
      "INFO : topic diff=0.310934, rho=0.377964\n",
      "INFO : -8.947 per-word bound, 493.4 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 6, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.007*\"standards\" + 0.006*\"users\" + 0.005*\"ontology\" + 0.005*\"mobile\" + 0.005*\"metadata\" + 0.005*\"web\" + 0.004*\"user\" + 0.004*\"content\" + 0.004*\"software\" + 0.004*\"iso\"\n",
      "INFO : topic #8 (0.058): 0.011*\"radar\" + 0.008*\"engineering\" + 0.004*\"materials\" + 0.004*\"energy\" + 0.003*\"surface\" + 0.003*\"engineers\" + 0.003*\"user\" + 0.003*\"software\" + 0.003*\"architecture\" + 0.002*\"technical\"\n",
      "INFO : topic #2 (0.125): 0.005*\"mobile\" + 0.005*\"signal\" + 0.005*\"zoom\" + 0.004*\"olympus\" + 0.004*\"car\" + 0.004*\"card\" + 0.003*\"digital\" + 0.003*\"business\" + 0.003*\"location\" + 0.003*\"sound\"\n",
      "INFO : topic #1 (0.155): 0.010*\"technological\" + 0.006*\"technologies\" + 0.005*\"innovation\" + 0.004*\"revolution\" + 0.003*\"industrial\" + 0.003*\"economic\" + 0.002*\"product\" + 0.002*\"artificial\" + 0.002*\"slide\" + 0.002*\"business\"\n",
      "INFO : topic #0 (0.203): 0.012*\"mobile\" + 0.007*\"video\" + 0.005*\"software\" + 0.004*\"lte\" + 0.004*\"devices\" + 0.004*\"digital\" + 0.003*\"tv\" + 0.003*\"device\" + 0.003*\"wireless\" + 0.003*\"phone\"\n",
      "INFO : topic diff=0.247669, rho=0.353553\n",
      "INFO : -8.926 per-word bound, 486.4 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 7, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.008*\"standards\" + 0.007*\"users\" + 0.005*\"ontology\" + 0.005*\"web\" + 0.005*\"mobile\" + 0.005*\"metadata\" + 0.004*\"user\" + 0.004*\"content\" + 0.004*\"software\" + 0.004*\"iso\"\n",
      "INFO : topic #8 (0.058): 0.011*\"radar\" + 0.009*\"engineering\" + 0.004*\"materials\" + 0.004*\"energy\" + 0.003*\"surface\" + 0.003*\"engineers\" + 0.003*\"architecture\" + 0.003*\"user\" + 0.003*\"software\" + 0.002*\"mass\"\n",
      "INFO : topic #2 (0.125): 0.005*\"signal\" + 0.005*\"zoom\" + 0.005*\"mobile\" + 0.004*\"olympus\" + 0.004*\"car\" + 0.004*\"card\" + 0.004*\"digital\" + 0.003*\"sound\" + 0.003*\"location\" + 0.003*\"steam\"\n",
      "INFO : topic #1 (0.155): 0.010*\"technological\" + 0.007*\"technologies\" + 0.005*\"innovation\" + 0.004*\"revolution\" + 0.003*\"industrial\" + 0.003*\"economic\" + 0.002*\"product\" + 0.002*\"artificial\" + 0.002*\"business\" + 0.002*\"slide\"\n",
      "INFO : topic #0 (0.203): 0.013*\"mobile\" + 0.007*\"video\" + 0.006*\"software\" + 0.004*\"lte\" + 0.004*\"devices\" + 0.004*\"digital\" + 0.003*\"tv\" + 0.003*\"device\" + 0.003*\"wireless\" + 0.003*\"phone\"\n",
      "INFO : topic diff=0.199778, rho=0.333333\n",
      "INFO : -8.910 per-word bound, 481.0 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 8, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.008*\"standards\" + 0.007*\"users\" + 0.005*\"web\" + 0.005*\"ontology\" + 0.005*\"metadata\" + 0.004*\"user\" + 0.004*\"mobile\" + 0.004*\"content\" + 0.004*\"iso\" + 0.004*\"software\"\n",
      "INFO : topic #8 (0.058): 0.011*\"radar\" + 0.010*\"engineering\" + 0.004*\"materials\" + 0.004*\"energy\" + 0.003*\"surface\" + 0.003*\"engineers\" + 0.003*\"architecture\" + 0.003*\"user\" + 0.003*\"software\" + 0.002*\"force\"\n",
      "INFO : topic #2 (0.125): 0.006*\"signal\" + 0.005*\"zoom\" + 0.005*\"mobile\" + 0.004*\"olympus\" + 0.004*\"car\" + 0.004*\"card\" + 0.004*\"sound\" + 0.004*\"digital\" + 0.003*\"steam\" + 0.003*\"location\"\n",
      "INFO : topic #1 (0.155): 0.011*\"technological\" + 0.007*\"technologies\" + 0.005*\"innovation\" + 0.004*\"revolution\" + 0.004*\"industrial\" + 0.003*\"economic\" + 0.002*\"product\" + 0.002*\"artificial\" + 0.002*\"business\" + 0.002*\"market\"\n",
      "INFO : topic #0 (0.203): 0.013*\"mobile\" + 0.007*\"video\" + 0.006*\"software\" + 0.004*\"devices\" + 0.004*\"lte\" + 0.004*\"digital\" + 0.004*\"device\" + 0.003*\"tv\" + 0.003*\"wireless\" + 0.003*\"phone\"\n",
      "INFO : topic diff=0.163444, rho=0.316228\n",
      "INFO : -8.897 per-word bound, 476.6 perplexity estimate based on a held-out corpus of 1968 documents with 804398 words\n",
      "INFO : PROGRESS: pass 9, at document #1968/1968\n",
      "INFO : topic #9 (0.053): 0.008*\"standards\" + 0.007*\"users\" + 0.005*\"web\" + 0.005*\"ontology\" + 0.005*\"content\" + 0.005*\"metadata\" + 0.005*\"user\" + 0.005*\"iso\" + 0.004*\"software\" + 0.004*\"mobile\"\n",
      "INFO : topic #8 (0.058): 0.012*\"radar\" + 0.010*\"engineering\" + 0.004*\"materials\" + 0.004*\"energy\" + 0.003*\"surface\" + 0.003*\"engineers\" + 0.003*\"architecture\" + 0.003*\"force\" + 0.003*\"software\" + 0.003*\"user\"\n",
      "INFO : topic #2 (0.125): 0.006*\"signal\" + 0.005*\"zoom\" + 0.004*\"olympus\" + 0.004*\"car\" + 0.004*\"mobile\" + 0.004*\"card\" + 0.004*\"sound\" + 0.004*\"digital\" + 0.003*\"windows\" + 0.003*\"steam\"\n",
      "INFO : topic #1 (0.155): 0.011*\"technological\" + 0.007*\"technologies\" + 0.005*\"innovation\" + 0.004*\"revolution\" + 0.004*\"industrial\" + 0.003*\"economic\" + 0.002*\"artificial\" + 0.002*\"product\" + 0.002*\"business\" + 0.002*\"market\"\n",
      "INFO : topic #0 (0.203): 0.014*\"mobile\" + 0.007*\"video\" + 0.006*\"software\" + 0.005*\"devices\" + 0.004*\"lte\" + 0.004*\"digital\" + 0.004*\"device\" + 0.004*\"tv\" + 0.003*\"wireless\" + 0.003*\"phone\"\n",
      "INFO : topic diff=0.135704, rho=0.301511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=len(category_list), id2word=id2word_wiki, passes=10, alpha='asymmetric')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : calculating IDF weights for 1968 documents and 37813 features (419342 matrix non-zeros)\n",
      "INFO : using serial LSI version on this node\n",
      "INFO : updating model with new documents\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (37814, 110) action matrix\n",
      "INFO : orthonormalizing (37814, 110) action matrix\n",
      "INFO : 2nd phase: running dense svd on (110, 1968) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 10 factors (discarding 74.579% of energy spectrum)\n",
      "INFO : processed documents up to #1968\n",
      "INFO : topic #0(5.167): -0.289*\"mobile\" + -0.121*\"engineering\" + -0.116*\"software\" + -0.106*\"digital\" + -0.105*\"user\" + -0.096*\"users\" + -0.095*\"internet\" + -0.087*\"web\" + -0.085*\"learning\" + -0.084*\"technologies\"\n",
      "INFO : topic #1(3.487): 0.576*\"mobile\" + -0.261*\"engineering\" + 0.143*\"phone\" + -0.115*\"technological\" + -0.110*\"energy\" + 0.109*\"phones\" + 0.108*\"wireless\" + 0.097*\"location\" + 0.094*\"sms\" + -0.088*\"industrial\"\n",
      "INFO : topic #2(3.038): 0.163*\"technical\" + 0.158*\"content\" + 0.153*\"document\" + 0.149*\"documentation\" + 0.143*\"learning\" + -0.137*\"energy\" + 0.133*\"web\" + 0.131*\"xml\" + -0.125*\"signal\" + 0.122*\"software\"\n",
      "INFO : topic #3(2.879): -0.243*\"mobile\" + -0.203*\"engineering\" + -0.166*\"technological\" + 0.158*\"iso\" + 0.142*\"xml\" + 0.135*\"document\" + 0.121*\"format\" + -0.120*\"innovation\" + 0.109*\"documentation\" + 0.109*\"message\"\n",
      "INFO : topic #4(2.721): -0.601*\"engineering\" + 0.215*\"digital\" + -0.174*\"mobile\" + -0.143*\"iso\" + -0.133*\"engineers\" + 0.119*\"internet\" + 0.108*\"video\" + -0.105*\"standards\" + -0.098*\"technical\" + 0.095*\"learning\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)\n",
    "lsi_model = lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki, num_topics=len(category_list))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcuating centroid\n",
      "Centroid calculation done\n"
     ]
    }
   ],
   "source": [
    "def calculate_centroid(topic_docs):\n",
    "    test_doc = [tokens for tokens in iter_wiki(topic_docs)]\n",
    "    part = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    \n",
    "    topic_dic={}\n",
    "    \n",
    "    for i in range(len(category_list)):\n",
    "        topic_dic[i]=0\n",
    "        \n",
    "    for doc in part:\n",
    "        for p in doc:\n",
    "            topic_dic[p[0]] += p[1]\n",
    "    \n",
    "    centroid = [(x, topic_dic[x]/len(part)) for x in range(len(category_list))]\n",
    "    return centroid\n",
    "\n",
    "print(\"Calcuating centroid\")\n",
    "\n",
    "centroids_dict={}\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            centroid = calculate_centroid(doc_path)\n",
    "            centroids_dict[file.replace(\".xml\",\"\")]=centroid\n",
    "\n",
    "print(\"Centroid calculation done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating KMeans \n"
     ]
    }
   ],
   "source": [
    "km=None\n",
    "def get_values(l):\n",
    "    g=[0]*len(category_list)\n",
    "    for i in l:\n",
    "        g[i[0]]=i[1]\n",
    "    return g\n",
    "\n",
    "def calculate_kmeans(part):\n",
    "    km = KMeans(n_clusters=len(category_list), init='k-means++', max_iter=500, n_init=1)\n",
    "    km.fit(part)\n",
    "    return km\n",
    "\n",
    "print(\"Calculating KMeans \")\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    part=[]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            test_doc = [tokens for tokens in iter_wiki(doc_path)]\n",
    "            part+=[get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    km = calculate_kmeans(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding data to KNeighborsClassifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=150, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=[]\n",
    "X=[]\n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            topic = file.replace(\".xml\",\"\")\n",
    "            \n",
    "            test_doc = [tokens for tokens in iter_wiki(doc_path)]\n",
    "            X+=[get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "            y+=[category_list.index(topic)]*len(test_doc)\n",
    "\n",
    "print(\"Feeding data to KNeighborsClassifier\")\n",
    "n_neighbors = 15\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feeding data to SVC\n"
     ]
    }
   ],
   "source": [
    "print(\"Feeding data to SVC\")\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_graph(x_label,y,file,text_data,topic):\n",
    "    x = np.arange(len(x_label))  # the x locations for the groups\n",
    "    width = 0.1       # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(8)\n",
    "    rects = ax.bar(x, y, width, color='blue')\n",
    "    ax.set_ylabel('Probabilities')\n",
    "    ax.set_xlabel('Categories')\n",
    "    ax.set_title('Topic Distribution of: ' + topic)\n",
    "    ax.title.set_position([.5, 1.2])\n",
    "    ax.set_ylim([0,1])\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        \"\"\"\n",
    "        Attach a text label above each bar displaying its height\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() - rect.get_width()*2, 1.05 * height,round(height,2))\n",
    "    \n",
    "\n",
    "    autolabel(rects)\n",
    "    plt.tight_layout(pad=6)\n",
    "    plt.xticks(x,category_list,rotation=90)\n",
    "    \n",
    "    plt.savefig(file.replace(\".xml\",\"_\")+str(len(category_list))+'.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results\n",
      "Allegory.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.95\n",
      "0.88\n",
      "0.78\n",
      "0.92\n",
      "0.85\n",
      "0.54\n",
      "0.79\n",
      "0.84\n",
      "0.87\n",
      "0.57\n",
      "Android.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.22\n",
      "0.26\n",
      "0.42\n",
      "0.17\n",
      "0.19\n",
      "0.13\n",
      "0.3\n",
      "0.12\n",
      "0.15\n",
      "0.66\n",
      "Artificial Intelligence.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.21\n",
      "0.39\n",
      "0.23\n",
      "0.16\n",
      "0.23\n",
      "0.82\n",
      "0.21\n",
      "0.56\n",
      "0.17\n",
      "0.39\n",
      "Cartoon Network.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.65\n",
      "0.66\n",
      "0.67\n",
      "0.58\n",
      "0.54\n",
      "0.31\n",
      "0.58\n",
      "0.49\n",
      "0.51\n",
      "0.76\n",
      "Computer Network.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.22\n",
      "0.32\n",
      "0.39\n",
      "0.18\n",
      "0.2\n",
      "0.26\n",
      "0.31\n",
      "0.22\n",
      "0.15\n",
      "0.7\n",
      "Computer Science.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.53\n",
      "0.66\n",
      "0.62\n",
      "0.47\n",
      "0.58\n",
      "0.93\n",
      "0.54\n",
      "0.77\n",
      "0.47\n",
      "0.68\n",
      "Dijkstra's Algorithm.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.36\n",
      "0.52\n",
      "0.37\n",
      "0.29\n",
      "0.37\n",
      "0.88\n",
      "0.42\n",
      "0.7\n",
      "0.3\n",
      "0.56\n",
      "Ebola.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.83\n",
      "0.79\n",
      "0.93\n",
      "0.81\n",
      "0.82\n",
      "0.5\n",
      "0.82\n",
      "0.72\n",
      "0.83\n",
      "0.82\n",
      "Fibonacci Heap.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.45\n",
      "0.48\n",
      "0.43\n",
      "0.34\n",
      "0.42\n",
      "0.58\n",
      "0.48\n",
      "0.66\n",
      "0.36\n",
      "0.5\n",
      "Google.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.41\n",
      "0.46\n",
      "0.67\n",
      "0.41\n",
      "0.4\n",
      "0.25\n",
      "0.45\n",
      "0.3\n",
      "0.41\n",
      "0.77\n",
      "Larry Page.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.72\n",
      "0.71\n",
      "0.91\n",
      "0.73\n",
      "0.75\n",
      "0.48\n",
      "0.72\n",
      "0.6\n",
      "0.75\n",
      "0.82\n",
      "USA.xml\n",
      "[[0.040777338125449328, 0.19818742955168864, 0.022553908759205259, 0.051970215828235709, 0.0512840412000138, 0.3384683595989586, 0.15898905286120468, 0.015435762085985362, 0, 0.11463976746898566]]\n",
      "0.94\n",
      "0.86\n",
      "0.87\n",
      "0.99\n",
      "0.92\n",
      "0.46\n",
      "0.8\n",
      "0.71\n",
      "0.93\n",
      "0.6\n",
      " Done \n"
     ]
    }
   ],
   "source": [
    "# Test LDA - Cosine\n",
    "model_name=\"LDA-Cosine\"\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "    \n",
    "def get_part(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "results=\"\"\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            path=get_part(doc_path)\n",
    "            print(file)\n",
    "            results+=file.replace(\".xml\",\"\")+\"\\n\\n\"\n",
    "            graph_data=[]\n",
    "            text_data=\"\"\n",
    "            for topic in category_list:\n",
    "                cos_dis=np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroids_dict[topic]], path)])\n",
    "                text_data +=topic+\":\"+str(cos_dis)+\"\\n\"\n",
    "                graph_data.append(cos_dis)\n",
    "            results+=text_data+\"\\n\\n\\n\\n\"\n",
    "            for g in graph_data:\n",
    "                print (round(g,2))\n",
    "                \n",
    "            draw_graph(list(centroids_dict.keys()),graph_data,root_folder+model_name+'/'+file,text_data,file)\n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results\n",
      "Geography\n",
      "Nature\n",
      "Arts\n",
      "Nature\n",
      "Nature\n",
      "Arts\n",
      "Arts\n",
      "Health\n",
      "Mathematics\n",
      "History\n",
      "History\n",
      "Health\n",
      " Done \n"
     ]
    }
   ],
   "source": [
    "# Test LDA- KMeans\n",
    "model_name=\"LDA-KMeans\"\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "    \n",
    "def get_part(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "results=\"\"\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            results+=file.replace(\".xml\",\"\")+\":  \"\n",
    "            part=get_part(doc_path)\n",
    "            print(category_list[km.predict(part)[0]])\n",
    "            results+=category_list[km.predict(part)[0]]\n",
    "            results += \"\\n\\n\"\n",
    "                \n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results\n",
      "Religion\n",
      "Literature\n",
      "Technology\n",
      "Technology\n",
      "Health\n",
      "Technology\n",
      "Mathematics\n",
      "Mathematics\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      " Done \n"
     ]
    }
   ],
   "source": [
    "#Test LDA _ KNN\n",
    "model_name=\"LDA-KNN\"\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "\n",
    "def get_part(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "results=\"\"\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            results+=file.replace(\".xml\",\"\")+\":  \"\n",
    "            print(category_list[clf.predict(part)[0]])\n",
    "            part=get_part(doc_path)\n",
    "            results+=category_list[clf.predict(part)[0]]\n",
    "            results += \"\\n\\n\"\n",
    "                \n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      "Health\n",
      " Done \n"
     ]
    }
   ],
   "source": [
    "#LDA Linear Classifier\n",
    "model_name=\"LDA-Linear Classifier\"\n",
    "\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "    \n",
    "def get_part(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            results+=file.replace(\".xml\",\"\")+\":  \"\n",
    "            \n",
    "            part=get_part(doc_path)\n",
    "            #for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "            print(category_list[poly_svc.predict(part)[0]])\n",
    "            #results +=category_list[clf.predict(part)[0]] + \"\\n\\n\"\n",
    "                \n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcuating centroid\n",
      "Centroid calculation done\n",
      "Testing results\n",
      "./test/Allegory.xml\n",
      "-0.12\n",
      "0.01\n",
      "-0.28\n",
      "-0.14\n",
      "-0.21\n",
      "0.15\n",
      "-0.3\n",
      "0.11\n",
      "-0.19\n",
      "-0.29\n",
      "./test/Android.xml\n",
      "0.18\n",
      "0.05\n",
      "0.04\n",
      "0.26\n",
      "0.21\n",
      "0.1\n",
      "0.19\n",
      "0.25\n",
      "0.38\n",
      "-0.25\n",
      "./test/Artificial Intelligence.xml\n",
      "-0.24\n",
      "-0.15\n",
      "-0.31\n",
      "-0.25\n",
      "-0.31\n",
      "-0.03\n",
      "-0.43\n",
      "-0.06\n",
      "-0.29\n",
      "-0.35\n",
      "./test/Cartoon Network.xml\n",
      "-0.21\n",
      "-0.26\n",
      "-0.36\n",
      "-0.16\n",
      "-0.2\n",
      "-0.19\n",
      "-0.24\n",
      "-0.07\n",
      "-0.11\n",
      "-0.52\n",
      "./test/Computer Network.xml\n",
      "-0.19\n",
      "-0.26\n",
      "-0.24\n",
      "-0.16\n",
      "-0.06\n",
      "-0.01\n",
      "-0.1\n",
      "-0.03\n",
      "-0.06\n",
      "-0.44\n",
      "./test/Computer Science.xml\n",
      "-0.54\n",
      "-0.47\n",
      "-0.68\n",
      "-0.53\n",
      "-0.57\n",
      "-0.25\n",
      "-0.59\n",
      "-0.38\n",
      "-0.52\n",
      "-0.7\n",
      "./test/Dijkstra's Algorithm.xml\n",
      "-0.14\n",
      "-0.08\n",
      "-0.21\n",
      "-0.16\n",
      "-0.14\n",
      "0.17\n",
      "-0.21\n",
      "0.12\n",
      "-0.13\n",
      "-0.3\n",
      "./test/Ebola.xml\n",
      "-0.2\n",
      "-0.17\n",
      "-0.33\n",
      "-0.15\n",
      "-0.2\n",
      "-0.09\n",
      "-0.4\n",
      "-0.11\n",
      "-0.2\n",
      "-0.52\n",
      "./test/Fibonacci Heap.xml\n",
      "-0.14\n",
      "-0.08\n",
      "-0.28\n",
      "-0.17\n",
      "-0.19\n",
      "0.07\n",
      "-0.3\n",
      "0.05\n",
      "-0.2\n",
      "-0.4\n",
      "./test/Google.xml\n",
      "0.06\n",
      "-0.02\n",
      "-0.15\n",
      "0.16\n",
      "0.07\n",
      "-0.05\n",
      "-0.02\n",
      "0.06\n",
      "0.19\n",
      "-0.43\n",
      "./test/Larry Page.xml\n",
      "0.04\n",
      "0.04\n",
      "-0.23\n",
      "0.09\n",
      "-0.03\n",
      "-0.01\n",
      "-0.1\n",
      "0.09\n",
      "0.06\n",
      "-0.4\n",
      "./test/USA.xml\n",
      "-0.2\n",
      "-0.18\n",
      "-0.37\n",
      "-0.11\n",
      "-0.22\n",
      "-0.28\n",
      "-0.41\n",
      "-0.28\n",
      "-0.21\n",
      "-0.55\n",
      " Done \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_centroid1(topic_docs):\n",
    "    test_doc = [tokens for tokens in iter_wiki(topic_docs)]\n",
    "    part = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    \n",
    "    topic_dic={}\n",
    "    \n",
    "    for i in range(len(category_list)):\n",
    "        topic_dic[i]=0\n",
    "        \n",
    "    for doc in part:\n",
    "        for p in doc:\n",
    "            topic_dic[p[0]] += p[1]\n",
    "    \n",
    "    centroid = [(x, topic_dic[x]/len(part)) for x in range(len(category_list))]\n",
    "    return centroid\n",
    "\n",
    "print(\"Calcuating centroid\")\n",
    "\n",
    "centroids_dict1={}\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            centroid = calculate_centroid1(doc_path)\n",
    "            centroids_dict1[file.replace(\".xml\",\"\")]=centroid\n",
    "\n",
    "print(\"Centroid calculation done\")\n",
    "model_name=\"LSI-Cosine\"\n",
    "\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "    \n",
    "# Test \n",
    "def get_part1(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "results=\"\"\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            path=get_part1(doc_path)\n",
    "            print(doc_path)\n",
    "            results+=file.replace(\".xml\",\"\")+\"\\n\\n\"\n",
    "            graph_data=[]\n",
    "            text_data=\"\"\n",
    "            graph_topic=[]\n",
    "            for topic in category_list:\n",
    "                \n",
    "                cos_dis=np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroids_dict[topic]], path)])\n",
    "                text_data +=  topic+\":\"+str(cos_dis)+\"\\n\"\n",
    "                #if(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroids_dict11[topic]], path)])>0):\n",
    "                graph_data.append(cos_dis)\n",
    "                graph_topic.append(topic)\n",
    "            for g in graph_data:\n",
    "                print (round(g,2))\n",
    "            results+=text_data+\"\\n\\n\\n\\n\"\n",
    "            draw_graph(graph_topic,graph_data,root_folder+model_name+'/'+file,text_data,file)\n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd\n",
      "Allegory\n",
      "Android\n",
      "Artificial Intelligence\n",
      "Cartoon Network\n",
      "Computer Network\n",
      "Computer Science\n",
      "Dijkstra's Algorithm\n",
      "Ebola\n",
      "Fibonacci Heap\n",
      "Google\n",
      "Larry Page\n",
      "USA\n"
     ]
    }
   ],
   "source": [
    "print(\"asd\")\n",
    "t=0\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            #print(root_folder+file)\n",
    "            \n",
    "            csvReader = csv.reader(open(test_folder+file,'r'))\n",
    "            #totalRecords = sum(1 for row in csv.reader(open(root_folder+file,'r',encoding=\"UTF-8\")) )\n",
    "            #t+=totalRecords\n",
    "            print(file.replace(\".xml\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
