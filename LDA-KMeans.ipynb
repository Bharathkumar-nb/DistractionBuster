{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "from sklearn.cluster import KMeans\n",
    "km=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# List Categories:\n",
    "category_list = [\"Mathematics\",\"Technology\",\"Music\",\"History\",\"Geography\",\"Arts\",\"Health\",\"Nature\",\"Religion\",\"Literature\"]\n",
    "testFolder =  \"./Simplex1/test/\"\n",
    "root_folder = \"./Simplex1/\"\n",
    "list.sort(category_list)\n",
    "root_folder='./'+''.join([x[0] for x in category_list])+'/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.mkdir(root_folder)\n",
    "\n",
    "wiki_bow_path = root_folder+'wiki_bow.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Arts\n",
      "Arts.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Geography\n",
      "Geography.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Health\n",
      "Health.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=History\n",
      "History.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Literature\n",
      "Literature.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Mathematics\n",
      "Mathematics.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Music\n",
      "Music.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Nature\n",
      "Nature.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Religion\n",
      "Religion.csv\n",
      "https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=Technology\n",
      "Technology.csv\n"
     ]
    }
   ],
   "source": [
    "# Download Page Ids:\n",
    "#https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&categories=mathematics&doit=Do it!\n",
    "for cat in category_list:\n",
    "    url=\"https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=\"+cat\n",
    "    print(url)\n",
    "    urllib.request.urlretrieve(url, root_folder+cat+\".csv\")\n",
    "\n",
    "    print(cat+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# CSV to XML Download data:\n",
    "\n",
    "def getData(ids,outputFile):\n",
    "    url=\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=xml&pageids=\"+ids\n",
    "    req = urllib.request.urlopen(url)\n",
    "    if req.getcode() == 200:\n",
    "        soup = BeautifulSoup(req.read(), 'html.parser')\n",
    "        s = soup.find_all('page')\n",
    "        for si in s:\n",
    "            outputFile.write(str(si))\n",
    "\n",
    "def batchTrain(file):\n",
    "    outputFile = open(file.replace(\".csv\",\".xml\"), 'a', encoding=\"utf8\")\n",
    "    outputFile.write(\"<pages>\")\n",
    "                    \n",
    "    csvReader = csv.reader(open(file,'r'))\n",
    "    totalRecords = sum(1 for row in csv.reader(open(file,'r',encoding=\"UTF-8\")) )\n",
    "    print (totalRecords)\n",
    "    start = 0\n",
    "    end = start + 50\n",
    "   \n",
    "    while (start <= totalRecords):\n",
    "        pageIds = \"\"\n",
    "        for row in itertools.islice(csv.reader(open(file,'r',encoding=\"UTF-8\")),start,end):\n",
    "            pageIds = pageIds + row[2] + \"|\"\n",
    "        \n",
    "        getData(pageIds,outputFile)\n",
    "        start = end + 1\n",
    "        end = start + 50\n",
    "        if end> totalRecords:\n",
    "            end=totalRecords\n",
    "            \n",
    "    outputFile.write(\"</pages>\")\n",
    "    \n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            if not os.path.exists(root_folder+file.replace(\".csv\",\".xml\")):\n",
    "                batchTrain(path + file)\n",
    "\n",
    "print (\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(id2word_wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AGHHLMMNRT/Arts.xml\n",
      "['clockwise', 'upper', 'left', 'self', 'portrait', 'vincent', 'van', 'gogh', 'female', 'ancestor']\n",
      "['johann', 'wolfgang', 'von', 'goethe', 'german', 'artist', 'known', 'works', 'poetry', 'drama']\n",
      "['art', 'deco', 'simply', 'referred', 'deco', 'style', 'visual', 'arts', 'architecture', 'design']\n",
      "['aesthetics', 'spelled', 'æsthetics', 'esthetics', 'known', 'greek', 'αισθητική', 'aisthētiké', 'branch', 'philosophy']\n",
      "['bauhaus', 'dessau', 'walter', 'gropius', 'expressionist', 'monument', 'march', 'deadtypography', 'herbert', 'bayer']\n",
      "['triumph', 'immaculate', 'paolo', 'matteis', 'church', 'sant', 'andrea', 'al', 'quirinale', 'designed']\n",
      "['example', 'modern', 'cartoon', 'text', 'excerpted', 'cartoonist', 'greg', 'williams', 'wikipedia', 'article']\n",
      "['crochet', 'process', 'creating', 'fabric', 'interlocking', 'loops', 'yarn', 'thread', 'strands', 'materials']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(76020 unique tokens: ['pmcflex', 'ascetic', 'stada', 'ivana', 'minty']...) from 1388 documents (total 958585 corpus positions)\n",
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.91 s\n",
      "Dictionary(76020 unique tokens: ['pmcflex', 'ascetic', 'stada', 'ivana', 'minty']...)\n",
      "./AGHHLMMNRT/Geography.xml\n",
      "['acre', 'unit', 'land', 'area', 'imperial', 'customary', 'systems', 'defined', 'area', 'chain']\n",
      "['map', 'oceania', 'country', 'codes', 'country', 'codes', 'short', 'alphabetic', 'numeric', 'geographical']\n",
      "['cairn', 'human', 'pile', 'stack', 'stones', 'word', 'cairn', 'comes', 'plural', 'cairns']\n",
      "['important', 'explorations', 'state', 'societies', 'chronological', 'order', 'exploration', 'explorer', 'northwest', 'african']\n",
      "['furlong', 'measure', 'distance', 'imperial', 'units', 'customary', 'units', 'equal', 'eighth', 'mile']\n",
      "['far', 'east', 'alternate', 'geographical', 'term', 'english', 'equivalents', 'languages', 'infobox', 'right']\n",
      "['clusters', 'cholera', 'cases', 'london', 'epidemic', 'classical', 'case', 'human', 'geography', 'human']\n",
      "['graticule', 'earth', 'sphere', 'ellipsoid', 'lines', 'pole', 'pole', 'lines', 'constant', 'longitude']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(55743 unique tokens: ['isenbruch', 'fage', 'isothermal', 'gunnislake', 'saada']...) from 978 documents (total 653601 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.73 s\n",
      "Dictionary(55743 unique tokens: ['isenbruch', 'fage', 'isothermal', 'gunnislake', 'saada']...)\n",
      "./AGHHLMMNRT/Health.xml\n",
      "['hearing', 'aid', 'assistive', 'technology', 'umbrella', 'term', 'includes', 'assistive', 'adaptive', 'rehabilitative']\n",
      "['antibiotic', 'resistance', 'tests', 'bacteria', 'streaked', 'dishes', 'white', 'antibiotic', 'impregnated', 'disks']\n",
      "['beryllium', 'chemical', 'element', 'symbol', 'atomic', 'number', 'relatively', 'rare', 'element', 'universe']\n",
      "['biostatistics', 'application', 'statistics', 'wide', 'range', 'topics', 'biology', 'science', 'biostatistics', 'encompasses']\n",
      "['condom', 'sheath', 'shaped', 'barrier', 'device', 'sexual', 'intercourse', 'reduce', 'probability', 'pregnancy']\n",
      "['chlorine', 'chemical', 'element', 'symbol', 'cl', 'atomic', 'number', 'second', 'lightest', 'halogens']\n",
      "['chromium', 'chemical', 'element', 'symbol', 'cr', 'atomic', 'number', 'element', 'group', 'steely']\n",
      "['cadmium', 'chemical', 'element', 'symbol', 'cd', 'atomic', 'number', 'soft', 'bluish', 'white']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(70914 unique tokens: ['ascetic', 'aβ', 'strictness', 'galvis', 'pds']...) from 2214 documents (total 1822659 corpus positions)\n",
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 s\n",
      "Dictionary(70914 unique tokens: ['ascetic', 'aβ', 'strictness', 'galvis', 'pds']...)\n",
      "./AGHHLMMNRT/History.xml\n",
      "['annales', 'school', 'group', 'historians', 'associated', 'style', 'historiography', 'developed', 'french', 'historians']\n",
      "['carinthia', 'austria', 'terms', 'ad', 'christ', 'bc', 'label', 'number', 'years', 'julian']\n",
      "['cover', 'english', 'edition', 'benjamin', 'franklin', 'autobiography', 'autobiography', 'greek', 'αὐτός', 'autos']\n",
      "['astronomical', 'year', 'numbering', 'based', 'ad', 'ce', 'year', 'numbering', 'follows', 'normal']\n",
      "['antoninianus', 'pacatianus', 'usurper', 'roman', 'emperor', 'philip', 'bears', 'legend', 'romae', 'aeternae']\n",
      "['common', 'era', 'current', 'era', 'ce', 'year', 'numbering', 'calendar', 'era', 'julian']\n",
      "['ancient', 'egypt', 'canonical', 'example', 'early', 'culture', 'considered', 'civilization', 'civilization', 'uk']\n",
      "['chronicle', 'greek', 'chronos', 'time', 'historical', 'account', 'facts', 'events', 'ranged', 'chronological']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(81702 unique tokens: ['ascetic', 'ingushetia', 'boğazkale', 'strictness', 'arcane']...) from 1468 documents (total 1098459 corpus positions)\n",
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.49 s\n",
      "Dictionary(81702 unique tokens: ['ascetic', 'ingushetia', 'boğazkale', 'strictness', 'arcane']...)\n",
      "./AGHHLMMNRT/Literature.xml\n",
      "['allegory', 'music', 'filippino', 'lippi', 'allegory', 'music', 'popular', 'theme', 'painting', 'lippi']\n",
      "['different', 'cultures', 'history', 'depicted', 'blindness', 'variety', 'ways', 'greeks', 'example', 'punishment']\n",
      "['literary', 'criticism', 'bildungsroman', 'novel', 'formation', 'novel', 'education', 'coming', 'age', 'story']\n",
      "['fiction', 'continuity', 'called', 'time', 'scheme', 'consistency', 'characteristics', 'people', 'plot', 'objects']\n",
      "['cut', 'technique', 'découpé', 'french', 'aleatory', 'literary', 'technique', 'text', 'cut', 'rearranged']\n",
      "['detective', 'fiction', 'subgenre', 'crime', 'fiction', 'mystery', 'fiction', 'investigator', 'detective', 'professional']\n",
      "['dyslexia', 'known', 'reading', 'disorder', 'characterized', 'trouble', 'reading', 'despite', 'normal', 'intelligence']\n",
      "['titlepage', 'aphra', 'behn', 'love', 'letters', 'epistolary', 'novel', 'novel', 'written', 'series']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(91750 unique tokens: ['tren', 'multiconference', 'galit', 'médicis', 'arcane']...) from 1772 documents (total 1128398 corpus positions)\n",
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.43 s\n",
      "Dictionary(91750 unique tokens: ['tren', 'multiconference', 'galit', 'médicis', 'arcane']...)\n",
      "./AGHHLMMNRT/Mathematics.xml\n",
      "['axiom', 'postulate', 'statement', 'taken', 'true', 'serve', 'premise', 'starting', 'point', 'reasoning']\n",
      "['derivative', 'cantor', 'function', 'unit', 'interval', 'numbers', 'unit', 'interval', 'mathematics', 'phrase']\n",
      "['absolute', 'infinite', 'mathematician', 'georg', 'cantor', 'concept', 'infinity', 'transcends', 'transfinite', 'numbers']\n",
      "['quantum', 'mechanics', 'bra', 'ket', 'notation', 'standard', 'notation', 'describing', 'quantum', 'states']\n",
      "['bayesian', 'probability', 'interpretation', 'concept', 'probability', 'instead', 'frequency', 'propensity', 'phenomenon', 'probability']\n",
      "['classical', 'mechanics', 'branch', 'physics', 'motion', 'macroscopic', 'objects', 'familiar', 'theories', 'physics']\n",
      "['conditional', 'proof', 'proof', 'takes', 'form', 'asserting', 'conditional', 'proving', 'antecedent', 'conditional']\n",
      "['mathematics', 'classification', 'finite', 'simple', 'groups', 'theorem', 'stating', 'finite', 'simple', 'group']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(45618 unique tokens: ['chile', 'borcherds', 'médicis', 'consecutive', 'mechanically']...) from 1054 documents (total 608299 corpus positions)\n",
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.02 s\n",
      "Dictionary(45618 unique tokens: ['chile', 'borcherds', 'médicis', 'consecutive', 'mechanically']...)\n",
      "./AGHHLMMNRT/Music.xml\n",
      "['cappella', 'italian', 'manner', 'chapel', 'music', 'specifically', 'group', 'solo', 'singing', 'instrumental']\n",
      "['embouchure', 'trumpeter', 'embouchure', 'use', 'facial', 'muscles', 'shaping', 'lips', 'mouthpiece', 'woodwind']\n",
      "['conclave', 'patrick', 'nielsen', 'hayden', 'emma', 'bull', 'making', 'music', 'wiscon', 'filk']\n",
      "['melody', 'opening', 'henry', 'purcell', 'thy', 'hand', 'belinda', 'dido', 'aeneas', 'figured']\n",
      "['artie', 'shaw', 'band', 'playing', 'jumpin', 'second', 'chorus', 'instrumental', 'musical', 'composition']\n",
      "['playing', 'ear', 'term', 'describing', 'ability', 'instrumental', 'musician', 'reproduce', 'piece', 'music']\n",
      "['american', 'jazz', 'singer', 'songwriter', 'billie', 'holiday', 'new', 'york', 'city', 'bonnie']\n",
      "['film', 'poster', 'hat', 'musical', 'film', 'film', 'genre', 'songs', 'sung', 'characters']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(68749 unique tokens: ['chuckie', 'chile', 'ingushetia', 'campra', 'ivana']...) from 1344 documents (total 853066 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.44 s\n",
      "Dictionary(68749 unique tokens: ['chuckie', 'chile', 'ingushetia', 'campra', 'ivana']...)\n",
      "./AGHHLMMNRT/Nature.xml\n",
      "['plot', 'lorenz', 'strange', 'attractor', 'values', 'butterfly', 'effect', 'sensitive', 'dependence', 'initial']\n",
      "['dc', 'plasma', 'violet', 'enhances', 'growth', 'carbon', 'nanotubes', 'laboratory', 'scale', 'pecvd']\n",
      "['casimir', 'forces', 'parallel', 'plates', 'sonicator', 'sonicator', 'turned', 'waves', 'excited', 'imitating']\n",
      "['inertial', 'frame', 'reference', 'upper', 'picture', 'black', 'ball', 'moves', 'straight', 'line']\n",
      "['earth', 'known', 'world', 'especially', 'geopolitics', 'geography', 'planet', 'sun', 'object', 'universe']\n",
      "['evolution', 'change', 'heritable', 'characteristics', 'biological', 'populations', 'successive', 'generations', 'evolutionary', 'processes']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ethical', 'naturalism', 'called', 'moral', 'naturalism', 'naturalistic', 'cognitivistic', 'definism', 'meta', 'ethical']\n",
      "['sun', 'source', 'energy', 'life', 'earth', 'derives', 'energy', 'mainly', 'nuclear', 'fusion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(44552 unique tokens: ['pmcflex', 'chile', 'isothermal', 'campra', 'consecutive']...) from 482 documents (total 449882 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.94 s\n",
      "Dictionary(44552 unique tokens: ['pmcflex', 'chile', 'isothermal', 'campra', 'consecutive']...)\n",
      "./AGHHLMMNRT/Religion.xml\n",
      "['kimiya', 'yi', 'sa', 'ādat', 'alchemy', 'happiness', 'text', 'islamic', 'philosophy', 'spiritual']\n",
      "['afterlife', 'referred', 'life', 'death', 'concept', 'realm', 'realm', 'physical', 'transcendental', 'essential']\n",
      "['absolute', 'infinite', 'mathematician', 'georg', 'cantor', 'concept', 'infinity', 'transcends', 'transfinite', 'numbers']\n",
      "['seat', 'universal', 'house', 'justice', 'governing', 'body', 'bahá', 'ís', 'haifa', 'israel']\n",
      "['icon', 'st', 'cyprian', 'carthage', 'urged', 'diligence', 'process', 'canonization', 'canonization', 'act']\n",
      "['word', 'catholic', 'lowercase', 'derived', 'late', 'latin', 'catholicus', 'greek', 'adjective', 'katholikos']\n",
      "['goddesses', 'juno', 'minerva', 'venus', 'religious', 'terms', 'divinity', 'godhead', 'state', 'things']\n",
      "['faith', 'rationality', 'ideologies', 'exist', 'varying', 'degrees', 'conflict', 'compatibility', 'rationality', 'based']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(61361 unique tokens: ['chile', 'brovary', 'strictness', 'wabisah', 'consecutive']...) from 842 documents (total 762579 corpus positions)\n",
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.36 s\n",
      "Dictionary(61361 unique tokens: ['chile', 'brovary', 'strictness', 'wabisah', 'consecutive']...)\n",
      "./AGHHLMMNRT/Technology.xml\n",
      "['american', 'national', 'standards', 'institute', 'ansi', 'private', 'non', 'profit', 'organization', 'oversees']\n",
      "['artificial', 'intelligence', 'ai', 'intelligence', 'exhibited', 'machines', 'science', 'field', 'ai', 'research']\n",
      "['computing', 'applet', 'small', 'application', 'performs', 'specific', 'task', 'runs', 'scope', 'dedicated']\n",
      "['basic', 'english', 'english', 'based', 'controlled', 'language', 'created', 'linguist', 'philosopher', 'charles']\n",
      "['british', 'science', 'fiction', 'writer', 'arthur', 'clarke', 'formulated', 'adages', 'known', 'clarke']\n",
      "['thought', 'experiment', 'person', 'chinese', 'room', 'passed', 'questions', 'outside', 'room', 'consults']\n",
      "['catapult', 'ballistic', 'device', 'launch', 'projectile', 'great', 'distance', 'aid', 'explosive', 'devices']\n",
      "['dyson', 'sphere', 'hypothetical', 'megastructure', 'completely', 'encompasses', 'star', 'captures', 'power', 'output']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(70949 unique tokens: ['tren', 'nanyo', 'nanorod', 'bachem', 'consecutive']...) from 1967 documents (total 1192363 corpus positions)\n",
      "INFO : discarding 62064 tokens: [('currently', 258), ('asc', 4), ('accrediting', 5), ('systems', 669), ('york', 255), ('new', 836), ('services', 415), ('designates', 4), ('technologies', 470), ('states', 445)]...\n",
      "INFO : keeping 8885 tokens which were in no less than 10 and no more than 196 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(8885 unique tokens: ['chile', 'maritime', 'agree', 'reconstruction', 'appeared']...)\n",
      "INFO : storing corpus in Matrix Market format to ./AGHHLMMNRT/wiki_bow.mm\n",
      "INFO : saving sparse matrix to ./AGHHLMMNRT/wiki_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n",
      "Dictionary(70949 unique tokens: ['tren', 'nanyo', 'nanorod', 'bachem', 'consecutive']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : saved 1967x8885 matrix, density=1.969% (344087/17476795)\n",
      "INFO : saving MmCorpus index to ./AGHHLMMNRT/wiki_bow.mm.index\n",
      "INFO : loaded corpus index from ./AGHHLMMNRT/wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from ./AGHHLMMNRT/wiki_bow.mm\n",
      "INFO : accepted corpus with 1967 documents, 8885 features, 344087 non-zero entries\n",
      "INFO : using asymmetric alpha [0.20349776650601445, 0.15460680268266819, 0.12465746387131386, 0.10442834231175278, 0.089848016434062955, 0.078840302634700543, 0.070235422324486096, 0.063324036444942805, 0.05765099744942407, 0.052910849340634232]\n",
      "INFO : using symmetric eta at 0.00011254924029262802\n",
      "INFO : using serial LDA version on this node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n",
      "MmCorpus(1967 documents, 8885 features, 344087 non-zero entries)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : running online LDA training, 10 topics, 10 passes over the supplied corpus of 1967 documents, updating model once every 1967 documents, evaluating perplexity every 1967 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : -9.805 per-word bound, 894.4 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 0, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.003*\"yes\" + 0.003*\"architecture\" + 0.002*\"ontology\" + 0.002*\"lte\" + 0.002*\"learning\" + 0.002*\"park\" + 0.002*\"nuclear\" + 0.002*\"canada\" + 0.002*\"mm\" + 0.001*\"gas\"\n",
      "INFO : topic #8 (0.058): 0.002*\"invented\" + 0.002*\"observatory\" + 0.002*\"html\" + 0.002*\"document\" + 0.002*\"mm\" + 0.002*\"file\" + 0.002*\"nuclear\" + 0.002*\"metadata\" + 0.002*\"bc\" + 0.002*\"canada\"\n",
      "INFO : topic #2 (0.125): 0.008*\"nuclear\" + 0.008*\"observatory\" + 0.003*\"learning\" + 0.002*\"anti\" + 0.002*\"document\" + 0.002*\"ontology\" + 0.002*\"clock\" + 0.001*\"test\" + 0.001*\"library\" + 0.001*\"invented\"\n",
      "INFO : topic #1 (0.155): 0.003*\"learning\" + 0.002*\"message\" + 0.002*\"tech\" + 0.002*\"document\" + 0.001*\"park\" + 0.001*\"audio\" + 0.001*\"students\" + 0.001*\"english\" + 0.001*\"format\" + 0.001*\"invented\"\n",
      "INFO : topic #0 (0.203): 0.005*\"engine\" + 0.002*\"lte\" + 0.002*\"airport\" + 0.002*\"aircraft\" + 0.002*\"journalism\" + 0.002*\"news\" + 0.002*\"windows\" + 0.002*\"file\" + 0.001*\"engineers\" + 0.001*\"china\"\n",
      "INFO : topic diff=0.926547, rho=1.000000\n",
      "INFO : -8.779 per-word bound, 439.3 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 1, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.005*\"yes\" + 0.004*\"architecture\" + 0.003*\"ontology\" + 0.002*\"literacy\" + 0.002*\"team\" + 0.002*\"battery\" + 0.002*\"park\" + 0.002*\"canada\" + 0.002*\"students\" + 0.001*\"mm\"\n",
      "INFO : topic #8 (0.058): 0.005*\"invented\" + 0.003*\"metadata\" + 0.003*\"html\" + 0.003*\"file\" + 0.003*\"patent\" + 0.002*\"bc\" + 0.002*\"document\" + 0.002*\"mm\" + 0.002*\"russian\" + 0.002*\"food\"\n",
      "INFO : topic #2 (0.125): 0.013*\"nuclear\" + 0.010*\"observatory\" + 0.004*\"learning\" + 0.003*\"anti\" + 0.002*\"ontology\" + 0.002*\"clock\" + 0.002*\"library\" + 0.002*\"clocks\" + 0.002*\"germany\" + 0.002*\"movement\"\n",
      "INFO : topic #1 (0.155): 0.004*\"learning\" + 0.002*\"tech\" + 0.002*\"students\" + 0.002*\"document\" + 0.002*\"style\" + 0.002*\"networking\" + 0.002*\"english\" + 0.002*\"specification\" + 0.002*\"economic\" + 0.002*\"culture\"\n",
      "INFO : topic #0 (0.203): 0.008*\"engine\" + 0.004*\"windows\" + 0.003*\"journalism\" + 0.003*\"news\" + 0.003*\"aircraft\" + 0.003*\"airport\" + 0.002*\"internal\" + 0.002*\"combustion\" + 0.002*\"samsung\" + 0.002*\"flight\"\n",
      "INFO : topic diff=0.384396, rho=0.577350\n",
      "INFO : -8.599 per-word bound, 387.8 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 2, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.007*\"yes\" + 0.005*\"architecture\" + 0.003*\"ontology\" + 0.003*\"literacy\" + 0.002*\"team\" + 0.002*\"stem\" + 0.002*\"nanotechnology\" + 0.002*\"battery\" + 0.002*\"iso\" + 0.002*\"students\"\n",
      "INFO : topic #8 (0.058): 0.007*\"invented\" + 0.004*\"metadata\" + 0.004*\"html\" + 0.003*\"patent\" + 0.003*\"file\" + 0.003*\"bc\" + 0.002*\"document\" + 0.002*\"russian\" + 0.002*\"discovered\" + 0.002*\"food\"\n",
      "INFO : topic #2 (0.125): 0.016*\"nuclear\" + 0.012*\"observatory\" + 0.004*\"anti\" + 0.004*\"learning\" + 0.003*\"ontology\" + 0.003*\"clock\" + 0.003*\"library\" + 0.002*\"clocks\" + 0.002*\"plant\" + 0.002*\"germany\"\n",
      "INFO : topic #1 (0.155): 0.005*\"learning\" + 0.002*\"students\" + 0.002*\"document\" + 0.002*\"networking\" + 0.002*\"tech\" + 0.002*\"style\" + 0.002*\"culture\" + 0.002*\"english\" + 0.002*\"sites\" + 0.002*\"school\"\n",
      "INFO : topic #0 (0.203): 0.011*\"engine\" + 0.006*\"windows\" + 0.004*\"journalism\" + 0.004*\"internal\" + 0.004*\"news\" + 0.003*\"combustion\" + 0.003*\"samsung\" + 0.003*\"aircraft\" + 0.003*\"car\" + 0.003*\"steam\"\n",
      "INFO : topic diff=0.364795, rho=0.500000\n",
      "INFO : -8.501 per-word bound, 362.3 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 3, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.009*\"yes\" + 0.006*\"architecture\" + 0.004*\"ontology\" + 0.003*\"nanotechnology\" + 0.003*\"team\" + 0.003*\"stem\" + 0.002*\"iso\" + 0.002*\"literacy\" + 0.002*\"battery\" + 0.002*\"codes\"\n",
      "INFO : topic #8 (0.058): 0.008*\"invented\" + 0.005*\"metadata\" + 0.004*\"patent\" + 0.004*\"file\" + 0.004*\"html\" + 0.003*\"bc\" + 0.003*\"discovered\" + 0.003*\"russian\" + 0.002*\"invention\" + 0.002*\"document\"\n",
      "INFO : topic #2 (0.125): 0.019*\"nuclear\" + 0.013*\"observatory\" + 0.005*\"anti\" + 0.004*\"ontology\" + 0.003*\"learning\" + 0.003*\"clock\" + 0.003*\"library\" + 0.003*\"clocks\" + 0.003*\"plant\" + 0.003*\"germany\"\n",
      "INFO : topic #1 (0.155): 0.005*\"learning\" + 0.003*\"document\" + 0.003*\"students\" + 0.002*\"networking\" + 0.002*\"style\" + 0.002*\"culture\" + 0.002*\"tech\" + 0.002*\"sites\" + 0.002*\"school\" + 0.002*\"english\"\n",
      "INFO : topic #0 (0.203): 0.012*\"engine\" + 0.007*\"windows\" + 0.005*\"linux\" + 0.005*\"steam\" + 0.004*\"internal\" + 0.004*\"journalism\" + 0.004*\"car\" + 0.004*\"combustion\" + 0.004*\"samsung\" + 0.004*\"android\"\n",
      "INFO : topic diff=0.353228, rho=0.447214\n",
      "INFO : -8.441 per-word bound, 347.6 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 4, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.009*\"yes\" + 0.007*\"architecture\" + 0.004*\"ontology\" + 0.004*\"nanotechnology\" + 0.003*\"team\" + 0.003*\"iso\" + 0.003*\"stem\" + 0.002*\"codes\" + 0.002*\"usability\" + 0.002*\"israel\"\n",
      "INFO : topic #8 (0.058): 0.009*\"invented\" + 0.005*\"metadata\" + 0.004*\"patent\" + 0.004*\"file\" + 0.004*\"html\" + 0.003*\"bc\" + 0.003*\"invention\" + 0.003*\"discovered\" + 0.003*\"russian\" + 0.003*\"ancient\"\n",
      "INFO : topic #2 (0.125): 0.021*\"nuclear\" + 0.015*\"observatory\" + 0.005*\"anti\" + 0.005*\"ontology\" + 0.004*\"library\" + 0.004*\"clock\" + 0.003*\"plant\" + 0.003*\"clocks\" + 0.003*\"germany\" + 0.003*\"weapons\"\n",
      "INFO : topic #1 (0.155): 0.006*\"learning\" + 0.003*\"document\" + 0.003*\"students\" + 0.003*\"networking\" + 0.002*\"culture\" + 0.002*\"style\" + 0.002*\"documents\" + 0.002*\"sites\" + 0.002*\"school\" + 0.002*\"political\"\n",
      "INFO : topic #0 (0.203): 0.013*\"engine\" + 0.008*\"windows\" + 0.006*\"linux\" + 0.005*\"steam\" + 0.005*\"internal\" + 0.005*\"car\" + 0.004*\"android\" + 0.004*\"combustion\" + 0.004*\"apple\" + 0.004*\"journalism\"\n",
      "INFO : topic diff=0.348158, rho=0.408248\n",
      "INFO : -8.400 per-word bound, 337.8 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 5, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.009*\"yes\" + 0.007*\"architecture\" + 0.004*\"nanotechnology\" + 0.004*\"ontology\" + 0.004*\"iso\" + 0.003*\"team\" + 0.003*\"stem\" + 0.003*\"usability\" + 0.003*\"codes\" + 0.003*\"test\"\n",
      "INFO : topic #8 (0.058): 0.009*\"invented\" + 0.005*\"metadata\" + 0.004*\"patent\" + 0.004*\"file\" + 0.004*\"html\" + 0.004*\"bc\" + 0.003*\"invention\" + 0.003*\"discovered\" + 0.003*\"ancient\" + 0.003*\"russian\"\n",
      "INFO : topic #2 (0.125): 0.024*\"nuclear\" + 0.016*\"observatory\" + 0.006*\"anti\" + 0.005*\"ontology\" + 0.004*\"library\" + 0.004*\"clock\" + 0.003*\"plant\" + 0.003*\"clocks\" + 0.003*\"germany\" + 0.003*\"weapons\"\n",
      "INFO : topic #1 (0.155): 0.006*\"learning\" + 0.004*\"document\" + 0.003*\"students\" + 0.003*\"networking\" + 0.002*\"documents\" + 0.002*\"culture\" + 0.002*\"style\" + 0.002*\"sites\" + 0.002*\"school\" + 0.002*\"writing\"\n",
      "INFO : topic #0 (0.203): 0.014*\"engine\" + 0.009*\"windows\" + 0.007*\"linux\" + 0.006*\"steam\" + 0.005*\"car\" + 0.005*\"internal\" + 0.005*\"android\" + 0.005*\"apple\" + 0.005*\"combustion\" + 0.005*\"samsung\"\n",
      "INFO : topic diff=0.339900, rho=0.377964\n",
      "INFO : -8.370 per-word bound, 330.7 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 6, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.009*\"yes\" + 0.007*\"architecture\" + 0.005*\"nanotechnology\" + 0.004*\"iso\" + 0.004*\"ontology\" + 0.003*\"usability\" + 0.003*\"team\" + 0.003*\"stem\" + 0.003*\"testing\" + 0.003*\"test\"\n",
      "INFO : topic #8 (0.058): 0.009*\"invented\" + 0.005*\"metadata\" + 0.005*\"patent\" + 0.004*\"file\" + 0.004*\"html\" + 0.004*\"bc\" + 0.003*\"invention\" + 0.003*\"ancient\" + 0.003*\"discovered\" + 0.003*\"russian\"\n",
      "INFO : topic #2 (0.125): 0.026*\"nuclear\" + 0.017*\"observatory\" + 0.006*\"anti\" + 0.006*\"ontology\" + 0.004*\"library\" + 0.004*\"clock\" + 0.004*\"plant\" + 0.003*\"germany\" + 0.003*\"clocks\" + 0.003*\"weapons\"\n",
      "INFO : topic #1 (0.155): 0.006*\"learning\" + 0.004*\"document\" + 0.003*\"students\" + 0.003*\"documents\" + 0.003*\"networking\" + 0.002*\"culture\" + 0.002*\"style\" + 0.002*\"sites\" + 0.002*\"writing\" + 0.002*\"school\"\n",
      "INFO : topic #0 (0.203): 0.015*\"engine\" + 0.010*\"windows\" + 0.007*\"linux\" + 0.006*\"steam\" + 0.005*\"car\" + 0.005*\"internal\" + 0.005*\"android\" + 0.005*\"apple\" + 0.005*\"game\" + 0.005*\"combustion\"\n",
      "INFO : topic diff=0.326496, rho=0.353553\n",
      "INFO : -8.346 per-word bound, 325.4 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 7, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.009*\"yes\" + 0.007*\"architecture\" + 0.005*\"iso\" + 0.005*\"nanotechnology\" + 0.004*\"usability\" + 0.003*\"team\" + 0.003*\"ontology\" + 0.003*\"manufacturing\" + 0.003*\"stem\" + 0.003*\"testing\"\n",
      "INFO : topic #8 (0.058): 0.009*\"invented\" + 0.005*\"metadata\" + 0.005*\"patent\" + 0.004*\"file\" + 0.004*\"bc\" + 0.004*\"html\" + 0.003*\"invention\" + 0.003*\"ancient\" + 0.003*\"discovered\" + 0.003*\"russian\"\n",
      "INFO : topic #2 (0.125): 0.027*\"nuclear\" + 0.018*\"observatory\" + 0.007*\"anti\" + 0.007*\"ontology\" + 0.005*\"library\" + 0.004*\"clock\" + 0.004*\"plant\" + 0.004*\"germany\" + 0.004*\"clocks\" + 0.004*\"weapons\"\n",
      "INFO : topic #1 (0.155): 0.006*\"learning\" + 0.004*\"document\" + 0.003*\"documents\" + 0.003*\"students\" + 0.003*\"networking\" + 0.002*\"culture\" + 0.002*\"writing\" + 0.002*\"sites\" + 0.002*\"style\" + 0.002*\"school\"\n",
      "INFO : topic #0 (0.203): 0.015*\"engine\" + 0.010*\"windows\" + 0.007*\"linux\" + 0.006*\"steam\" + 0.006*\"car\" + 0.005*\"internal\" + 0.005*\"android\" + 0.005*\"game\" + 0.005*\"apple\" + 0.005*\"combustion\"\n",
      "INFO : topic diff=0.308640, rho=0.333333\n",
      "INFO : -8.327 per-word bound, 321.1 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 8, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.008*\"yes\" + 0.007*\"architecture\" + 0.005*\"iso\" + 0.005*\"nanotechnology\" + 0.004*\"usability\" + 0.004*\"manufacturing\" + 0.003*\"team\" + 0.003*\"testing\" + 0.003*\"stem\" + 0.003*\"test\"\n",
      "INFO : topic #8 (0.058): 0.009*\"invented\" + 0.005*\"patent\" + 0.005*\"metadata\" + 0.004*\"file\" + 0.004*\"bc\" + 0.003*\"invention\" + 0.003*\"html\" + 0.003*\"ancient\" + 0.003*\"discovered\" + 0.003*\"russian\"\n",
      "INFO : topic #2 (0.125): 0.029*\"nuclear\" + 0.019*\"observatory\" + 0.007*\"ontology\" + 0.007*\"anti\" + 0.005*\"library\" + 0.004*\"clock\" + 0.004*\"plant\" + 0.004*\"germany\" + 0.004*\"weapons\" + 0.004*\"clocks\"\n",
      "INFO : topic #1 (0.155): 0.006*\"learning\" + 0.004*\"document\" + 0.003*\"documents\" + 0.003*\"students\" + 0.003*\"networking\" + 0.002*\"culture\" + 0.002*\"writing\" + 0.002*\"sites\" + 0.002*\"style\" + 0.002*\"xml\"\n",
      "INFO : topic #0 (0.203): 0.015*\"engine\" + 0.010*\"windows\" + 0.008*\"linux\" + 0.006*\"steam\" + 0.006*\"car\" + 0.006*\"game\" + 0.006*\"internal\" + 0.006*\"android\" + 0.005*\"apple\" + 0.005*\"microsoft\"\n",
      "INFO : topic diff=0.287924, rho=0.316228\n",
      "INFO : -8.312 per-word bound, 317.7 perplexity estimate based on a held-out corpus of 1967 documents with 662487 words\n",
      "INFO : PROGRESS: pass 9, at document #1967/1967\n",
      "INFO : topic #9 (0.053): 0.008*\"yes\" + 0.007*\"architecture\" + 0.006*\"iso\" + 0.005*\"nanotechnology\" + 0.004*\"manufacturing\" + 0.004*\"usability\" + 0.004*\"testing\" + 0.003*\"team\" + 0.003*\"maintenance\" + 0.003*\"test\"\n",
      "INFO : topic #8 (0.058): 0.010*\"invented\" + 0.005*\"patent\" + 0.005*\"metadata\" + 0.004*\"file\" + 0.004*\"bc\" + 0.004*\"invention\" + 0.003*\"html\" + 0.003*\"ancient\" + 0.003*\"discovered\" + 0.003*\"russian\"\n",
      "INFO : topic #2 (0.125): 0.030*\"nuclear\" + 0.020*\"observatory\" + 0.008*\"ontology\" + 0.007*\"anti\" + 0.005*\"library\" + 0.004*\"clock\" + 0.004*\"germany\" + 0.004*\"plant\" + 0.004*\"weapons\" + 0.004*\"clocks\"\n",
      "INFO : topic #1 (0.155): 0.006*\"learning\" + 0.004*\"document\" + 0.003*\"documents\" + 0.003*\"students\" + 0.003*\"networking\" + 0.002*\"culture\" + 0.002*\"writing\" + 0.002*\"sites\" + 0.002*\"xml\" + 0.002*\"style\"\n",
      "INFO : topic #0 (0.203): 0.015*\"engine\" + 0.011*\"windows\" + 0.008*\"linux\" + 0.006*\"car\" + 0.006*\"game\" + 0.006*\"steam\" + 0.006*\"internal\" + 0.006*\"android\" + 0.005*\"apple\" + 0.005*\"microsoft\"\n",
      "INFO : topic diff=0.266027, rho=0.301511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "    \n",
    "#if not os.path.exists(wiki_bow_path):\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            print(doc_path)\n",
    "            stream = iter_wiki(doc_path)\n",
    "            for tokens in itertools.islice(iter_wiki(doc_path), 8):\n",
    "                print (tokens[:10])\n",
    "            doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "            %time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "            print(id2word_wiki)\n",
    "\n",
    "id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "\n",
    "# create a stream of bag-of-words vectors\n",
    "wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "\n",
    "#wiki_bow_path = root_folder+'wiki_bow.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)\n",
    "\n",
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "print(mm_corpus)\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000) \n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=len(category_list), id2word=id2word_wiki, passes=10, alpha='asymmetric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AGHHLMMNRT/Arts.xml\n",
      "./AGHHLMMNRT/Geography.xml\n",
      "./AGHHLMMNRT/Health.xml\n",
      "./AGHHLMMNRT/History.xml\n",
      "./AGHHLMMNRT/Literature.xml\n",
      "./AGHHLMMNRT/Mathematics.xml\n",
      "./AGHHLMMNRT/Music.xml\n",
      "./AGHHLMMNRT/Nature.xml\n",
      "./AGHHLMMNRT/Religion.xml\n",
      "./AGHHLMMNRT/Technology.xml\n",
      "{'Geography': [(0, 0.011791355894419096), (1, 0.16658928858671304), (2, 0.15731707482932109), (3, 0.088883679464277068), (4, 0.062664567894465895), (5, 0.054968828868599405), (6, 0.21751491475480583), (7, 0.035238980939058522), (8, 0.15251178715669866), (9, 0.045332156509909204)], 'Nature': [(0, 0.0040957422234728742), (1, 0.11640268790950072), (2, 0.084708711991682437), (3, 0.01835855424713544), (4, 0.15853358485424762), (5, 0.11681042214667688), (6, 0.092161462426417082), (7, 0.22212030865860119), (8, 0.15192920565220186), (9, 0.028873547948347438)], 'Music': [(0, 0.037226605732226789), (1, 0.31380052708233969), (2, 0.088398329707307183), (3, 0.020226880116930832), (4, 0.19232875046545511), (5, 0.01833184195890895), (6, 0.017925118402416496), (7, 0.035929512967943215), (8, 0.25478289295900869), (9, 0.013002302490801011)], 'Technology': [(0, 0.094232946713212404), (1, 0.22454356689160662), (2, 0.043794108124363379), (3, 0.1575089196519274), (4, 0.089902473014982179), (5, 0.075676272965420413), (6, 0.067481868029006076), (7, 0.082337066048174326), (8, 0.072842920435313846), (9, 0.083246396441165743)], 'History': [(0, 0.011587721731582385), (1, 0.26916454386245192), (2, 0.20440990416236607), (3, 0.0084779576942075191), (4, 0.014078729833389292), (5, 0.0066857009299229331), (6, 0.087947067914660296), (7, 0.11920259959118987), (8, 0.25040457869551935), (9, 0.020066769063391707)], 'Mathematics': [(0, 0.010384287413166715), (1, 0.25046230528625951), (2, 0.061303786345381507), (3, 0.077660361896395344), (4, 0.25694521190372083), (5, 0.08341230392656504), (6, 0.0084794262891227917), (7, 0.10266745157009603), (8, 0.096323639797179672), (9, 0.04503565058942971)], 'Arts': [(0, 0.011497512324944576), (1, 0.39153530722141366), (2, 0.10105159929535419), (3, 0.0026296969079579336), (4, 0.030588238425515002), (5, 0.041941237420325503), (6, 0.047482663913132772), (7, 0.045752333707558435), (8, 0.30208494697427646), (9, 0.017837496606525925)], 'Health': [(0, 0.0064984631038501831), (1, 0.17218515824338304), (2, 0.055862471211944861), (3, 0.0263979028286636), (4, 0.022936706492797738), (5, 0.037964668896801029), (6, 0.03685285710631149), (7, 0.50095557038634531), (8, 0.076046448192807106), (9, 0.058120739352704578)], 'Religion': [(0, 0.0052959640092918479), (1, 0.28603713016945387), (2, 0.12275953567427585), (3, 0.0035169125108171998), (4, 0.017669352883242437), (5, 0.003329755423677624), (6, 0.021186157888887055), (7, 0.29514674821373416), (8, 0.22601636834485517), (9, 0.011781545073106478)], 'Literature': [(0, 0.014830965872635301), (1, 0.47492051177125444), (2, 0.13527269006442355), (3, 0.0036392551065288241), (4, 0.031856595980210782), (5, 0.0094592663179317443), (6, 0.014866310820678076), (7, 0.10910709380613588), (8, 0.19073088530078164), (9, 0.0078660440878669195)]}\n"
     ]
    }
   ],
   "source": [
    "def calculateCentroid(topic_docs):\n",
    "    test_doc = [tokens for tokens in iter_wiki(topic_docs)]\n",
    "    part = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    \n",
    "    topic_dic={}\n",
    "    \n",
    "    for i in range(len(category_list)):\n",
    "        topic_dic[i]=0\n",
    "        \n",
    "    for doc in part:\n",
    "        for p in doc:\n",
    "            topic_dic[p[0]] += p[1]\n",
    "    \n",
    "    centroid = [(x, topic_dic[x]/len(part)) for x in range(len(category_list))]\n",
    "    return centroid\n",
    "\n",
    "    \n",
    "centroids_dict={}\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            print(doc_path)\n",
    "            centroid = calculateCentroid(doc_path)\n",
    "            centroids_dict[file.replace(\".xml\",\"\")]=centroid\n",
    "            \n",
    "print(centroids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AGHHLMMNRT/Arts.xml\n",
      "./AGHHLMMNRT/Geography.xml\n",
      "./AGHHLMMNRT/Health.xml\n",
      "./AGHHLMMNRT/History.xml\n",
      "./AGHHLMMNRT/Literature.xml\n",
      "./AGHHLMMNRT/Mathematics.xml\n",
      "./AGHHLMMNRT/Music.xml\n",
      "./AGHHLMMNRT/Nature.xml\n",
      "./AGHHLMMNRT/Religion.xml\n",
      "./AGHHLMMNRT/Technology.xml\n",
      "[[ 0.01627921  0.11715731  0.03181314  0.05778004  0.52589895  0.0677934\n",
      "   0.0176143   0.04995426  0.08883577  0.01908459]\n",
      " [ 0.59972617  0.10568627  0.00981675  0.06194827  0.05198937  0.03325015\n",
      "   0.02216838  0.01454496  0.04479864  0.04712439]\n",
      " [ 0.01412895  0.40229812  0.08994092  0.0183581   0.04869675  0.01734975\n",
      "   0.0231164   0.14560316  0.16802676  0.06691448]\n",
      " [ 0.01318473  0.13624423  0.07369427  0.01096234  0.05134124  0.02810069\n",
      "   0.03770116  0.0553779   0.57141093  0.0136704 ]\n",
      " [ 0.00385632  0.13782755  0.03619274  0.02080084  0.02646481  0.02299453\n",
      "   0.02453488  0.6214677   0.05838575  0.04125445]\n",
      " [ 0.01262234  0.73579553  0.06984985  0.01125391  0.02800177  0.00856313\n",
      "   0.01330206  0.04851836  0.04816946  0.01572323]\n",
      " [ 0.01354352  0.0784655   0.10360777  0.01821154  0.02130102  0.02888256\n",
      "   0.51533454  0.04243192  0.10768413  0.06214142]\n",
      " [ 0.03753126  0.06433311  0.01793561  0.66551207  0.05844106  0.02566312\n",
      "   0.02142108  0.02603929  0.03349687  0.04067397]\n",
      " [ 0.01307534  0.17794469  0.51255248  0.00793504  0.02233373  0.00631699\n",
      "   0.05827817  0.03974003  0.11775417  0.03550239]\n",
      " [ 0.01466831  0.05357514  0.02315882  0.04352447  0.10429793  0.55012923\n",
      "   0.03280174  0.04836905  0.08392802  0.03719188]]\n"
     ]
    }
   ],
   "source": [
    "def get_values(l):\n",
    "    #print (l)\n",
    "    g=[0]*len(category_list)\n",
    "    for i in l:\n",
    "        g[i[0]]=i[1]\n",
    "        \n",
    "    return g\n",
    "\n",
    "\n",
    "\n",
    "def calculate_kmeans(part):\n",
    "    km = KMeans(n_clusters=len(category_list), init='k-means++', max_iter=500, n_init=1)\n",
    "    km.fit(part)\n",
    "    print(km.cluster_centers_)\n",
    "    return km\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    part=[]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            print(doc_path)\n",
    "            test_doc = [tokens for tokens in iter_wiki(doc_path)]\n",
    "            #print(lda_model[id2word_wiki.doc2bow(tokens))\n",
    "            \n",
    "            part+=[get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "        \n",
    "    \n",
    "    km = calculate_kmeans(part)\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./AGHHLMMNRT/Arts.xml\n",
      "Counter({3: 453, 5: 376, 2: 335, 8: 90, 6: 61, 9: 32, 4: 22, 0: 13, 1: 6})\n",
      "./AGHHLMMNRT/Geography.xml\n",
      "Counter({6: 273, 8: 146, 3: 137, 2: 118, 7: 90, 5: 75, 0: 66, 9: 48, 4: 20, 1: 5})\n",
      "./AGHHLMMNRT/Health.xml\n",
      "Counter({4: 1479, 2: 293, 3: 107, 5: 104, 8: 79, 9: 56, 6: 48, 0: 27, 7: 14, 1: 7})\n",
      "./AGHHLMMNRT/History.xml\n",
      "Counter({3: 373, 8: 316, 2: 283, 5: 212, 4: 134, 6: 131, 0: 8, 1: 7, 9: 3, 7: 1})\n",
      "./AGHHLMMNRT/Literature.xml\n",
      "Counter({5: 641, 2: 534, 3: 265, 8: 209, 4: 79, 0: 28, 6: 10, 1: 4, 9: 2})\n",
      "./AGHHLMMNRT/Mathematics.xml\n",
      "Counter({0: 364, 2: 213, 5: 111, 4: 94, 3: 82, 9: 75, 8: 56, 7: 46, 1: 8, 6: 5})\n",
      "./AGHHLMMNRT/Music.xml\n",
      "Counter({3: 378, 2: 335, 0: 226, 5: 218, 8: 96, 4: 34, 1: 29, 6: 13, 7: 9, 9: 6})\n",
      "./AGHHLMMNRT/Nature.xml\n",
      "Counter({4: 118, 0: 92, 3: 69, 9: 59, 2: 46, 6: 46, 8: 32, 5: 16, 7: 3, 1: 1})\n",
      "./AGHHLMMNRT/Religion.xml\n",
      "Counter({4: 251, 2: 247, 3: 181, 8: 82, 5: 62, 6: 11, 0: 7, 1: 1})\n",
      "./AGHHLMMNRT/Technology.xml\n",
      "Counter({5: 382, 7: 338, 1: 213, 0: 186, 6: 171, 2: 166, 9: 161, 4: 154, 3: 120, 8: 76})\n",
      "{'Geography': 6, 'Nature': 4, 'Music': 3, 'Technology': 5, 'History': 3, 'Mathematics': 0, 'Arts': 3, 'Health': 4, 'Religion': 4, 'Literature': 5}\n"
     ]
    }
   ],
   "source": [
    "def get_values(l):\n",
    "    #print (l)\n",
    "    g=[0]*len(category_list)\n",
    "    for i in l:\n",
    "        g[i[0]]=i[1]\n",
    "        \n",
    "    return g\n",
    "import collections\n",
    "\n",
    "label_dict={}\n",
    "\n",
    "def predict_label(doc_path):\n",
    "    test_doc = [tokens for tokens in iter_wiki(doc_path)]\n",
    "    #part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc] \n",
    "    labels=[km.predict([get_values(lda_model[id2word_wiki.doc2bow(tokens)])])[0] for tokens in test_doc]\n",
    "    counter = collections.Counter(labels)\n",
    "    print(counter)\n",
    "    return counter.most_common(1)[0][0]\n",
    "\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    part=[]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            print(doc_path)\n",
    "            counter = predict_label(doc_path)\n",
    "            \n",
    "            #_label_dict[file.replace(\".xml\",\"\")] = counter\n",
    "            label_dict[file.replace(\".xml\",\"\")] = counter\n",
    "            \n",
    "    \n",
    "    \n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drawgraph(x_label,y,file,text_data):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    x = np.arange(len(x_label))  # the x locations for the groups\n",
    "    width = 0.3       # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x, y, width, color='blue')\n",
    "    ax.set_ylabel('Weights')\n",
    "    ax.set_title('Topic Distribution')\n",
    "    #ax.set_xticks(x + width / 2)\n",
    "    #ax.set_rotation(90)\n",
    "    ax.set_xticklabels(category_list)\n",
    "    ax.text(3, 8, text_data, style='italic',\n",
    "        bbox={'facecolor':'green', 'alpha':0.5, 'pad':10})\n",
    "\n",
    "    def autolabel(rects):\n",
    "        \"\"\"\n",
    "        Attach a text label above each bar displaying its height\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                    '%d' % int(height),\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.setp(plt.xticks()[0], rotation=45)\n",
    "    #fig = plt.figure(figuresize=4, 5)\n",
    "    plt.savefig(file.replace(\".xml\",\"\")+'.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Simplex1/test/Test_doc_AI.xml\n",
      "[[0, 0, 0, 0, 0.75216177474420698, 0.23792168289009072, 0, 0, 0, 0]]\n",
      "[0]\n",
      "./Simplex1/test/Test_doc_Allegory.xml\n",
      "[[0, 0.44688787122227502, 0.051161283057231145, 0, 0.011893890673878962, 0, 0, 0.11826688629582272, 0.37083911222211074, 0]]\n",
      "[2]\n",
      "./Simplex1/test/Test_doc_CS.xml\n",
      "[[0, 0.38739112264163889, 0, 0.013455103442951186, 0.097264129072204913, 0, 0, 0.33947677866437376, 0, 0.16203058686206484]]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "def getPart(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(testFolder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = testFolder + file\n",
    "            print(doc_path)\n",
    "            \n",
    "            part=getPart(doc_path)\n",
    "            print(part)\n",
    "            \n",
    "            print(km.predict(part))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(2, 500)\n",
      "[[ 1.        0.253986]]\n"
     ]
    }
   ],
   "source": [
    "#Cosine Similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer( max_features=500, stop_words='english')\n",
    "t1=[]\n",
    "with open(\"./Simplex1/test/Test_doc_AI.xml\",encoding=\"UTF-8\") as f:\n",
    "    t1.append(str(f.read()))\n",
    "t2=[]\n",
    "with open(\"./Simplex1/test/Test_doc_Allegory.xml\",encoding=\"UTF-8\") as f:\n",
    "    t1.append(str(f.read()))\n",
    "    \n",
    "print(len(t1))\n",
    "#print(t1,t2)\n",
    "\n",
    "#file = open(\"./Simplex1/test/Test_doc_AI.xml\", 'r',encoding=\"UTF-8\")\n",
    "#p1=vectorizer.fit_transform(file.read())\n",
    "#file = open(\"./Simplex1/test/Test_doc_CS.xml\", 'r',encoding=\"UTF-8\")\n",
    "#p2=vectorizer.fit_transform(file.read())\n",
    "p1=vectorizer.fit_transform(t1)\n",
    "print(p1.shape)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(p1[0:1], p1))\n",
    "\n",
    "\n",
    "#print(gensim.matutils.cossim(p1, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
