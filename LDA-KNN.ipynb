{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import time;\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO\n",
    "model_name=\"LDA-KNN\"\n",
    "id2word_wiki=gensim.corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List Categories:\n",
    "#category_list = [\"Mathematics\",\"Technology\",\"Music\"]\n",
    "#category_list = [\"Mathematics\",\"Technology\",\"Music\",\"History\",\"Geography\",\"Arts\",\"Health\",\"Nature\",\"Religion\",\"Literature\"]\n",
    "category_list = [\"1977 introductions\", \"Language\" ,\"Arts\", \"Asia\", \"Asthma\", \"Automobiles\", \"BRICS nation\", \"Belief\", \"Climate\", \"Crime\", \"Culture\", \"Dance\", \"Deserts\", \"Dolls\", \"Earth\", \"Engines\", \"Fishing\", \"Folklore\", \"Games\", \"Geography\", \"Glass\", \"Government agencies\", \"Health\", \"History\", \"Humans\", \"Hygiene\", \"India\", \"Industry\", \"Internet\", \"Law\", \"Life\", \"Literature\", \"Mathematics\", \"Matter\", \"Millionaires\", \"Music\", \"Nature\", \"Nothing\", \"Parties\", \"Peace\", \"Politics\", \"Religion\", \"Sexology\", \"Society\", \"Songs\", \"Space\", \"Technology\", \"Television\", \"Transport\", \"Water sports\"]\n",
    "test_folder =  \"./test/\"\n",
    "root_folder = \"./Simplex/\"\n",
    "list.sort(category_list)\n",
    "folder_name=''.join([x[0]+x[1]+x[2] for x in category_list])\n",
    "root_folder='./'+folder_name+'/'\n",
    "\n",
    "print(\"Checking folder : \"+folder_name)\n",
    "if not os.path.exists(root_folder):\n",
    "    os.mkdir(root_folder)\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "\n",
    "    \n",
    "\n",
    "wiki_bow_path = root_folder+'wiki_bow.mm'\n",
    "wiki_dict_path = root_folder+'wiki_dict.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download Page Ids:\n",
    "#https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&categories=mathematics&doit=Do it!\n",
    "print (\"Scanning CSV\")\n",
    "for cat in category_list:\n",
    "    if not os.path.exists(root_folder+cat+\".csv\"):\n",
    "        url=\"https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=\"+cat\n",
    "        urllib.request.urlretrieve(url, root_folder+cat+\".csv\")\n",
    "        print(\"Downloading \",cat+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CSV to XML Download data:\n",
    "\n",
    "def get_data(ids,output_file):\n",
    "    url=\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=xml&pageids=\"+ids\n",
    "    req = urllib.request.urlopen(url)\n",
    "    if req.getcode() == 200:\n",
    "        soup = BeautifulSoup(req.read(), 'html.parser')\n",
    "        s = soup.find_all('page')\n",
    "        for si in s:\n",
    "            output_file.write(str(si))\n",
    "\n",
    "def batch_train(file):\n",
    "    output_file = open(file.replace(\".csv\",\".xml\"), 'a', encoding=\"utf8\")\n",
    "    output_file.write(\"<pages>\")\n",
    "                    \n",
    "    csvReader = csv.reader(open(file,'r'))\n",
    "    totalRecords = sum(1 for row in csv.reader(open(file,'r',encoding=\"UTF-8\")) )\n",
    "    print (file.replace(\".csv\",\".xml\"),totalRecords)\n",
    "    start = 0\n",
    "    end = start + 50\n",
    "   \n",
    "    while (start <= totalRecords):\n",
    "        pageIds = \"\"\n",
    "        for row in itertools.islice(csv.reader(open(file,'r',encoding=\"UTF-8\")),start,end):\n",
    "            pageIds = pageIds + row[2] + \"|\"\n",
    "        \n",
    "        get_data(pageIds,output_file)\n",
    "        start = end + 1\n",
    "        end = start + 50\n",
    "        if end> totalRecords:\n",
    "            end=totalRecords\n",
    "            \n",
    "    output_file.write(\"</pages>\")\n",
    "    \n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            if not os.path.exists(root_folder+file.replace(\".csv\",\".xml\")):\n",
    "                print(\"Downloading \",root_folder+file.replace(\".csv\",\".xml\"))\n",
    "                batch_train(path + file)\n",
    "\n",
    "print (\"Done downloading\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Training Model\")\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "if not (os.path.exists(wiki_bow_path) and  os.path.exists(wiki_dict_path)):\n",
    "    for path, subdirs, files in os.walk(root_folder):\n",
    "        del subdirs[:]\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                doc_path = path + file\n",
    "                stream = iter_wiki(doc_path)\n",
    "                doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "                id2word_wiki.merge_with(gensim.corpora.Dictionary(doc_stream))\n",
    "\n",
    "    id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "    \n",
    "    # create a stream of bag-of-words vectors\n",
    "    wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "    \n",
    "    id2word_wiki.save(wiki_dict_path)\n",
    "    gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)\n",
    "\n",
    "id2word_wiki = gensim.corpora.Dictionary.load(wiki_dict_path)\n",
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000) \n",
    "lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=len(category_list), id2word=id2word_wiki, passes=10, alpha='asymmetric')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=[]\n",
    "X=[]\n",
    "\n",
    "def get_values(l):\n",
    "    g=[0]*len(category_list)\n",
    "    for i in l:\n",
    "        g[i[0]]=i[1]        \n",
    "    return g\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            topic = file.replace(\".xml\",\"\")\n",
    "            \n",
    "            test_doc = [tokens for tokens in iter_wiki(doc_path)]\n",
    "            X+=[get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "            y+=[category_list.index(topic)]*len(test_doc)\n",
    "\n",
    "print(\"Feeding data to KNeighborsClassifier\")\n",
    "n_neighbors = 15\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test \n",
    "def get_part(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "results=\"\"\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            results+=file.replace(\".xml\",\"\")+\":  \"\n",
    "            \n",
    "            part=get_part(doc_path)\n",
    "            results+=category_list[clf.predict(part)[0]]\n",
    "            results += \"\\n\\n\"\n",
    "                \n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
