{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\Anaconda\\envs\\py35\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports:\n",
    "import time;\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import svm\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO\n",
    "model_name=\"LDA-Linear Classifier\"\n",
    "id2word_wiki=gensim.corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking folder : 197ArtAsiAstAutBRIBelCliCriCulDanDesDolEarEngFisFolGamGeoGlaGovHeaHisHumHygIndIndIntLanLawLifLitMatMatMilMusNatNotParPeaPolRelSexSocSonSpaTecTelTraWat\n"
     ]
    }
   ],
   "source": [
    "# List Categories:\n",
    "#category_list = [\"Mathematics\",\"Technology\",\"Music\"]\n",
    "#category_list = [\"Mathematics\",\"Technology\",\"Music\",\"History\",\"Geography\",\"Arts\",\"Health\",\"Nature\",\"Religion\",\"Literature\"]\n",
    "category_list = [\"1977 introductions\", \"Language\" ,\"Arts\", \"Asia\", \"Asthma\", \"Automobiles\", \"BRICS nation\", \"Belief\", \"Climate\", \"Crime\", \"Culture\", \"Dance\", \"Deserts\", \"Dolls\", \"Earth\", \"Engines\", \"Fishing\", \"Folklore\", \"Games\", \"Geography\", \"Glass\", \"Government agencies\", \"Health\", \"History\", \"Humans\", \"Hygiene\", \"India\", \"Industry\", \"Internet\", \"Law\", \"Life\", \"Literature\", \"Mathematics\", \"Matter\", \"Millionaires\", \"Music\", \"Nature\", \"Nothing\", \"Parties\", \"Peace\", \"Politics\", \"Religion\", \"Sexology\", \"Society\", \"Songs\", \"Space\", \"Technology\", \"Television\", \"Transport\", \"Water sports\"]\n",
    "test_folder =  \"./test/\"\n",
    "root_folder = \"./Simplex/\"\n",
    "list.sort(category_list)\n",
    "folder_name=''.join([x[0]+x[1]+x[2] for x in category_list])\n",
    "root_folder='./'+folder_name+'/'\n",
    "\n",
    "print(\"Checking folder : \"+folder_name)\n",
    "if not os.path.exists(root_folder):\n",
    "    os.mkdir(root_folder)\n",
    "if not os.path.exists(root_folder+model_name):\n",
    "    os.mkdir(root_folder+model_name)\n",
    "\n",
    "    \n",
    "\n",
    "wiki_bow_path = root_folder+'wiki_bow.mm'\n",
    "wiki_dict_path = root_folder+'wiki_dict.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning CSV\n"
     ]
    }
   ],
   "source": [
    "# Download Page Ids:\n",
    "#https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&categories=mathematics&doit=Do it!\n",
    "print (\"Scanning CSV\")\n",
    "for cat in category_list:\n",
    "    if not os.path.exists(root_folder+cat+\".csv\"):\n",
    "        url=\"https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=\"+cat\n",
    "        urllib.request.urlretrieve(url, root_folder+cat+\".csv\")\n",
    "        print(\"Downloading \",cat+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done downloading\n"
     ]
    }
   ],
   "source": [
    "# CSV to XML Download data:\n",
    "\n",
    "def get_data(ids,output_file):\n",
    "    url=\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=xml&pageids=\"+ids\n",
    "    req = urllib.request.urlopen(url)\n",
    "    if req.getcode() == 200:\n",
    "        soup = BeautifulSoup(req.read(), 'html.parser')\n",
    "        s = soup.find_all('page')\n",
    "        for si in s:\n",
    "            output_file.write(str(si))\n",
    "\n",
    "def batch_train(file):\n",
    "    output_file = open(file.replace(\".csv\",\".xml\"), 'a', encoding=\"utf8\")\n",
    "    output_file.write(\"<pages>\")\n",
    "                    \n",
    "    csvReader = csv.reader(open(file,'r'))\n",
    "    totalRecords = sum(1 for row in csv.reader(open(file,'r',encoding=\"UTF-8\")) )\n",
    "    print (file.replace(\".csv\",\".xml\"),totalRecords)\n",
    "    start = 0\n",
    "    end = start + 50\n",
    "   \n",
    "    while (start <= totalRecords):\n",
    "        pageIds = \"\"\n",
    "        for row in itertools.islice(csv.reader(open(file,'r',encoding=\"UTF-8\")),start,end):\n",
    "            pageIds = pageIds + row[2] + \"|\"\n",
    "        \n",
    "        get_data(pageIds,output_file)\n",
    "        start = end + 1\n",
    "        end = start + 50\n",
    "        if end> totalRecords:\n",
    "            end=totalRecords\n",
    "            \n",
    "    output_file.write(\"</pages>\")\n",
    "    \n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            if not os.path.exists(root_folder+file.replace(\".csv\",\".xml\")):\n",
    "                print(\"Downloading \",root_folder+file.replace(\".csv\",\".xml\"))\n",
    "                batch_train(path + file)\n",
    "\n",
    "print (\"Done downloading\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading Dictionary object from ./197ArtAsiAstAutBRIBelCliCriCulDanDesDolEarEngFisFolGamGeoGlaGovHeaHisHumHygIndIndIntLanLawLifLitMatMatMilMusNatNotParPeaPolRelSexSocSonSpaTecTelTraWat/wiki_dict.dict\n",
      "INFO : loaded ./197ArtAsiAstAutBRIBelCliCriCulDanDesDolEarEngFisFolGamGeoGlaGovHeaHisHumHygIndIndIntLanLawLifLitMatMatMilMusNatNotParPeaPolRelSexSocSonSpaTecTelTraWat/wiki_dict.dict\n",
      "INFO : loaded corpus index from ./197ArtAsiAstAutBRIBelCliCriCulDanDesDolEarEngFisFolGamGeoGlaGovHeaHisHumHygIndIndIntLanLawLifLitMatMatMilMusNatNotParPeaPolRelSexSocSonSpaTecTelTraWat/wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from ./197ArtAsiAstAutBRIBelCliCriCulDanDesDolEarEngFisFolGamGeoGlaGovHeaHisHumHygIndIndIntLanLawLifLitMatMatMilMusNatNotParPeaPolRelSexSocSonSpaTecTelTraWat/wiki_bow.mm\n",
      "INFO : accepted corpus with 1608 documents, 81913 features, 415419 non-zero entries\n",
      "INFO : using asymmetric alpha [0.065720114619765069, 0.057577435589965774, 0.0512300642788781, 0.046143209018255173, 0.041975299490250938, 0.038497951823544055, 0.035552672036333638, 0.033026021428740539, 0.030834668975084991, 0.028916024281649043, 0.027222162796221014, 0.025715767984380015, 0.024367350148626718, 0.023153296647486065, 0.022054477315964192, 0.021055229001204018, 0.020142604185876857, 0.019305806901132063, 0.018535763636680632, 0.017824793001705544, 0.017166348601743871, 0.016554816873888198, 0.01598535664693598, 0.015453770713674625, 0.014956402203289119, 0.014490050340886969, 0.014051901490559003, 0.013639472341929111, 0.013250562816416619, 0.012883216806990807, 0.012535689272246658, 0.012206418516453994, 0.011894002726457412, 0.011597180021799278, 0.011314811419285154, 0.011045866227071753, 0.010789409473426357, 0.010544591046981935, 0.010310636282675109, 0.010086837773711103, 0.0098725482272315878, 0.0096671742117037975, 0.0094701706688266173, 0.009281036083073237, 0.0090993082187320693, 0.0089245603481562606, 0.0087563979064320153, 0.0085944555172619147, 0.008438394342878618, 0.0082878997175365107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric eta at 1.2208074420421667e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 50 topics, 10 passes over the supplied corpus of 1608 documents, updating model once every 1608 documents, evaluating perplexity every 1608 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : -24.807 per-word bound, 29358925.4 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 0, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.004*\"treatment\" + 0.004*\"surface\" + 0.004*\"hot\" + 0.003*\"zone\" + 0.003*\"organisms\" + 0.003*\"ocean\" + 0.003*\"springs\" + 0.002*\"sea\" + 0.002*\"groundwater\" + 0.002*\"river\"\n",
      "INFO : topic #48 (0.008): 0.005*\"surface\" + 0.003*\"overall\" + 0.002*\"winner\" + 0.002*\"sea\" + 0.002*\"ocean\" + 0.002*\"ice\" + 0.002*\"irrigation\" + 0.002*\"marine\" + 0.002*\"winners\" + 0.002*\"snow\"\n",
      "INFO : topic #2 (0.051): 0.005*\"ice\" + 0.005*\"sea\" + 0.004*\"surface\" + 0.003*\"ocean\" + 0.002*\"river\" + 0.002*\"temperature\" + 0.002*\"ship\" + 0.002*\"waters\" + 0.002*\"flow\" + 0.002*\"stream\"\n",
      "INFO : topic #1 (0.058): 0.005*\"river\" + 0.004*\"surface\" + 0.003*\"species\" + 0.003*\"ocean\" + 0.002*\"sea\" + 0.002*\"flow\" + 0.002*\"plant\" + 0.002*\"solar\" + 0.002*\"waters\" + 0.002*\"flood\"\n",
      "INFO : topic #0 (0.066): 0.006*\"hot\" + 0.005*\"springs\" + 0.005*\"flood\" + 0.005*\"river\" + 0.003*\"bath\" + 0.003*\"spring\" + 0.002*\"species\" + 0.002*\"floods\" + 0.002*\"flash\" + 0.002*\"flow\"\n",
      "INFO : topic diff=34.510100, rho=1.000000\n",
      "INFO : -11.456 per-word bound, 2809.2 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 1, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.007*\"treatment\" + 0.006*\"organisms\" + 0.006*\"zone\" + 0.004*\"hot\" + 0.004*\"ocean\" + 0.004*\"surface\" + 0.003*\"ecosystems\" + 0.003*\"springs\" + 0.003*\"organic\" + 0.003*\"spa\"\n",
      "INFO : topic #48 (0.008): 0.004*\"surface\" + 0.003*\"ocean\" + 0.003*\"rock\" + 0.002*\"ice\" + 0.002*\"sea\" + 0.002*\"valve\" + 0.002*\"twins\" + 0.002*\"marine\" + 0.002*\"zan\" + 0.002*\"overall\"\n",
      "INFO : topic #2 (0.051): 0.005*\"ice\" + 0.005*\"tsunami\" + 0.004*\"sea\" + 0.004*\"surface\" + 0.004*\"ship\" + 0.003*\"ocean\" + 0.003*\"qanat\" + 0.003*\"ships\" + 0.003*\"vessel\" + 0.002*\"wave\"\n",
      "INFO : topic #1 (0.058): 0.004*\"surface\" + 0.004*\"species\" + 0.003*\"energy\" + 0.003*\"plant\" + 0.003*\"treatment\" + 0.003*\"desalination\" + 0.003*\"river\" + 0.003*\"ocean\" + 0.003*\"solar\" + 0.002*\"bacteria\"\n",
      "INFO : topic #0 (0.066): 0.021*\"hot\" + 0.018*\"springs\" + 0.009*\"bath\" + 0.008*\"spring\" + 0.006*\"flood\" + 0.005*\"flash\" + 0.004*\"degrees\" + 0.004*\"river\" + 0.003*\"dead\" + 0.003*\"floods\"\n",
      "INFO : topic diff=2.487827, rho=0.577350\n",
      "INFO : -10.373 per-word bound, 1325.9 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 2, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.009*\"treatment\" + 0.008*\"organisms\" + 0.007*\"zone\" + 0.005*\"ocean\" + 0.004*\"sewage\" + 0.004*\"ecosystems\" + 0.004*\"hot\" + 0.004*\"spa\" + 0.004*\"surface\" + 0.004*\"primary\"\n",
      "INFO : topic #48 (0.008): 0.004*\"rock\" + 0.004*\"surface\" + 0.003*\"twins\" + 0.003*\"zan\" + 0.003*\"ocean\" + 0.002*\"valve\" + 0.002*\"super\" + 0.002*\"check\" + 0.002*\"powers\" + 0.002*\"pull\"\n",
      "INFO : topic #2 (0.051): 0.006*\"tsunami\" + 0.005*\"ship\" + 0.004*\"sea\" + 0.004*\"qanat\" + 0.004*\"ships\" + 0.004*\"ice\" + 0.004*\"surface\" + 0.004*\"vessel\" + 0.003*\"wave\" + 0.003*\"ocean\"\n",
      "INFO : topic #1 (0.058): 0.004*\"treatment\" + 0.004*\"energy\" + 0.003*\"surface\" + 0.003*\"desalination\" + 0.003*\"plant\" + 0.003*\"species\" + 0.003*\"bacteria\" + 0.003*\"drinking\" + 0.003*\"chemical\" + 0.003*\"temperature\"\n",
      "INFO : topic #0 (0.066): 0.032*\"hot\" + 0.030*\"springs\" + 0.015*\"bath\" + 0.015*\"spring\" + 0.006*\"degrees\" + 0.004*\"flash\" + 0.004*\"flood\" + 0.004*\"ailments\" + 0.004*\"spa\" + 0.004*\"skin\"\n",
      "INFO : topic diff=2.011162, rho=0.500000\n",
      "INFO : -9.873 per-word bound, 938.0 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 3, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.009*\"treatment\" + 0.009*\"organisms\" + 0.008*\"zone\" + 0.005*\"sewage\" + 0.005*\"ocean\" + 0.005*\"primary\" + 0.005*\"ecosystems\" + 0.005*\"organic\" + 0.005*\"spa\" + 0.004*\"ecosystem\"\n",
      "INFO : topic #48 (0.008): 0.005*\"rock\" + 0.004*\"twins\" + 0.004*\"zan\" + 0.003*\"surface\" + 0.003*\"valve\" + 0.003*\"super\" + 0.003*\"ocean\" + 0.003*\"sneak\" + 0.003*\"check\" + 0.002*\"pull\"\n",
      "INFO : topic #2 (0.051): 0.007*\"ship\" + 0.007*\"tsunami\" + 0.005*\"ships\" + 0.005*\"vessel\" + 0.004*\"qanat\" + 0.004*\"sea\" + 0.003*\"cargo\" + 0.003*\"surface\" + 0.003*\"wave\" + 0.003*\"mermaid\"\n",
      "INFO : topic #1 (0.058): 0.005*\"treatment\" + 0.004*\"energy\" + 0.004*\"desalination\" + 0.003*\"drinking\" + 0.003*\"plant\" + 0.003*\"chemical\" + 0.003*\"surface\" + 0.003*\"bacteria\" + 0.003*\"temperature\" + 0.003*\"species\"\n",
      "INFO : topic #0 (0.066): 0.040*\"hot\" + 0.038*\"springs\" + 0.020*\"bath\" + 0.019*\"spring\" + 0.007*\"degrees\" + 0.006*\"spa\" + 0.005*\"towel\" + 0.005*\"baths\" + 0.004*\"skin\" + 0.004*\"ailments\"\n",
      "INFO : topic diff=1.599982, rho=0.447214\n",
      "INFO : -9.582 per-word bound, 766.4 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 4, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.010*\"treatment\" + 0.010*\"organisms\" + 0.008*\"zone\" + 0.006*\"sewage\" + 0.005*\"primary\" + 0.005*\"ecosystems\" + 0.005*\"organic\" + 0.005*\"ocean\" + 0.005*\"ecosystem\" + 0.005*\"spa\"\n",
      "INFO : topic #48 (0.008): 0.006*\"rock\" + 0.004*\"twins\" + 0.004*\"zan\" + 0.003*\"sneak\" + 0.003*\"valve\" + 0.003*\"super\" + 0.003*\"check\" + 0.003*\"surface\" + 0.003*\"pull\" + 0.003*\"bollard\"\n",
      "INFO : topic #2 (0.051): 0.008*\"ship\" + 0.007*\"tsunami\" + 0.006*\"ships\" + 0.006*\"vessel\" + 0.005*\"qanat\" + 0.004*\"cargo\" + 0.004*\"sea\" + 0.004*\"wave\" + 0.004*\"rocket\" + 0.003*\"mermaid\"\n",
      "INFO : topic #1 (0.058): 0.005*\"treatment\" + 0.004*\"energy\" + 0.004*\"drinking\" + 0.004*\"desalination\" + 0.003*\"chemical\" + 0.003*\"plant\" + 0.003*\"bacteria\" + 0.003*\"disease\" + 0.003*\"temperature\" + 0.003*\"surface\"\n",
      "INFO : topic #0 (0.066): 0.045*\"hot\" + 0.043*\"springs\" + 0.023*\"bath\" + 0.022*\"spring\" + 0.007*\"spa\" + 0.007*\"degrees\" + 0.006*\"baths\" + 0.006*\"towel\" + 0.005*\"skin\" + 0.004*\"ailments\"\n",
      "INFO : topic diff=1.260836, rho=0.408248\n",
      "INFO : -9.396 per-word bound, 673.9 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 5, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.011*\"treatment\" + 0.011*\"organisms\" + 0.008*\"zone\" + 0.006*\"sewage\" + 0.005*\"primary\" + 0.005*\"ecosystems\" + 0.005*\"organic\" + 0.005*\"biological\" + 0.005*\"ecosystem\" + 0.005*\"ocean\"\n",
      "INFO : topic #48 (0.008): 0.006*\"rock\" + 0.005*\"twins\" + 0.005*\"zan\" + 0.003*\"sneak\" + 0.003*\"super\" + 0.003*\"valve\" + 0.003*\"check\" + 0.003*\"pull\" + 0.003*\"bollard\" + 0.003*\"box\"\n",
      "INFO : topic #2 (0.051): 0.009*\"ship\" + 0.007*\"tsunami\" + 0.007*\"ships\" + 0.006*\"vessel\" + 0.005*\"qanat\" + 0.005*\"cargo\" + 0.004*\"sea\" + 0.004*\"rocket\" + 0.004*\"wave\" + 0.003*\"mermaid\"\n",
      "INFO : topic #1 (0.058): 0.006*\"treatment\" + 0.004*\"energy\" + 0.004*\"drinking\" + 0.004*\"desalination\" + 0.004*\"chemical\" + 0.003*\"bacteria\" + 0.003*\"plant\" + 0.003*\"disease\" + 0.003*\"acid\" + 0.003*\"temperature\"\n",
      "INFO : topic #0 (0.066): 0.049*\"hot\" + 0.046*\"springs\" + 0.025*\"bath\" + 0.023*\"spring\" + 0.009*\"spa\" + 0.007*\"degrees\" + 0.007*\"baths\" + 0.006*\"towel\" + 0.005*\"skin\" + 0.004*\"ailments\"\n",
      "INFO : topic diff=0.990397, rho=0.377964\n",
      "INFO : -9.272 per-word bound, 618.0 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 6, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.011*\"treatment\" + 0.011*\"organisms\" + 0.008*\"zone\" + 0.007*\"sewage\" + 0.006*\"primary\" + 0.005*\"ecosystems\" + 0.005*\"organic\" + 0.005*\"biological\" + 0.005*\"ecosystem\" + 0.005*\"plants\"\n",
      "INFO : topic #48 (0.008): 0.007*\"rock\" + 0.005*\"twins\" + 0.005*\"zan\" + 0.004*\"sneak\" + 0.003*\"super\" + 0.003*\"valve\" + 0.003*\"check\" + 0.003*\"box\" + 0.003*\"pull\" + 0.003*\"bollard\"\n",
      "INFO : topic #2 (0.051): 0.010*\"ship\" + 0.008*\"tsunami\" + 0.008*\"ships\" + 0.007*\"vessel\" + 0.005*\"qanat\" + 0.005*\"cargo\" + 0.004*\"rocket\" + 0.004*\"sea\" + 0.004*\"wave\" + 0.004*\"safety\"\n",
      "INFO : topic #1 (0.058): 0.006*\"treatment\" + 0.004*\"drinking\" + 0.004*\"energy\" + 0.004*\"chemical\" + 0.004*\"desalination\" + 0.003*\"disease\" + 0.003*\"bacteria\" + 0.003*\"plant\" + 0.003*\"acid\" + 0.003*\"infection\"\n",
      "INFO : topic #0 (0.066): 0.051*\"hot\" + 0.048*\"springs\" + 0.027*\"bath\" + 0.024*\"spring\" + 0.011*\"spa\" + 0.008*\"baths\" + 0.007*\"degrees\" + 0.006*\"towel\" + 0.005*\"skin\" + 0.005*\"mineral\"\n",
      "INFO : topic diff=0.778549, rho=0.353553\n",
      "INFO : -9.184 per-word bound, 581.6 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 7, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.011*\"treatment\" + 0.011*\"organisms\" + 0.008*\"zone\" + 0.007*\"sewage\" + 0.006*\"primary\" + 0.006*\"biological\" + 0.006*\"ecosystems\" + 0.006*\"organic\" + 0.005*\"ecosystem\" + 0.005*\"plants\"\n",
      "INFO : topic #48 (0.008): 0.007*\"rock\" + 0.005*\"twins\" + 0.005*\"zan\" + 0.004*\"sneak\" + 0.004*\"super\" + 0.003*\"box\" + 0.003*\"valve\" + 0.003*\"boat\" + 0.003*\"check\" + 0.003*\"pull\"\n",
      "INFO : topic #2 (0.051): 0.011*\"ship\" + 0.008*\"ships\" + 0.008*\"tsunami\" + 0.007*\"vessel\" + 0.005*\"cargo\" + 0.005*\"qanat\" + 0.004*\"rocket\" + 0.004*\"sea\" + 0.004*\"wave\" + 0.004*\"safety\"\n",
      "INFO : topic #1 (0.058): 0.006*\"treatment\" + 0.004*\"drinking\" + 0.004*\"energy\" + 0.004*\"chemical\" + 0.004*\"desalination\" + 0.003*\"disease\" + 0.003*\"bacteria\" + 0.003*\"acid\" + 0.003*\"plant\" + 0.003*\"infection\"\n",
      "INFO : topic #0 (0.066): 0.052*\"hot\" + 0.049*\"springs\" + 0.028*\"bath\" + 0.024*\"spring\" + 0.012*\"spa\" + 0.008*\"baths\" + 0.007*\"degrees\" + 0.006*\"towel\" + 0.006*\"skin\" + 0.006*\"mineral\"\n",
      "INFO : topic diff=0.614019, rho=0.333333\n",
      "INFO : -9.120 per-word bound, 556.5 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 8, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.012*\"treatment\" + 0.012*\"organisms\" + 0.008*\"zone\" + 0.008*\"sewage\" + 0.006*\"biological\" + 0.006*\"primary\" + 0.006*\"ecosystems\" + 0.006*\"organic\" + 0.005*\"ecosystem\" + 0.005*\"plants\"\n",
      "INFO : topic #48 (0.008): 0.007*\"rock\" + 0.005*\"twins\" + 0.005*\"zan\" + 0.004*\"sneak\" + 0.004*\"super\" + 0.004*\"boat\" + 0.004*\"box\" + 0.003*\"valve\" + 0.003*\"check\" + 0.003*\"pull\"\n",
      "INFO : topic #2 (0.051): 0.011*\"ship\" + 0.009*\"ships\" + 0.008*\"tsunami\" + 0.008*\"vessel\" + 0.006*\"cargo\" + 0.005*\"qanat\" + 0.004*\"rocket\" + 0.004*\"sea\" + 0.004*\"safety\" + 0.004*\"wave\"\n",
      "INFO : topic #1 (0.058): 0.006*\"treatment\" + 0.005*\"drinking\" + 0.004*\"chemical\" + 0.004*\"energy\" + 0.004*\"desalination\" + 0.004*\"disease\" + 0.003*\"acid\" + 0.003*\"bacteria\" + 0.003*\"plant\" + 0.003*\"infection\"\n",
      "INFO : topic #0 (0.066): 0.053*\"hot\" + 0.049*\"springs\" + 0.029*\"bath\" + 0.025*\"spring\" + 0.013*\"spa\" + 0.009*\"baths\" + 0.007*\"degrees\" + 0.007*\"towel\" + 0.006*\"bathing\" + 0.006*\"mineral\"\n",
      "INFO : topic diff=0.486637, rho=0.316228\n",
      "INFO : -9.072 per-word bound, 538.4 perplexity estimate based on a held-out corpus of 1608 documents with 782325 words\n",
      "INFO : PROGRESS: pass 9, at document #1608/1608\n",
      "INFO : topic #49 (0.008): 0.012*\"treatment\" + 0.012*\"organisms\" + 0.008*\"sewage\" + 0.008*\"zone\" + 0.006*\"biological\" + 0.006*\"primary\" + 0.006*\"ecosystems\" + 0.006*\"organic\" + 0.005*\"ecosystem\" + 0.005*\"plants\"\n",
      "INFO : topic #48 (0.008): 0.007*\"rock\" + 0.005*\"twins\" + 0.005*\"zan\" + 0.004*\"sneak\" + 0.004*\"super\" + 0.004*\"boat\" + 0.004*\"box\" + 0.003*\"check\" + 0.003*\"valve\" + 0.003*\"pull\"\n",
      "INFO : topic #2 (0.051): 0.012*\"ship\" + 0.010*\"ships\" + 0.008*\"vessel\" + 0.008*\"tsunami\" + 0.006*\"cargo\" + 0.005*\"qanat\" + 0.004*\"rocket\" + 0.004*\"sea\" + 0.004*\"safety\" + 0.004*\"wave\"\n",
      "INFO : topic #1 (0.058): 0.007*\"treatment\" + 0.005*\"drinking\" + 0.004*\"chemical\" + 0.004*\"energy\" + 0.004*\"desalination\" + 0.004*\"disease\" + 0.004*\"acid\" + 0.003*\"bacteria\" + 0.003*\"plant\" + 0.003*\"sodium\"\n",
      "INFO : topic #0 (0.066): 0.053*\"hot\" + 0.050*\"springs\" + 0.029*\"bath\" + 0.025*\"spring\" + 0.014*\"spa\" + 0.009*\"baths\" + 0.007*\"degrees\" + 0.007*\"bathing\" + 0.007*\"towel\" + 0.006*\"mineral\"\n",
      "INFO : topic diff=0.388060, rho=0.301511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Model\")\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "\n",
    "\n",
    "if not (os.path.exists(wiki_bow_path) and  os.path.exists(wiki_dict_path)):\n",
    "    for path, subdirs, files in os.walk(root_folder):\n",
    "        del subdirs[:]\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                doc_path = path + file\n",
    "                stream = iter_wiki(doc_path)\n",
    "                doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "                id2word_wiki.merge_with(gensim.corpora.Dictionary(doc_stream))\n",
    "\n",
    "    id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "    \n",
    "    # create a stream of bag-of-words vectors\n",
    "    wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "    \n",
    "    id2word_wiki.save(wiki_dict_path) \n",
    "\n",
    "    gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)\n",
    "\n",
    "id2word_wiki = gensim.corpora.Dictionary.load(wiki_dict_path)\n",
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000) \n",
    "lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=len(category_list), id2word=id2word_wiki, passes=10, alpha='asymmetric')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d173902ba269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".xml\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtest_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid2word_wiki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d173902ba269>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtopic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".xml\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtest_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid2word_wiki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcategory_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_doc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-75a86e1d8d3b>\u001b[0m in \u001b[0;36miter_wiki\u001b[0;34m(dump_file)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;34m\"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mignore_namespaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmy_extract_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdump_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilter_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-75a86e1d8d3b>\u001b[0m in \u001b[0;36mmy_extract_pages\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0melems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpage_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rev\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpage_tag\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-75a86e1d8d3b>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmy_extract_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0melems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpage_tag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"rev\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0melems\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Work\\Anaconda\\envs\\py35\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1300\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                 \u001b[1;31m# load event buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y=[]\n",
    "X=[]\n",
    "\n",
    "def get_values(l):\n",
    "    g=[0]*len(category_list)\n",
    "    for i in l:\n",
    "        g[i[0]]=i[1]        \n",
    "    return g\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            topic = file.replace(\".xml\",\"\")\n",
    "            \n",
    "            test_doc = [tokens for tokens in iter_wiki(doc_path)]\n",
    "            X+=[get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "            y+=[category_list.index(topic)]*len(test_doc)\n",
    "\n",
    "print(\"Feeding data to SVC\")\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear', C=C).fit(X, y)\n",
    "rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\n",
    "poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\n",
    "lin_svc = svm.LinearSVC(C=C).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'svc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b031caf125b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlin_svc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrbf_svc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoly_svc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                  \u001b[1;31m#print(category_list[clf.predict(part)[0]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                  \u001b[0mresults\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mcategory_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svc' is not defined"
     ]
    }
   ],
   "source": [
    "# Test \n",
    "results=\"\"\n",
    "def get_part(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [get_values(lda_model[id2word_wiki.doc2bow(tokens)]) for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "print(\"Testing results\")\n",
    "for path, subdirs, files in os.walk(test_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = test_folder + file\n",
    "            results+=file.replace(\".xml\",\"\")+\":  \"\n",
    "            \n",
    "            part=get_part(doc_path)\n",
    "            for i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n",
    "                 results +=category_list[clf.predict(part)[0]] + \"\\n\\n\"\n",
    "                \n",
    "output_file = open(root_folder+model_name+\"/\"+model_name+str(len(category_list))+\".txt\", 'w', encoding=\"utf8\")\n",
    "output_file.write(results)\n",
    "output_file.close()\n",
    "print(\" Done \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
