{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports:\n",
    "import urllib.request\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# List Categories:\n",
    "category_list = [\"Mathematics\",\"Technology\",\"Music\"]\n",
    "testFolder =  \"./Simplex1/test/\"\n",
    "root_folder = \"./Simplex1/\"\n",
    "list.sort(category_list)\n",
    "root_folder='./'+''.join([x[0] for x in category_list])+'/'\n",
    "\n",
    "if not os.path.exists(root_folder):\n",
    "    os.mkdir(root_folder)\n",
    "\n",
    "wiki_bow_path = root_folder+'wiki_bow.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download Page Ids:\n",
    "#https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&categories=mathematics&doit=Do it!\n",
    "for cat in category_list:\n",
    "    url=\"https://petscan.wmflabs.org/?language=en&project=wikipedia&depth=1&format=csv&doit=Do%20it!&categories=\"+cat\n",
    "    print(url)\n",
    "    urllib.request.urlretrieve(url, root_folder+cat+\".csv\")\n",
    "\n",
    "    print(cat+\".csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CSV to XML Download data:\n",
    "\n",
    "def getData(ids,outputFile):\n",
    "    url=\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&rvprop=content&format=xml&pageids=\"+ids\n",
    "    req = urllib.request.urlopen(url)\n",
    "    if req.getcode() == 200:\n",
    "        soup = BeautifulSoup(req.read(), 'html.parser')\n",
    "        s = soup.find_all('page')\n",
    "        for si in s:\n",
    "            outputFile.write(str(si))\n",
    "\n",
    "def batchTrain(file):\n",
    "    outputFile = open(file.replace(\".csv\",\".xml\"), 'a', encoding=\"utf8\")\n",
    "    outputFile.write(\"<pages>\")\n",
    "                    \n",
    "    csvReader = csv.reader(open(file,'r'))\n",
    "    totalRecords = sum(1 for row in csv.reader(open(file,'r',encoding=\"UTF-8\")) )\n",
    "    print (totalRecords)\n",
    "    start = 0\n",
    "    end = start + 50\n",
    "   \n",
    "    while (start <= totalRecords):\n",
    "        pageIds = \"\"\n",
    "        for row in itertools.islice(csv.reader(open(file,'r',encoding=\"UTF-8\")),start,end):\n",
    "            pageIds = pageIds + row[2] + \"|\"\n",
    "        \n",
    "        getData(pageIds,outputFile)\n",
    "        start = end + 1\n",
    "        end = start + 50\n",
    "        if end> totalRecords:\n",
    "            end=totalRecords\n",
    "            \n",
    "    outputFile.write(\"</pages>\")\n",
    "    \n",
    "\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            if not os.path.exists(root_folder+file.replace(\".csv\",\".xml\")):\n",
    "                batchTrain(path + file)\n",
    "\n",
    "print (\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(id2word_wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "    \n",
    "if not os.path.exists(wiki_bow_path):\n",
    "    for path, subdirs, files in os.walk(root_folder):\n",
    "        del subdirs[:]\n",
    "        for file in files:\n",
    "            if file.endswith('.xml'):\n",
    "                doc_path = path + file\n",
    "                print(doc_path)\n",
    "                stream = iter_wiki(doc_path)\n",
    "                for tokens in itertools.islice(iter_wiki(doc_path), 8):\n",
    "                    print (tokens[:10])\n",
    "                doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "                %time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "                print(id2word_wiki)\n",
    "\n",
    "    id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "    \n",
    "    # create a stream of bag-of-words vectors\n",
    "    wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "\n",
    "    #wiki_bow_path = root_folder+'wiki_bow.mm'\n",
    "    %time gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)\n",
    "\n",
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "print(mm_corpus)\n",
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000) \n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=len(category_list), id2word=id2word_wiki, passes=10, alpha='asymmetric')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculateCentroid(topic_docs):\n",
    "    test_doc = [tokens for tokens in iter_wiki(topic_docs)]\n",
    "    part = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    \n",
    "    topic_dic={}\n",
    "    \n",
    "    for i in range(len(category_list)):\n",
    "        topic_dic[i]=0\n",
    "        \n",
    "    for doc in part:\n",
    "        for p in doc:\n",
    "            topic_dic[p[0]] += p[1]\n",
    "    \n",
    "    centroid = [(x, topic_dic[x]/len(part)) for x in range(len(category_list))]\n",
    "    return centroid\n",
    "    \n",
    "centroids_dict={}\n",
    "for path, subdirs, files in os.walk(root_folder):\n",
    "    del subdirs[:]\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = path + file\n",
    "            print(doc_path)\n",
    "            centroid = calculateCentroid(doc_path)\n",
    "            centroids_dict[file.replace(\".xml\",\"\")]=centroid\n",
    "            \n",
    "print(centroids_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drawgraph(x_label,y,file,text_data):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    x = np.arange(len(x_label))  # the x locations for the groups\n",
    "    width = 0.3       # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x, y, width, color='blue')\n",
    "    ax.set_ylabel('Weights')\n",
    "    ax.set_title('Topic Distribution')\n",
    "    #ax.set_xticks(x + width / 2)\n",
    "    #ax.set_rotation(90)\n",
    "    ax.set_xticklabels(category_list)\n",
    "    ax.text(3, 8, text_data, style='italic',\n",
    "        bbox={'facecolor':'green', 'alpha':0.5, 'pad':10})\n",
    "\n",
    "    def autolabel(rects):\n",
    "        \"\"\"\n",
    "        Attach a text label above each bar displaying its height\n",
    "        \"\"\"\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,\n",
    "                    '%d' % int(height),\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "    autolabel(rects1)\n",
    "    plt.xticks(rotation=90)\n",
    "    #plt.setp(plt.xticks()[0], rotation=45)\n",
    "    #fig = plt.figure(figuresize=4, 5)\n",
    "    plt.savefig(file.replace(\".xml\",\"\")+'.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test \n",
    "def getPart(testFile):\n",
    "    test_doc = [tokens for tokens in iter_wiki(testFile)]\n",
    "    part = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc]\n",
    "    return part\n",
    "\n",
    "\n",
    "for path, subdirs, files in os.walk(testFolder):\n",
    "    for file in files:\n",
    "        if file.endswith('.xml'):\n",
    "            doc_path = testFolder + file\n",
    "            print(doc_path)\n",
    "            path=getPart(doc_path)\n",
    "            graph_data=[]\n",
    "            #for topic,centroid in centroids_dict.items():\n",
    "            #    print(topic, np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid], path)]))\n",
    "            #    graph_data.append(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid], path)]))\n",
    "            text_data=\"\"\n",
    "            for topic in category_list:\n",
    "                cos_dis=np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroids_dict[topic]], path)])\n",
    "                text_data = text_data+ topic+\":\"+str(cos_dis)+\"\\n\"\n",
    "                graph_data.append(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroids_dict[topic]], path)]))\n",
    "            print(graph_data)\n",
    "            drawgraph(list(centroids_dict.keys()),graph_data,root_folder+file,text_data)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
