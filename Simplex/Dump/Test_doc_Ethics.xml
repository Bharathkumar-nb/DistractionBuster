<?xml version="1.0"?><api batchcomplete=""><query><pages><page _idx="13659583" pageid="13659583" ns="0" title="Ethics of artificial intelligence"><revisions><rev contentformat="text/x-wiki" contentmodel="wikitext" xml:space="preserve">The '''ethics of artificial intelligence''' is the part of the [[ethics of technology]] specific to [[robot]]s and other [[Artificial Intelligence|artificially intelligent]] beings. It is typically{{citation needed|date=May 2015}} divided into [[roboethics]], a concern with the moral behavior of humans as they design, construct, use and treat artificially intelligent beings, and [[machine ethics]], which is concerned with the moral behavior of artificial moral agents (AMAs).

==Robot ethics==
{{Main article|Robot ethics}}

The term &quot;robot ethics&quot; (sometimes &quot;roboethics&quot;) refers to the morality of how humans design, construct, use and treat robots and other artificially intelligent beings.&lt;ref name=Veruggio2002&gt;{{cite journal | url = http://ethicbots.na.infn.it/meetings/kom/veruggio.pdf | title = The Roboethics Roadmap | page = 2 | author = Veruggio, Gianmarco | publisher = Scuola di Robotica | year = 2007 | accessdate = 28 March 2011}}&lt;/ref&gt; It considers both how artificially intelligent beings may be used to harm humans and how they may be used to benefit humans.

===Robot rights===
Robot rights are the moral obligations of society towards its machines, similar to [[human rights]] or [[animal rights]].&lt;ref&gt;{{cite journal | url = http://revistas.ucm.es/index.php/TEKN/article/view/49072 | title = Posthuman Rights: Dimensions of Transhuman Worlds | last = Evans | first = Woody | authorlink = Woody Evans | publisher = Universidad Complutense Madrid | year = 2015 | accessdate = 7 April 2016}}&lt;/ref&gt; These may include the right to life and liberty, freedom of thought and expression and equality before the law.&lt;ref&gt;
The American Heritage Dictionary of the English Language, Fourth Edition
&lt;/ref&gt;
The issue has been considered by the [[Institute for the Future]]&lt;ref&gt;
{{cite news | url=http://news.bbc.co.uk/2/hi/technology/6200005.stm | title=Robots could demand legal rights  |publisher=BBC News | date=December 21, 2006 | accessdate=January 3, 2010}}&lt;/ref&gt;
and by the [[United Kingdom|U.K.]] [[Department of Trade and Industry (United Kingdom)|Department of Trade and Industry]].&lt;ref name=TimesOnline&gt;
{{cite news | url=http://www.timesonline.co.uk/tol/news/uk/science/article1695546.ece | title=Human rights for robots? We're getting carried away |publisher=The Times of London| work=The Times Online | first=Mark | last=Henderson | date=April 24, 2007 | accessdate=May 2, 2010}}&lt;/ref&gt;

Experts disagree whether specific and detailed laws will be required soon or safely in the distant future.&lt;ref name=TimesOnline/&gt; Glenn McGee reports that sufficiently humanoid robots may appear by 2020.&lt;ref&gt;
{{cite web | url=http://www.the-scientist.com/2007/5/1/30/1/ | title=A Robot Code of Ethics | first=Glenn | last=McGee | publisher=The Scientist}}
&lt;/ref&gt; 
[[Ray Kurzweil]] sets the date at 2029.&lt;ref&gt;
{{Cite book | last=Kurzweil | first=Ray | author-link=Ray Kurzweil | year=2005 | title=[[The Singularity is Near]] | publisher=Penguin Books | isbn=0-670-03384-7 | ref=harv }}
&lt;/ref&gt;
Another group of scientists meeting in 2007 supposed that at least 50 years had to pass before any sufficiently advanced system would exist.&lt;ref&gt;[https://web.archive.org/web/20080522163926/http://www.independent.co.uk/news/science/the-big-question-should-the-human-race-be-worried-by-the-rise-of-robots-446107.html The Big Question: Should the human race be worried by the rise of robots?], Independent Newspaper, 
&lt;/ref&gt;

The rules for the 2003 [[Loebner Prize]] competition envisioned possibility of robots having rights of their own:
&lt;blockquote&gt;61. If, in any given year, a publicly available open source Entry entered by the University of Surrey or the Cambridge Center wins the Silver Medal or the Gold Medal, then the Medal and the Cash Award will be awarded to the body responsible for the development of that Entry. If no such body can be identified, or if there is disagreement among two or more claimants, the Medal and the Cash Award will be held in trust until such time as the Entry may legally possess, either in the United States of America or in the venue of the contest, the Cash Award and Gold Medal in its own right.&lt;ref&gt;[http://loebner03.hamill.co.uk/docs/LPC%20Official%20Rules%20v2.0.pdf Loebner Prize Contest Official Rules — Version 2.0] The competition was directed by [[David Hamill]] and the rules were developed by members of the Robitron Yahoo group.&lt;/ref&gt; &lt;/blockquote&gt;

===Threat to privacy===
[[Aleksandr Solzhenitsyn]]'s  ''[[The First Circle]]'' describes the use of [[speech recognition]] technology in the service of tyranny.&lt;ref&gt;{{Harv|McCorduck|2004|p=308}}&lt;/ref&gt; If an AI program exists that can [[natural language understanding|understand natural languages]] and speech (e.g. [[English language|English]]), then, with adequate processing power it could theoretically listen to every phone conversation and read every email in the world, understand them and report back to the program's operators exactly what is said and exactly who is saying it. An AI program like this could allow governments or other entities to efficiently suppress dissent and attack their enemies.

===Threat to human dignity===
{{Main article|Computer Power and Human Reason}}
[[Joseph Weizenbaum]] argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as any of these:
* A customer service representative (AI technology is already used today for telephone-based [[interactive voice response]] systems)
* A therapist (as was proposed by [[Kenneth Colby]] in the 1970s)
* A nursemaid for the elderly (as was reported by [[Pamela McCorduck]] in her book ''The Fifth Generation'')
* A soldier
* A judge
* A police officer
Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an &quot;atrophy of the human spirit that comes from thinking of ourselves as computers.&quot;&lt;ref name=MWZ&gt;[[Joseph Weizenbaum]], quoted in {{Harvnb|McCorduck|2004|pp=356, 374–376}}&lt;/ref&gt;

[[Pamela McCorduck]] counters that, speaking for women and minorities &quot;I'd rather take my chances with an impartial computer,&quot; pointing out that there are conditions where we would prefer to have automated  judges and police that have no personal agenda at all.&lt;ref name=MWZ/&gt; AI founder [[John McCarthy (computer scientist)|John McCarthy]] objects to the moralizing tone of Weizenbaum's critique. &quot;When moralizing is both vehement and vague, it invites authoritarian abuse,&quot; he writes.

[[Bill Hibbard]]&lt;ref name=&quot;hibbard 2014&quot;&gt;Hibbard, Bill (2014): [http://arxiv.org/abs/1411.1373 &quot;Ethical Artificial Intelligence&quot;].&lt;/ref&gt; writes that &quot;Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.&quot;

===Transparency and open source===
[[Bill Hibbard]] argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts.&lt;ref name=&quot;AGI-08a&quot;&gt;[http://www.ssec.wisc.edu/~billh/g/hibbard_agi_workshop.pdf Open Source AI.] Bill Hibbard. 2008 proceedings of the First Conference on Artificial General Intelligence, eds. Pei Wang, Ben Goertzel and Stan Franklin.&lt;/ref&gt; [[Ben Goertzel]] and David Hart created [[OpenCog]] as an [[open source]] framework for AI development.&lt;ref name=&quot;AGI-08b&quot;&gt;[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.621&amp;rep=rep1&amp;type=pdf OpenCog: A Software Framework for Integrative Artificial General Intelligence.] David Hart and Ben Goertzel. 2008 proceedings of the First Conference on Artificial General Intelligence, eds. Pei Wang, Ben Goertzel and Stan Franklin.&lt;/ref&gt;  [[OpenAI]] is a non-profit AI research company created by [[Elon Musk]], [[Sam Altman]] and others to develop open source AI beneficial to humanity.&lt;ref name=&quot;OpenAI&quot;&gt;[https://www.wired.com/2016/04/openai-elon-musk-sam-altman-plan-to-set-artificial-intelligence-free/ Inside OpenAI, Elon Musk’s Wild Plan to Set Artificial Intelligence Free] Cade Metz, Wired 27 April 2016.&lt;/ref&gt; There are numerous other open source AI developments.

==Weaponization of artificial intelligence==
Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.&lt;ref name=&quot;Call for debate on killer robots&quot;&gt;[http://news.bbc.co.uk/2/hi/technology/8182003.stm Call for debate on killer robots], By Jason Palmer, Science and technology reporter, BBC News, 8/3/09.&lt;/ref&gt;&lt;ref&gt;[https://www.wired.com/dangerroom/2009/08/robot-three-way-portends-autonomous-future/ Robot Three-Way Portends Autonomous Future], By David Axe wired.com, August 13, 2009.&lt;/ref&gt; The US Navy has funded a report which indicates that as [[military robots]] become more complex, there should be greater attention to implications of their ability to make autonomous decisions.&lt;ref&gt;[http://www.dailytech.com/New%20Navyfunded%20Report%20Warns%20of%20War%20Robots%20Going%20Terminator/article14298.htm New Navy-funded Report Warns of War Robots Going &quot;Terminator&quot;], by Jason Mick (Blog), dailytech.com, February 17, 2009.&lt;/ref&gt;&lt;ref&gt;[http://www.engadget.com/2009/02/18/navy-report-warns-of-robot-uprising-suggests-a-strong-moral-com/ Navy report warns of robot uprising, suggests a strong moral compass], by Joseph L. Flatley engadget.com, Feb 18th 2009.&lt;/ref&gt; One researcher states that [[autonomous robot]]s might be more humane, as they could make decisions more effectively.

Within this last decade, there has been intensive research in autonomous power with the ability to learn using assigned moral responsibilities. &quot;The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.&quot;&lt;ref&gt;http://search.proquest.com/docview/1372020233&lt;/ref&gt; From a [[Consequentialism|consequentialist]] view, there is a chance that robots will develop the ability to make their own logical decisions on who to kill and that is why there should be a set moral framework that the A.I cannot override.

There has been a recent outcry with regards to the engineering of artificial-intelligence weapons and has even fostered up ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop [[Unmanned combat aerial vehicle|autonomous drone weapons]], paralleling similar announcements by Russia and Korea respectively. 
Due to the potential of AI weapons becoming more dangerous than human operated weapons, [[Stephen Hawking]] and [[Max Tegmark]] have signed a Future of Life petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.&lt;ref name=&quot;theatlantic.com&quot;&gt;{{cite web|url=https://www.theatlantic.com/technology/archive/2015/08/humans-not-robots-are-the-real-reason-artificial-intelligence-is-scary/400994/|title=Why Artificial Intelligence Can Too Easily Be Weaponized - The Atlantic|author=Zach Musgrave and Bryan W. Roberts|work=The Atlantic}}&lt;/ref&gt;

&quot;If any major military power pushes ahead with the AI weapon development, a global [[arms race]] is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikov's of tomorrow&quot;, says the petition, which includes [[Skype]] co-founder [[Jaan Tallinn]] and MIT professor of linguistics [[Noam Chomsky]] as additional supporters against AI weaponry.&lt;ref&gt;{{cite web|url=http://blogs.wsj.com/digits/2015/07/27/musk-hawking-warn-of-artificial-intelligence-weapons/|title=Musk, Hawking Warn of Artificial Intelligence Weapons|author=Cat Zakrzewski|work=WSJ}}&lt;/ref&gt;

Physicist and Astronomer Royal [[Sir Martin Rees]] warned of catastrophic instances like &quot;dumb robots going rogue or a network that develops a mind of its own.&quot; [[Huw Price]], a colleague of Rees at Cambridge, has voiced a similar warning that humans may not survive when intelligence &quot;escapes the constraints of biology.&quot; These two professors created the [[Centre for the Study of Existential Risk]] at Cambridge University in the hopes of avoiding this threat to human existence.&lt;ref name=&quot;theatlantic.com&quot;/&gt;

Regarding the potential for smarter-than-human systems to be employed militarily, the [[Open Philanthropy Project]] writes that this scenario &quot;seem potentially as important as the risks related to loss of control&quot;, but that research organizations investigating AI's long-run social impact have spent relatively little time on this concern: &quot;this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the [[Machine Intelligence Research Institute]] (MIRI) and the [[Future of Humanity Institute]] (FHI), and there seems to have been less analysis and debate regarding them&quot;.&lt;ref name=&quot;givewell&quot;&gt;{{cite report |author=GiveWell |authorlink=GiveWell |coauthors= |date=2015 |title=Potential risks from advanced artificial intelligence |url=http://www.givewell.org/labs/causes/ai-risk |publisher= |page= |docket= |accessdate=11 October 2015 |quote= }}&lt;/ref&gt;

==Machine ethics==
{{Main article|Machine ethics}}
Machine ethics (or machine morality) is the field of research concerned with designing [[Moral agency#Artificial moral agents|Artificial Moral Agents]] (AMAs), robots or artificially intelligent computers that behave morally or as though moral.&lt;ref name=Andersonweb&gt;{{cite web | url= http://uhaweb.hartford.edu/anderson/MachineEthics.html | title= Machine Ethics | last= Anderson |accessdate=27 June 2011}}&lt;/ref&gt;&lt;ref name=Anderson2011&gt;{{Cite book |editor1-first= Michael |editor1-last= Anderson | editor2-first= Susan Leigh |editor2-last= Anderson  |date=July 2011 | title = Machine Ethics | isbn = 978-0-521-11235-2 | publisher = [[Cambridge University Press]] | ref = harv}}&lt;/ref&gt;&lt;ref name=Anderson2006&gt;{{cite journal |date=July–August 2006 |title= Special Issue on Machine Ethics |journal= IEEE Intelligent Systems |volume= 21 |issue= 4 |pages= 10–63 |editor1-first= Michael |editor1-last= Anderson | editor2-first= Susan Leigh |editor2-last= Anderson |issn= 1541-1672 |url= http://www.computer.org/portal/web/csdl/abs/mags/ex/2006/04/x4toc.htm |doi=10.1109/mis.2006.70}}&lt;/ref&gt;&lt;ref name=Anderson2007&gt;{{cite journal  | first1= Michael | last1= Anderson | first2= Susan Leigh | last2= Anderson |date=Winter 2007 |title= Machine Ethics: Creating an Ethical Intelligent Agent | journal= AI Magazine |volume= 28 |issue= 4 |pages= 15–26 |publisher= American Association for Artificial Intelligence |issn= 0738-4602 |url= http://www.aaai.org/ojs/index.php/aimagazine/article/view/2065 }}&lt;/ref&gt;

[[Isaac Asimov]] considered the issue in the 1950s in his ''[[I, Robot]]''.  At the insistence of his editor [[John W. Campbell Jr.]], he proposed the [[Three Laws of Robotics]] to govern artificially intelligent systems.  Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.&lt;ref name=Asimov2008&gt;{{Cite book | last = Asimov | first =  Isaac | year = 2008 | title = [[I, Robot]] | isbn = 0-553-38256-X | publisher = Bantam  | location = New York | ref= harv}}&lt;/ref&gt;

In 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of [[Lausanne]] in [[Switzerland]], robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.&lt;ref&gt;[http://www.popsci.com/scitech/article/2009-08/evolving-robots-learn-lie-hide-resources-each-other Evolving Robots Learn To Lie To Each Other], Popular Science, August 18, 2009&lt;/ref&gt; One problem in this case may have been that the goals were &quot;terminal&quot; (i.e. in contrast, ultimate human motives typically have a quality of requiring never-ending learning).&lt;ref name=SantosLang2002&gt;{{cite web | url= http://santoslang.wordpress.com/article/ethics-for-artificial-intelligences-3iue30fi4gfq9-1 | title= Ethics for Artificial Intelligences | first= Chris | last= Santos-Lang | year= 2002}}&lt;/ref&gt;

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.&lt;ref name=&quot;Call for debate on killer robots&quot;/&gt; The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.&lt;ref&gt;[http://www.dailytech.com/New%20Navyfunded%20Report%20Warns%20of%20War%20Robots%20Going%20Terminator/article14298.htm Science New Navy-funded Report Warns of War Robots Going &quot;Terminator&quot;], by Jason Mick (Blog), dailytech.com, February 17, 2009.&lt;/ref&gt;&lt;ref&gt;[http://www.engadget.com/2009/02/18/navy-report-warns-of-robot-uprising-suggests-a-strong-moral-com/ Navy report warns of robot uprising, suggests a strong moral compass], by Joseph L. Flatley  engadget.com, Feb 18th 2009.&lt;/ref&gt; The President of the [[Association for the Advancement of Artificial Intelligence]] has commissioned a study to look at this issue.&lt;ref&gt;[http://research.microsoft.com/en-us/um/people/horvitz/AAAI_Presidential_Panel_2008-2009.htm AAAI Presidential Panel on Long-Term AI Futures 2008-2009 Study], Association for the Advancement of Artificial Intelligence, Accessed 7/26/09.&lt;/ref&gt; They point to programs like the [[Language Acquisition Device (computer)|Language Acquisition Device]] which can emulate human interaction.

[[Vernor Vinge]] has suggested that a moment may come when some computers are smarter than humans. He calls this &quot;[[Technological singularity|the Singularity]].&quot;&lt;ref name=&quot;nytimes july09&quot;/&gt;  He suggests that it may be somewhat or possibly very dangerous for humans.&lt;ref&gt;[http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html The Coming Technological Singularity: How to Survive in the Post-Human Era], by Vernor Vinge, Department of Mathematical Sciences, San Diego State University, (c) 1993 by Vernor Vinge.&lt;/ref&gt; This is discussed by a philosophy called [[Singularitarianism]]. The [[Machine Intelligence Research Institute]] has suggested a need to build &quot;[[Friendly AI]]&quot;, meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.&lt;ref&gt;[http://www.asimovlaws.com/articles/archives/2004/07/why_we_need_fri_1.html Article at Asimovlaws.com] {{webarchive |url=https://web.archive.org/web/20120524150856/http://www.asimovlaws.com/articles/archives/2004/07/why_we_need_fri_1.html |date=May 24, 2012 }}, July 2004, accessed 7/27/09.&lt;/ref&gt;

In 2009, academics and technical experts attended a conference organized by the [[Association for the Advancement of Artificial Intelligence]] to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved &quot;cockroach intelligence.&quot; They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.&lt;ref name=&quot;nytimes july09&quot;&gt;[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&amp;ref=todayspaper Scientists Worry Machines May Outsmart Man] By JOHN MARKOFF, NY Times, July 26, 2009.&lt;/ref&gt;

However, there is one technology in particular that could truly bring the possibility of robots with moral competence to reality. In a [https://www.foreignaffairs.com/articles/2015-08-12/moral-code paper] on the acquisition of moral values by robots , Oxford University Professor [[Nayef Al-Rodhan]] mentions the case of [https://www.technologyreview.com/s/526506/neuromorphic-chips/ neuromorphic] (&quot;brainlike&quot;) chips, which aim to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Robots embedded with neuromorphic technology could learn and develop knowledge in a uniquely humanlike way. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit - or if they end up developing human 'weaknesses' as well: selfishness, a pro-survival attitude, hesitation etc.

In ''Moral Machines: Teaching Robots Right from Wrong'',&lt;ref name=Wallach2008&gt;{{Cite book | first1 = Wendell | last1 = Wallach  | first2 = Colin | last2 = Allen |date=November 2008 | title = Moral Machines: Teaching Robots Right from Wrong | isbn = 978-0-19-537404-9 | publisher = [[Oxford University Press]] | location = USA | ref = harv&lt;!--url=http://www.ttu.ee/public/m/mart-murdvee/Techno-Psy/Wallach_Allen_2008_Moral_Machines_-_Teaching_Robots_Right_from_Wrong.pdf--&gt;}}&lt;/ref&gt; Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern [[Normative ethics|normative theory]] and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific [[List of machine learning algorithms|learning algorithms]] to use in machines. [[Nick Bostrom]] and [[Eliezer Yudkowsky]] have argued for [[decision trees]] (such as [[ID3 algorithm|ID3]]) over [[Artificial neural network|neural networks]] and [[genetic algorithm]]s on the grounds that decision trees obey modern social norms of transparency and predictability (e.g. ''[[stare decisis]]''),&lt;ref&gt;{{cite web | url= http://www.nickbostrom.com/ethics/artificial-intelligence.pdf | title= The Ethics of Artificial Intelligence | first1= Nick | last1= Bostrom | authorlink1= Nick Bostrom | first2= Eliezer | last2= Yudkowsky | authorlink2= Eliezer Yudkowsky | year=2011 | format= PDF | work= Cambridge Handbook of Artificial Intelligence | publisher= [[Cambridge Press]]}}&lt;/ref&gt; while Chris Santos-Lang argued in the opposite direction on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal &quot;[[Hacker (programmer subculture)|hackers]]&quot;.&lt;ref name=SantosLang2002/&gt;

==Unintended consequences==
{{Further information|Existential risk from advanced artificial intelligence}}

Many researchers have argued that, by way of an &quot;intelligence explosion&quot; sometime in the 21st century, a self-improving AI could become so vastly more powerful than humans that we would not be able to stop it from achieving its goals.&lt;ref name=&quot;Muehlhauser, Luke 2012&quot;&gt;Muehlhauser, Luke, and Louie Helm. 2012. [https://intelligence.org/files/IE-ME.pdf &quot;Intelligence Explosion and Machine Ethics&quot;]. In Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric Steinhart. Berlin: Springer.&lt;/ref&gt;
In his paper ''Ethical Issues in Advanced Artificial Intelligence'', the Oxford philosopher [[Nick Bostrom]] even argues that artificial intelligence has the capability to bring about human extinction. He claims that general super-intelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the super-intelligence to specify its original motivations.  In theory, a super-intelligent AI  would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its top goal, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.&lt;ref name=&quot;Bostrom, Nick 2003&quot;&gt;Bostrom, Nick. 2003. [http://www.nickbostrom.com/ethics/ai.html &quot;Ethical Issues in Advanced Artificial Intelligence&quot;]. In Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, edited by Iva Smit and George E. Lasker, 12–17. Vol. 2. Windsor, ON: International Institute for Advanced Studies in Systems Research / Cybernetics.&lt;/ref&gt;

However, the sheer complexity of human value systems makes it very difficult to make AI's motivations human-friendly.&lt;ref name=&quot;Muehlhauser, Luke 2012&quot;/&gt;&lt;ref name=&quot;Bostrom, Nick 2003&quot;/&gt; Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not &quot;common sense&quot;. According to [[Eliezer Yudkowsky]], there is little reason to suppose that an artificially designed mind would have such an adaptation.&lt;ref&gt;Yudkowsky, Eliezer. 2011. [https://intelligence.org/files/ComplexValues.pdf &quot;Complex Value Systems in Friendly AI&quot;]. In Schmidhuber, Thórisson, and Looks 2011, 388–393.&lt;/ref&gt;

[[Bill Hibbard]]&lt;ref name=&quot;hibbard 2014&quot;/&gt; proposes an AI design that avoids several types of unintended AI behavior including self-delusion, unintended instrumental actions, and corruption of the reward generator.

Instead of overwhelming the human race and leading to our destruction [[Nick Bostrom]] believes that Super Intelligence can help us solve many difficult problems such as disease, poverty, and environmental destruction, and could help us to “enhance” ourselves.&lt;ref&gt;{{Cite web|title = Sure, Artificial Intelligence May End Our World, But That Is Not the Main Problem|url = https://www.wired.com/2014/12/armageddon-is-not-the-ai-problem/|website = WIRED|accessdate = 2015-11-04|language = en-US}}&lt;/ref&gt;

==Ethics of artificial intelligence in fiction==
{{Main article|Artificial intelligence in fiction}}
The movie ''[[The Thirteenth Floor]]'' suggests a future where [[simulated reality|simulated worlds]] with sentient inhabitants are created by computer [[game console]]s for the purpose of entertainment. The movie ''[[The Matrix]]'' suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost [[Speciesism]]. The short story &quot;[[The Planck Dive]]&quot; suggest a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the [[Emergency Medical Hologram]] of ''[[USS Voyager (NCC-74656)|Starship Voyager]]'', which is an apparently sentient copy of a reduced subset of the consciousness of its creator, [[Lewis Zimmerman|Dr. Zimmerman]], who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies ''[[Bicentennial Man]]'' and ''[[A.I. (film)|A.I.]]'' deal with the possibility of sentient robots that could love. ''[[I, Robot (film)|I, Robot]]'' explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.{{citation needed|date=May 2015}}

The ethics of artificial intelligence is one of several core themes in BioWare's [[Mass Effect]] series of games.{{citation needed|date=May 2015}} It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale [[neural network]]. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.

Over time, debates have tended to focus less and less on ''possibility'' and more on ''desirability'',{{citation needed|date=May 2015}} as emphasized in the [[Hugo de Garis#The Artilect War|&quot;Cosmist&quot; and &quot;Terran&quot; debates]] initiated by [[Hugo de Garis]] and [[Kevin Warwick]]. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.

== Literature ==
The standard bibliography on ethics of AI is on [http://philpapers.org/browse/ethics-of-artificial-intelligence/ PhilPapers]. A recent collection is V.C. Müller(ed.) (2016).&lt;ref&gt;{{Cite web|url=http://philpapers.org/rec/MLLROA-3|title=Risks of artificial intelligence|date=2016|publisher=CRC Press - Chapman &amp; Hall|last=Müller|first=Vincent C.}}&lt;/ref&gt;

==See also==
{{Portal|Artificial intelligence|Ethics}}

*[[Artificial consciousness]]
*[[Artificial general intelligence]]
*[[Effective altruism#Far future and global catastrophic risks|Effective altruism, the far future and global catastrophic risks]]
*[[Existential risk]]
*[[Existential risk from advanced artificial intelligence]]
*[[Philosophy of artificial intelligence]]
*[[Superintelligence]]
**''[[Superintelligence: Paths, Dangers, Strategies]]''

; Researchers
*[[Nick Bostrom]]
*[[Ray Kurzweil]]
*[[Peter Norvig]]
*[[Steve Omohundro]]
*[[Stuart J. Russell]]
*[[Anders Sandberg]]
*[[Eliezer Yudkowsky]]

; Organisations
*[[Centre for the Study of Existential Risk]]
*[[Future of Humanity Institute]]
*[[Future of Life Institute]]
*[[Machine Intelligence Research Institute]]
*[[Partnership on AI]]

==Notes==
{{Reflist|2}}

==External links==
* [http://www.nature.com/news/robotics-ethics-of-artificial-intelligence-1.17611 Robotics: Ethics of artificial intelligence]. &quot;Four leading researchers share their concerns and solutions for reducing societal risks from intelligent machines.&quot; ''[[Nature (journal)|Nature]],''     521,    415–418    (28 May 2015)    doi:10.1038/521415a
* [http://www.aaai.org/aitopics/pmwiki/pmwiki.php/AITopics/Ethics: Artificial Intelligence Topics in Ethics]
* [http://www.cc.gatech.edu/ai/robot-lab/online-publications/formalizationv35.pdf Governing Lethal Behavior: Embedding Ethics in a Hybrid Deliberative/Reactive Robot Architecture]
* [http://www.shawnkilmer.com/Kilmer-PhilosophyOfAI.pdf Research Paper: Philosophy of Consciousness and Ethics in Artificial Intelligence]
* [http://www.asimovlaws.com 3 Laws Unsafe Campaign - Asimov's Laws &amp; I, Robot]
* [http://news.bbc.co.uk/1/hi/sci/tech/1809769.stm BBC News: Games to take on a life of their own]
* [http://www.dasboot.org/thorisson.htm Who's Afraid of Robots?], an article on humanity's fear of artificial intelligence.
* [https://web.archive.org/web/20080418122849/http://www.southernct.edu/organizations/rccs/resources/research/introduction/bynum_shrt_hist.html A short history of computer ethics]
* [http://www.aspcr.com ASPCR - The American Society for the Prevention of Cruelty To Robots]

{{Philosophy of science}}
{{Ethics}}
{{Technology}}

[[Category:Philosophy of artificial intelligence]]
[[Category:Ethics of science and technology]]</rev></revisions></page></pages></query></api>