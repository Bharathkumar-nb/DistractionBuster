{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhara\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'ai', 'intelligence', 'exhibited', 'machines', 'science', 'field', 'ai', 'research']\n",
      "['association', 'computing', 'machinery', 'acm', 'international', 'learned', 'society', 'computing', 'founded', 'world']\n",
      "['user', 'interacts', 'application', 'software', 'typical', 'desktop', 'application', 'software', 'layer', 'interfaces']\n",
      "['link', 'programming', 'language', 'theory', 'link', 'computational', 'complexity', 'theory', 'link', 'graphics']\n",
      "['computational', 'linguistics', 'field', 'concerned', 'statistical', 'rule', 'based', 'modeling', 'natural', 'language']\n",
      "['language', 'hello', 'world', 'source', 'code', 'known', 'hello', 'world', 'snippet', 'seminal']\n",
      "['computational', 'chemistry', 'branch', 'chemistry', 'uses', 'simulation', 'assist', 'solving', 'chemical', 'problems']\n",
      "['diagram', 'complexity', 'classes', 'provided', 'np', 'existence', 'problems', 'np', 'outside', 'np']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(125701 unique tokens: ['fractionibus', 'michiyo', 'veṇvāroha', 'daikichi', 'ecstatic']...) from 3919 documents (total 2112796 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.7 s\n",
      "Dictionary(125701 unique tokens: ['fractionibus', 'michiyo', 'veṇvāroha', 'daikichi', 'ecstatic']...)\n"
     ]
    }
   ],
   "source": [
    "doc_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\AllTopics.xml'\n",
    "stream = iter_wiki(doc_path)\n",
    "\n",
    "for tokens in itertools.islice(iter_wiki(doc_path), 8):\n",
    "    print (tokens[:10])\n",
    "doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 111000 tokens: [('source', 395), ('superhumanly', 2), ('culture', 404), ('known', 1244), ('level', 484), ('bowel', 2), ('devalues', 2), ('beginning', 407), ('sensorimotor', 3), ('fodor', 4)]...\n",
      "INFO : keeping 14701 tokens which were in no less than 10 and no more than 391 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(14701 unique tokens: ['worldview', 'desert', 'slow', 'army', 'captain']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14701 unique tokens: ['worldview', 'desert', 'slow', 'army', 'captain']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "print(id2word_wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "# create a stream of bag-of-words vectors\n",
    "wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "vector = next(iter(wiki_corpus))\n",
    "# print(vector)  # print the first vector in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai 136\n",
      "detection\n",
      "[(3096, 136), (4793, 107), (12118, 81)]\n"
     ]
    }
   ],
   "source": [
    "# what is the most common word in that first article?\n",
    "most_index, most_count = max(vector, key=lambda x: x[1])\n",
    "print(id2word_wiki[most_index], most_count)\n",
    "\n",
    "print(id2word_wiki[68])\n",
    "\n",
    "import heapq\n",
    "print(heapq.nlargest(3, vector, key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : saving sparse matrix to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : saved 3919x14701 matrix, density=1.257% (724187/57613219)\n",
      "INFO : saving MmCorpus index to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "wiki_bow_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : accepted corpus with 3919 documents, 14701 features, 724187 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(3919 documents, 14701 features, 724187 non-zero entries)\n",
      "3919\n"
     ]
    }
   ],
   "source": [
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "print(mm_corpus)\n",
    "\n",
    "print(len([ x for x in iter(mm_corpus)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.3333333333333333\n",
      "INFO : using symmetric eta at 6.802258349772124e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 3 topics, 6 passes over the supplied corpus of 3919 documents, updating model once every 2000 documents, evaluating perplexity every 3919 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.003*\"film\" + 0.002*\"award\" + 0.002*\"computational\" + 0.002*\"prize\" + 0.002*\"genre\" + 0.002*\"narrative\" + 0.001*\"poet\" + 0.001*\"algorithm\" + 0.001*\"tales\" + 0.001*\"canto\"\n",
      "INFO : topic #1 (0.333): 0.004*\"conference\" + 0.003*\"film\" + 0.002*\"programming\" + 0.002*\"genre\" + 0.002*\"computational\" + 0.002*\"algorithm\" + 0.002*\"students\" + 0.002*\"machine\" + 0.001*\"tales\" + 0.001*\"code\"\n",
      "INFO : topic #2 (0.333): 0.003*\"computational\" + 0.002*\"conference\" + 0.002*\"engineering\" + 0.002*\"algorithm\" + 0.002*\"film\" + 0.001*\"programming\" + 0.001*\"latin\" + 0.001*\"acm\" + 0.001*\"million\" + 0.001*\"web\"\n",
      "INFO : topic diff=1.027833, rho=1.000000\n",
      "INFO : -9.294 per-word bound, 627.6 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 0, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.006*\"gospels\" + 0.004*\"greece\" + 0.003*\"px\" + 0.003*\"gr\" + 0.002*\"theorem\" + 0.002*\"france\" + 0.002*\"epistles\" + 0.002*\"museum\" + 0.002*\"film\" + 0.002*\"acts\"\n",
      "INFO : topic #1 (0.333): 0.003*\"logic\" + 0.003*\"composite\" + 0.003*\"theorem\" + 0.002*\"proof\" + 0.002*\"models\" + 0.002*\"notation\" + 0.002*\"algorithm\" + 0.002*\"conference\" + 0.002*\"machine\" + 0.002*\"film\"\n",
      "INFO : topic #2 (0.333): 0.007*\"born\" + 0.005*\"px\" + 0.005*\"usa\" + 0.003*\"algebra\" + 0.003*\"vector\" + 0.003*\"theorem\" + 0.002*\"germany\" + 0.002*\"groups\" + 0.002*\"matrix\" + 0.002*\"france\"\n",
      "INFO : topic diff=0.603230, rho=0.707107\n",
      "INFO : PROGRESS: pass 1, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.003*\"film\" + 0.002*\"gospels\" + 0.002*\"genre\" + 0.002*\"award\" + 0.002*\"narrative\" + 0.002*\"greece\" + 0.002*\"prize\" + 0.002*\"poet\" + 0.002*\"tales\" + 0.001*\"greek\"\n",
      "INFO : topic #1 (0.333): 0.004*\"conference\" + 0.003*\"algorithm\" + 0.003*\"programming\" + 0.002*\"logic\" + 0.002*\"models\" + 0.002*\"machine\" + 0.002*\"computational\" + 0.002*\"students\" + 0.002*\"code\" + 0.002*\"algorithms\"\n",
      "INFO : topic #2 (0.333): 0.006*\"born\" + 0.004*\"usa\" + 0.004*\"px\" + 0.003*\"computational\" + 0.003*\"engineering\" + 0.002*\"algebra\" + 0.002*\"algorithm\" + 0.002*\"million\" + 0.002*\"vector\" + 0.002*\"germany\"\n",
      "INFO : topic diff=0.315218, rho=0.502551\n",
      "INFO : -8.734 per-word bound, 425.7 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 1, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.005*\"gospels\" + 0.003*\"greece\" + 0.003*\"film\" + 0.002*\"genre\" + 0.002*\"gr\" + 0.002*\"france\" + 0.002*\"women\" + 0.002*\"british\" + 0.002*\"epistles\" + 0.002*\"award\"\n",
      "INFO : topic #1 (0.333): 0.003*\"logic\" + 0.003*\"proof\" + 0.003*\"theorem\" + 0.003*\"algorithm\" + 0.003*\"composite\" + 0.003*\"models\" + 0.002*\"notation\" + 0.002*\"conference\" + 0.002*\"machine\" + 0.002*\"probability\"\n",
      "INFO : topic #2 (0.333): 0.010*\"born\" + 0.009*\"px\" + 0.007*\"usa\" + 0.004*\"theorem\" + 0.004*\"algebra\" + 0.004*\"vector\" + 0.003*\"groups\" + 0.003*\"matrix\" + 0.003*\"germany\" + 0.003*\"notation\"\n",
      "INFO : topic diff=0.357038, rho=0.502551\n",
      "INFO : PROGRESS: pass 2, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"film\" + 0.002*\"genre\" + 0.002*\"gospels\" + 0.002*\"award\" + 0.002*\"narrative\" + 0.002*\"tales\" + 0.002*\"greece\" + 0.002*\"king\" + 0.002*\"poet\" + 0.002*\"women\"\n",
      "INFO : topic #1 (0.333): 0.004*\"conference\" + 0.003*\"algorithm\" + 0.003*\"computational\" + 0.003*\"programming\" + 0.003*\"models\" + 0.003*\"logic\" + 0.002*\"students\" + 0.002*\"machine\" + 0.002*\"proof\" + 0.002*\"theorem\"\n",
      "INFO : topic #2 (0.333): 0.008*\"born\" + 0.007*\"px\" + 0.006*\"usa\" + 0.004*\"algebra\" + 0.003*\"vector\" + 0.003*\"theorem\" + 0.003*\"computational\" + 0.003*\"groups\" + 0.003*\"germany\" + 0.003*\"engineering\"\n",
      "INFO : topic diff=0.250274, rho=0.449036\n",
      "INFO : -8.595 per-word bound, 386.6 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 2, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"gospels\" + 0.003*\"film\" + 0.003*\"greece\" + 0.002*\"genre\" + 0.002*\"women\" + 0.002*\"france\" + 0.002*\"gr\" + 0.002*\"british\" + 0.002*\"narrative\" + 0.002*\"award\"\n",
      "INFO : topic #1 (0.333): 0.003*\"logic\" + 0.003*\"proof\" + 0.003*\"algorithm\" + 0.003*\"models\" + 0.003*\"theorem\" + 0.003*\"composite\" + 0.003*\"conference\" + 0.002*\"machine\" + 0.002*\"programming\" + 0.002*\"students\"\n",
      "INFO : topic #2 (0.333): 0.011*\"born\" + 0.010*\"px\" + 0.007*\"usa\" + 0.005*\"theorem\" + 0.005*\"algebra\" + 0.004*\"vector\" + 0.004*\"matrix\" + 0.004*\"groups\" + 0.003*\"notation\" + 0.003*\"germany\"\n",
      "INFO : topic diff=0.306523, rho=0.449036\n",
      "INFO : PROGRESS: pass 3, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"film\" + 0.002*\"genre\" + 0.002*\"gospels\" + 0.002*\"award\" + 0.002*\"narrative\" + 0.002*\"king\" + 0.002*\"tales\" + 0.002*\"women\" + 0.002*\"greece\" + 0.002*\"poet\"\n",
      "INFO : topic #1 (0.333): 0.004*\"conference\" + 0.004*\"algorithm\" + 0.004*\"computational\" + 0.003*\"programming\" + 0.003*\"models\" + 0.003*\"logic\" + 0.003*\"students\" + 0.002*\"machine\" + 0.002*\"proof\" + 0.002*\"code\"\n",
      "INFO : topic #2 (0.333): 0.010*\"born\" + 0.009*\"px\" + 0.007*\"usa\" + 0.005*\"theorem\" + 0.004*\"algebra\" + 0.004*\"vector\" + 0.003*\"matrix\" + 0.003*\"groups\" + 0.003*\"germany\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.203103, rho=0.409633\n",
      "INFO : -8.539 per-word bound, 371.9 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 3, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"gospels\" + 0.003*\"film\" + 0.003*\"greece\" + 0.002*\"genre\" + 0.002*\"women\" + 0.002*\"france\" + 0.002*\"british\" + 0.002*\"gr\" + 0.002*\"narrative\" + 0.002*\"award\"\n",
      "INFO : topic #1 (0.333): 0.004*\"logic\" + 0.003*\"algorithm\" + 0.003*\"models\" + 0.003*\"proof\" + 0.003*\"conference\" + 0.003*\"composite\" + 0.003*\"computational\" + 0.002*\"programming\" + 0.002*\"students\" + 0.002*\"machine\"\n",
      "INFO : topic #2 (0.333): 0.011*\"born\" + 0.010*\"px\" + 0.008*\"usa\" + 0.006*\"theorem\" + 0.005*\"algebra\" + 0.004*\"vector\" + 0.004*\"matrix\" + 0.004*\"notation\" + 0.004*\"groups\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.261059, rho=0.409633\n",
      "INFO : PROGRESS: pass 4, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"film\" + 0.002*\"genre\" + 0.002*\"gospels\" + 0.002*\"award\" + 0.002*\"narrative\" + 0.002*\"women\" + 0.002*\"king\" + 0.002*\"greece\" + 0.002*\"tales\" + 0.002*\"poet\"\n",
      "INFO : topic #1 (0.333): 0.005*\"conference\" + 0.004*\"computational\" + 0.004*\"algorithm\" + 0.003*\"programming\" + 0.003*\"models\" + 0.003*\"students\" + 0.003*\"logic\" + 0.003*\"machine\" + 0.002*\"proof\" + 0.002*\"learning\"\n",
      "INFO : topic #2 (0.333): 0.010*\"born\" + 0.009*\"px\" + 0.008*\"usa\" + 0.006*\"theorem\" + 0.004*\"algebra\" + 0.004*\"vector\" + 0.004*\"matrix\" + 0.004*\"groups\" + 0.004*\"notation\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.170813, rho=0.379063\n",
      "INFO : -8.512 per-word bound, 364.9 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 4, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"gospels\" + 0.003*\"film\" + 0.003*\"greece\" + 0.002*\"genre\" + 0.002*\"women\" + 0.002*\"france\" + 0.002*\"narrative\" + 0.002*\"british\" + 0.002*\"award\" + 0.002*\"gr\"\n",
      "INFO : topic #1 (0.333): 0.004*\"logic\" + 0.004*\"conference\" + 0.003*\"models\" + 0.003*\"algorithm\" + 0.003*\"proof\" + 0.003*\"computational\" + 0.003*\"composite\" + 0.003*\"students\" + 0.003*\"programming\" + 0.003*\"machine\"\n",
      "INFO : topic #2 (0.333): 0.011*\"born\" + 0.010*\"px\" + 0.008*\"usa\" + 0.007*\"theorem\" + 0.005*\"algebra\" + 0.005*\"vector\" + 0.005*\"notation\" + 0.004*\"matrix\" + 0.004*\"groups\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.219747, rho=0.379063\n",
      "INFO : PROGRESS: pass 5, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.004*\"film\" + 0.002*\"genre\" + 0.002*\"gospels\" + 0.002*\"award\" + 0.002*\"narrative\" + 0.002*\"women\" + 0.002*\"greece\" + 0.002*\"king\" + 0.002*\"tales\" + 0.002*\"poet\"\n",
      "INFO : topic #1 (0.333): 0.005*\"conference\" + 0.004*\"computational\" + 0.004*\"algorithm\" + 0.003*\"programming\" + 0.003*\"models\" + 0.003*\"students\" + 0.003*\"logic\" + 0.003*\"machine\" + 0.002*\"engineering\" + 0.002*\"design\"\n",
      "INFO : topic #2 (0.333): 0.010*\"born\" + 0.010*\"px\" + 0.008*\"usa\" + 0.007*\"theorem\" + 0.005*\"algebra\" + 0.004*\"vector\" + 0.004*\"notation\" + 0.004*\"matrix\" + 0.004*\"groups\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.150805, rho=0.354452\n",
      "INFO : -8.497 per-word bound, 361.2 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 5, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.003*\"gospels\" + 0.003*\"film\" + 0.003*\"greece\" + 0.002*\"genre\" + 0.002*\"women\" + 0.002*\"france\" + 0.002*\"narrative\" + 0.002*\"british\" + 0.002*\"award\" + 0.002*\"gr\"\n",
      "INFO : topic #1 (0.333): 0.004*\"conference\" + 0.004*\"models\" + 0.004*\"logic\" + 0.003*\"algorithm\" + 0.003*\"computational\" + 0.003*\"students\" + 0.003*\"composite\" + 0.003*\"programming\" + 0.003*\"proof\" + 0.003*\"machine\"\n",
      "INFO : topic #2 (0.333): 0.011*\"born\" + 0.010*\"px\" + 0.008*\"theorem\" + 0.007*\"usa\" + 0.005*\"notation\" + 0.005*\"algebra\" + 0.005*\"vector\" + 0.004*\"matrix\" + 0.004*\"groups\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.185663, rho=0.354452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)  # use fewer documents during training, LDA is slow\n",
    "# ClippedCorpus new in gensim 0.10.1\n",
    "# copy&paste it from https://github.com/piskvorky/gensim/blob/0.10.1/gensim/utils.py#L467 if necessary (or upgrade your gensim)\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=3, id2word=id2word_wiki, passes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.333): 0.003*\"gospels\" + 0.003*\"film\" + 0.003*\"greece\" + 0.002*\"genre\" + 0.002*\"women\" + 0.002*\"france\" + 0.002*\"narrative\" + 0.002*\"british\" + 0.002*\"award\" + 0.002*\"gr\"\n",
      "INFO : topic #1 (0.333): 0.004*\"conference\" + 0.004*\"models\" + 0.004*\"logic\" + 0.003*\"algorithm\" + 0.003*\"computational\" + 0.003*\"students\" + 0.003*\"composite\" + 0.003*\"programming\" + 0.003*\"proof\" + 0.003*\"machine\"\n",
      "INFO : topic #2 (0.333): 0.011*\"born\" + 0.010*\"px\" + 0.008*\"theorem\" + 0.007*\"usa\" + 0.005*\"notation\" + 0.005*\"algebra\" + 0.005*\"vector\" + 0.004*\"matrix\" + 0.004*\"groups\" + 0.003*\"equations\"\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.print_topics()  # print a few most important words for each LDA topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 100.8841118872226, 1: 842.95956837281324, 2: 125.88467491466653}\n",
      "Centroid : ( 0.0938456854765 ,  0.784148435696 ,  0.117102023176 )\n",
      "{0: 1500.1150636054119, 1: 263.28190585544775, 2: 14.52272062251264}\n",
      "Centroid : ( 0.840400595857 ,  0.147496866025 ,  0.00813597793978 )\n",
      "{0: 115.72818486309917, 1: 319.27097212756991, 2: 620.47922085580171}\n",
      "Centroid : ( 0.109280627822 ,  0.301483448657 ,  0.58591050128 )\n",
      "[(0, 0.016995560656335944), (1, 0.98291475414915641)]\n",
      "[(0, 0.15233463669467232), (1, 0.84725772453144954)]\n",
      "[(0, 0.94624383725445882), (1, 0.048830811403771796)]\n",
      "[(0, 0.96525813627786394), (1, 0.033040514970716951)]\n",
      "[(1, 0.68621808255902161), (2, 0.31347616092018554)]\n",
      "[(2, 0.99053389457929131)]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "test1_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\CompSci.xml'\n",
    "test2_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Literature.xml'\n",
    "test3_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Mathematics.xml'\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_1 = [tokens for tokens in iter_wiki(test1_path)]\n",
    "part1 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "\n",
    "\n",
    "topic_dic = {0:0, 1:0, 2:0}\n",
    "\n",
    "for doc in part1:\n",
    "    for p in doc:\n",
    "        topic_dic[p[0]] += p[1]\n",
    "\n",
    "print(topic_dic)\n",
    "\n",
    "num_docs = len(part1)\n",
    "\n",
    "print(\"Centroid : (\", topic_dic[0]/num_docs, \", \", topic_dic[1]/num_docs, \", \", topic_dic[2]/num_docs, \")\")\n",
    "\n",
    "centroid_1 = [(x, topic_dic[x]/num_docs) for x in range(3)]\n",
    "\n",
    "topic_dic = {0:0, 1:0, 2:0}\n",
    "\n",
    "for doc in part2:\n",
    "    for p in doc:\n",
    "        topic_dic[p[0]] += p[1]\n",
    "\n",
    "print(topic_dic)\n",
    "\n",
    "num_docs = len(part2)\n",
    "\n",
    "print(\"Centroid : (\", topic_dic[0]/num_docs, \", \", topic_dic[1]/num_docs, \", \", topic_dic[2]/num_docs, \")\")\n",
    "\n",
    "centroid_2 = [(x, topic_dic[x]/num_docs) for x in range(3)]\n",
    "\n",
    "topic_dic = {0:0, 1:0, 2:0}\n",
    "\n",
    "for doc in part3:\n",
    "    for p in doc:\n",
    "        topic_dic[p[0]] += p[1]\n",
    "\n",
    "print(topic_dic)\n",
    "\n",
    "num_docs = len(part3)\n",
    "\n",
    "print(\"Centroid : (\", topic_dic[0]/num_docs, \", \", topic_dic[1]/num_docs, \", \", topic_dic[2]/num_docs, \")\")\n",
    "\n",
    "centroid_3 = [(x, topic_dic[x]/num_docs) for x in range(3)]\n",
    "\n",
    "print(part1[0])\n",
    "print(part1[1])\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_2 = [tokens for tokens in iter_wiki(test2_path)]\n",
    "part2 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "print(part2[0])\n",
    "print(part2[1])\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_3 = [tokens for tokens in iter_wiki(test3_path)]\n",
    "part3 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "print(part3[0])\n",
    "print(part3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.46647961193421716), (1, 0.40096047952909852), (2, 0.13255990853668426)]\n"
     ]
    }
   ],
   "source": [
    "all_tokens = set()\n",
    "for tokens in test_doc_1:\n",
    "    all_tokens |= set(tokens)\n",
    "\n",
    "print(lda_model[id2word_wiki.doc2bow(list(all_tokens))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(part1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.279016973723\n",
      "0.437059622703\n",
      "0.20983812019\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "test1_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Ethics.xml'\n",
    "test2_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Allegory.xml'\n",
    "test3_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_AI.xml'\n",
    "\n",
    "\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_1 = [tokens for tokens in iter_wiki(test1_path)]\n",
    "part1 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "#print(part1)\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_2 = [tokens for tokens in iter_wiki(test2_path)]\n",
    "part2 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_3 = [tokens for tokens in iter_wiki(test3_path)]\n",
    "part3 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.09384568547648614), (1, 0.78414843569564019), (2, 0.11710202317643398)]\n",
      "0.479728631933\n",
      "0.186654974109\n",
      "0.877208213632\n",
      "\n",
      "0.97161344392\n",
      "0.168003631847\n",
      "0.146674770281\n",
      "\n",
      "0.463191073284\n",
      "0.99249988287\n",
      "0.00953489320499\n"
     ]
    }
   ],
   "source": [
    "print(centroid_1)\n",
    "\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_3], part1)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_3], part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_3], part3)]))\n",
    "print()\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_1], part1)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_1], part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_1], part3)]))\n",
    "print()\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_2], part1)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_2], part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip([centroid_2], part3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache the transformed corpora to disk, for use in later notebooks\n",
    "wiki_tfidf_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_tfidf.mm'\n",
    "wiki_lsa_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_lsa.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_tfidf_path, tfidf_model[mm_corpus])\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_lsa_path, lsi_model[tfidf_model[mm_corpus]])\n",
    "# gensim.corpora.MmCorpus.serialize('./data/wiki_lda.mm', lda_model[mm_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part1 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "\n",
    "part2 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "part3 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
