{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhara\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'ai', 'intelligence', 'exhibited', 'machines', 'science', 'field', 'ai', 'research']\n",
      "['association', 'computing', 'machinery', 'acm', 'international', 'learned', 'society', 'computing', 'founded', 'world']\n",
      "['user', 'interacts', 'application', 'software', 'typical', 'desktop', 'application', 'software', 'layer', 'interfaces']\n",
      "['link', 'programming', 'language', 'theory', 'link', 'computational', 'complexity', 'theory', 'link', 'graphics']\n",
      "['computational', 'linguistics', 'field', 'concerned', 'statistical', 'rule', 'based', 'modeling', 'natural', 'language']\n",
      "['language', 'hello', 'world', 'source', 'code', 'known', 'hello', 'world', 'snippet', 'seminal']\n",
      "['computational', 'chemistry', 'branch', 'chemistry', 'uses', 'simulation', 'assist', 'solving', 'chemical', 'problems']\n",
      "['diagram', 'complexity', 'classes', 'provided', 'np', 'existence', 'problems', 'np', 'outside', 'np']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(125701 unique tokens: ['azimov', 'soibelman', 'nuovo', 'brit', 'clarify']...) from 3919 documents (total 2112796 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.5 s\n",
      "Dictionary(125701 unique tokens: ['azimov', 'soibelman', 'nuovo', 'brit', 'clarify']...)\n"
     ]
    }
   ],
   "source": [
    "doc_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\AllTopics.xml'\n",
    "stream = iter_wiki(doc_path)\n",
    "\n",
    "for tokens in itertools.islice(iter_wiki(doc_path), 8):\n",
    "    print (tokens[:10])\n",
    "doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 111000 tokens: [('beginning', 407), ('applied', 475), ('anytime', 8), ('outsource', 1), ('enormity', 1), ('st', 409), ('kinect', 3), ('case', 718), ('neats', 1), ('unanswered', 6)]...\n",
      "INFO : keeping 14701 tokens which were in no less than 10 and no more than 391 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(14701 unique tokens: ['anticipating', 'yoga', 'inherent', 'zeta', 'euclidean']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14701 unique tokens: ['anticipating', 'yoga', 'inherent', 'zeta', 'euclidean']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "print(id2word_wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "# create a stream of bag-of-words vectors\n",
    "wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "vector = next(iter(wiki_corpus))\n",
    "# print(vector)  # print the first vector in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai 136\n",
      "cyprus\n",
      "[(4462, 136), (11654, 107), (5247, 81)]\n"
     ]
    }
   ],
   "source": [
    "# what is the most common word in that first article?\n",
    "most_index, most_count = max(vector, key=lambda x: x[1])\n",
    "print(id2word_wiki[most_index], most_count)\n",
    "\n",
    "print(id2word_wiki[68])\n",
    "\n",
    "import heapq\n",
    "print(heapq.nlargest(3, vector, key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : saving sparse matrix to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : saved 3919x14701 matrix, density=1.257% (724187/57613219)\n",
      "INFO : saving MmCorpus index to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "wiki_bow_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : accepted corpus with 3919 documents, 14701 features, 724187 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(3919 documents, 14701 features, 724187 non-zero entries)\n",
      "3919\n"
     ]
    }
   ],
   "source": [
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "print(mm_corpus)\n",
    "\n",
    "print(len([ x for x in iter(mm_corpus)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.3333333333333333\n",
      "INFO : using symmetric eta at 6.802258349772124e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 3 topics, 6 passes over the supplied corpus of 3919 documents, updating model once every 2000 documents, evaluating perplexity every 3919 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.005*\"conference\" + 0.004*\"film\" + 0.002*\"computational\" + 0.002*\"award\" + 0.002*\"engineering\" + 0.001*\"poet\" + 0.001*\"king\" + 0.001*\"genre\" + 0.001*\"born\" + 0.001*\"million\"\n",
      "INFO : topic #1 (0.333): 0.002*\"conference\" + 0.002*\"computational\" + 0.002*\"narrative\" + 0.002*\"film\" + 0.002*\"award\" + 0.002*\"acm\" + 0.001*\"engineering\" + 0.001*\"programming\" + 0.001*\"learning\" + 0.001*\"criticism\"\n",
      "INFO : topic #2 (0.333): 0.003*\"computational\" + 0.003*\"algorithm\" + 0.002*\"genre\" + 0.002*\"programming\" + 0.002*\"film\" + 0.002*\"tales\" + 0.002*\"students\" + 0.001*\"machine\" + 0.001*\"algorithms\" + 0.001*\"code\"\n",
      "INFO : topic diff=1.032320, rho=1.000000\n",
      "INFO : -9.264 per-word bound, 614.7 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 0, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.011*\"born\" + 0.007*\"usa\" + 0.004*\"greece\" + 0.003*\"gospels\" + 0.003*\"france\" + 0.003*\"germany\" + 0.003*\"film\" + 0.002*\"england\" + 0.002*\"conference\" + 0.002*\"russia\"\n",
      "INFO : topic #1 (0.333): 0.003*\"gospels\" + 0.002*\"gr\" + 0.002*\"france\" + 0.002*\"greece\" + 0.002*\"uk\" + 0.002*\"narrative\" + 0.001*\"british\" + 0.001*\"equations\" + 0.001*\"institute\" + 0.001*\"music\"\n",
      "INFO : topic #2 (0.333): 0.006*\"px\" + 0.005*\"theorem\" + 0.003*\"notation\" + 0.003*\"proof\" + 0.003*\"algorithm\" + 0.003*\"algebra\" + 0.002*\"matrix\" + 0.002*\"vector\" + 0.002*\"groups\" + 0.002*\"logic\"\n",
      "INFO : topic diff=0.605476, rho=0.707107\n",
      "INFO : PROGRESS: pass 1, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.005*\"born\" + 0.004*\"film\" + 0.004*\"conference\" + 0.004*\"usa\" + 0.002*\"award\" + 0.002*\"france\" + 0.002*\"genre\" + 0.002*\"germany\" + 0.002*\"poet\" + 0.002*\"women\"\n",
      "INFO : topic #1 (0.333): 0.002*\"narrative\" + 0.002*\"students\" + 0.002*\"engineering\" + 0.001*\"conference\" + 0.001*\"institute\" + 0.001*\"film\" + 0.001*\"learning\" + 0.001*\"uk\" + 0.001*\"person\" + 0.001*\"gospels\"\n",
      "INFO : topic #2 (0.333): 0.004*\"algorithm\" + 0.004*\"px\" + 0.004*\"theorem\" + 0.003*\"computational\" + 0.002*\"proof\" + 0.002*\"notation\" + 0.002*\"logic\" + 0.002*\"algebra\" + 0.002*\"algorithms\" + 0.002*\"models\"\n",
      "INFO : topic diff=0.299544, rho=0.502551\n",
      "INFO : -8.610 per-word bound, 390.8 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 1, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.009*\"born\" + 0.006*\"usa\" + 0.004*\"gospels\" + 0.004*\"greece\" + 0.004*\"france\" + 0.003*\"film\" + 0.003*\"germany\" + 0.003*\"england\" + 0.002*\"conference\" + 0.002*\"women\"\n",
      "INFO : topic #1 (0.333): 0.002*\"narrative\" + 0.002*\"students\" + 0.002*\"gospels\" + 0.002*\"uk\" + 0.001*\"learning\" + 0.001*\"institute\" + 0.001*\"gr\" + 0.001*\"music\" + 0.001*\"comics\" + 0.001*\"design\"\n",
      "INFO : topic #2 (0.333): 0.006*\"px\" + 0.006*\"theorem\" + 0.004*\"notation\" + 0.004*\"algorithm\" + 0.003*\"proof\" + 0.003*\"algebra\" + 0.003*\"vector\" + 0.003*\"logic\" + 0.003*\"matrix\" + 0.002*\"groups\"\n",
      "INFO : topic diff=0.311865, rho=0.502551\n",
      "INFO : PROGRESS: pass 2, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.006*\"born\" + 0.004*\"film\" + 0.004*\"usa\" + 0.003*\"conference\" + 0.002*\"award\" + 0.002*\"france\" + 0.002*\"genre\" + 0.002*\"greece\" + 0.002*\"gospels\" + 0.002*\"germany\"\n",
      "INFO : topic #1 (0.333): 0.002*\"narrative\" + 0.002*\"students\" + 0.002*\"conference\" + 0.002*\"engineering\" + 0.002*\"learning\" + 0.002*\"institute\" + 0.001*\"person\" + 0.001*\"design\" + 0.001*\"film\" + 0.001*\"acm\"\n",
      "INFO : topic #2 (0.333): 0.005*\"algorithm\" + 0.005*\"px\" + 0.005*\"theorem\" + 0.004*\"computational\" + 0.003*\"notation\" + 0.003*\"proof\" + 0.003*\"logic\" + 0.003*\"algebra\" + 0.002*\"models\" + 0.002*\"vector\"\n",
      "INFO : topic diff=0.237886, rho=0.449036\n",
      "INFO : -8.531 per-word bound, 370.0 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 2, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.009*\"born\" + 0.006*\"usa\" + 0.005*\"gospels\" + 0.004*\"france\" + 0.004*\"greece\" + 0.003*\"film\" + 0.003*\"germany\" + 0.003*\"england\" + 0.002*\"women\" + 0.002*\"russia\"\n",
      "INFO : topic #1 (0.333): 0.003*\"students\" + 0.002*\"narrative\" + 0.002*\"learning\" + 0.002*\"conference\" + 0.002*\"institute\" + 0.001*\"music\" + 0.001*\"reader\" + 0.001*\"engineering\" + 0.001*\"person\" + 0.001*\"design\"\n",
      "INFO : topic #2 (0.333): 0.007*\"px\" + 0.006*\"theorem\" + 0.004*\"notation\" + 0.004*\"algorithm\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"matrix\" + 0.003*\"groups\"\n",
      "INFO : topic diff=0.256909, rho=0.449036\n",
      "INFO : PROGRESS: pass 3, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.006*\"born\" + 0.004*\"film\" + 0.004*\"usa\" + 0.003*\"france\" + 0.003*\"gospels\" + 0.002*\"award\" + 0.002*\"greece\" + 0.002*\"genre\" + 0.002*\"conference\" + 0.002*\"germany\"\n",
      "INFO : topic #1 (0.333): 0.003*\"conference\" + 0.003*\"students\" + 0.002*\"narrative\" + 0.002*\"engineering\" + 0.002*\"learning\" + 0.002*\"institute\" + 0.002*\"person\" + 0.002*\"design\" + 0.001*\"programming\" + 0.001*\"acm\"\n",
      "INFO : topic #2 (0.333): 0.005*\"px\" + 0.005*\"algorithm\" + 0.005*\"theorem\" + 0.004*\"computational\" + 0.003*\"notation\" + 0.003*\"proof\" + 0.003*\"logic\" + 0.003*\"algebra\" + 0.003*\"models\" + 0.003*\"vector\"\n",
      "INFO : topic diff=0.196438, rho=0.409633\n",
      "INFO : -8.499 per-word bound, 361.7 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 3, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.009*\"born\" + 0.006*\"usa\" + 0.005*\"gospels\" + 0.004*\"france\" + 0.004*\"greece\" + 0.003*\"film\" + 0.003*\"germany\" + 0.003*\"england\" + 0.002*\"women\" + 0.002*\"gr\"\n",
      "INFO : topic #1 (0.333): 0.003*\"students\" + 0.002*\"narrative\" + 0.002*\"conference\" + 0.002*\"learning\" + 0.002*\"institute\" + 0.002*\"engineering\" + 0.001*\"music\" + 0.001*\"design\" + 0.001*\"reader\" + 0.001*\"person\"\n",
      "INFO : topic #2 (0.333): 0.007*\"px\" + 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"matrix\" + 0.003*\"groups\"\n",
      "INFO : topic diff=0.216181, rho=0.409633\n",
      "INFO : PROGRESS: pass 4, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.006*\"born\" + 0.004*\"film\" + 0.004*\"usa\" + 0.003*\"france\" + 0.003*\"gospels\" + 0.002*\"award\" + 0.002*\"greece\" + 0.002*\"poet\" + 0.002*\"germany\" + 0.002*\"women\"\n",
      "INFO : topic #1 (0.333): 0.003*\"conference\" + 0.003*\"students\" + 0.002*\"narrative\" + 0.002*\"engineering\" + 0.002*\"learning\" + 0.002*\"programming\" + 0.002*\"institute\" + 0.002*\"design\" + 0.002*\"person\" + 0.002*\"web\"\n",
      "INFO : topic #2 (0.333): 0.006*\"px\" + 0.006*\"algorithm\" + 0.005*\"theorem\" + 0.004*\"computational\" + 0.003*\"notation\" + 0.003*\"proof\" + 0.003*\"logic\" + 0.003*\"algebra\" + 0.003*\"vector\" + 0.003*\"models\"\n",
      "INFO : topic diff=0.167215, rho=0.379063\n",
      "INFO : -8.484 per-word bound, 358.0 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 4, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.009*\"born\" + 0.006*\"usa\" + 0.005*\"gospels\" + 0.004*\"france\" + 0.004*\"greece\" + 0.003*\"film\" + 0.003*\"germany\" + 0.003*\"england\" + 0.002*\"women\" + 0.002*\"gr\"\n",
      "INFO : topic #1 (0.333): 0.003*\"students\" + 0.003*\"conference\" + 0.002*\"narrative\" + 0.002*\"learning\" + 0.002*\"institute\" + 0.002*\"engineering\" + 0.002*\"design\" + 0.001*\"reader\" + 0.001*\"music\" + 0.001*\"person\"\n",
      "INFO : topic #2 (0.333): 0.007*\"px\" + 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.004*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"matrix\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.184273, rho=0.379063\n",
      "INFO : PROGRESS: pass 5, at document #2000/3919\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.006*\"born\" + 0.004*\"film\" + 0.004*\"usa\" + 0.003*\"france\" + 0.003*\"gospels\" + 0.003*\"greece\" + 0.002*\"award\" + 0.002*\"poet\" + 0.002*\"germany\" + 0.002*\"women\"\n",
      "INFO : topic #1 (0.333): 0.003*\"conference\" + 0.003*\"students\" + 0.002*\"narrative\" + 0.002*\"engineering\" + 0.002*\"learning\" + 0.002*\"programming\" + 0.002*\"institute\" + 0.002*\"design\" + 0.002*\"computational\" + 0.002*\"web\"\n",
      "INFO : topic #2 (0.333): 0.006*\"px\" + 0.006*\"algorithm\" + 0.006*\"theorem\" + 0.004*\"computational\" + 0.004*\"notation\" + 0.003*\"proof\" + 0.003*\"logic\" + 0.003*\"algebra\" + 0.003*\"vector\" + 0.003*\"models\"\n",
      "INFO : topic diff=0.149292, rho=0.354452\n",
      "INFO : -8.475 per-word bound, 355.9 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 5, at document #3919/3919\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.333): 0.008*\"born\" + 0.005*\"usa\" + 0.005*\"gospels\" + 0.004*\"france\" + 0.004*\"greece\" + 0.003*\"film\" + 0.003*\"germany\" + 0.002*\"england\" + 0.002*\"women\" + 0.002*\"gr\"\n",
      "INFO : topic #1 (0.333): 0.003*\"students\" + 0.003*\"conference\" + 0.002*\"narrative\" + 0.002*\"learning\" + 0.002*\"institute\" + 0.002*\"engineering\" + 0.002*\"design\" + 0.001*\"programming\" + 0.001*\"reader\" + 0.001*\"person\"\n",
      "INFO : topic #2 (0.333): 0.007*\"px\" + 0.006*\"theorem\" + 0.005*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.004*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"matrix\" + 0.003*\"equations\"\n",
      "INFO : topic diff=0.160273, rho=0.354452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)  # use fewer documents during training, LDA is slow\n",
    "# ClippedCorpus new in gensim 0.10.1\n",
    "# copy&paste it from https://github.com/piskvorky/gensim/blob/0.10.1/gensim/utils.py#L467 if necessary (or upgrade your gensim)\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=3, id2word=id2word_wiki, passes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.100): 0.008*\"models\" + 0.007*\"computational\" + 0.005*\"programming\" + 0.004*\"equations\" + 0.004*\"distribution\" + 0.004*\"engineering\" + 0.004*\"design\" + 0.004*\"modeling\" + 0.003*\"code\" + 0.003*\"energy\"\n",
      "INFO : topic #1 (0.100): 0.054*\"px\" + 0.012*\"triangle\" + 0.010*\"regular\" + 0.008*\"circle\" + 0.007*\"cell\" + 0.007*\"square\" + 0.006*\"symmetry\" + 0.006*\"angle\" + 0.006*\"truncated\" + 0.006*\"tiling\"\n",
      "INFO : topic #2 (0.100): 0.008*\"students\" + 0.006*\"award\" + 0.005*\"women\" + 0.005*\"prize\" + 0.005*\"conference\" + 0.004*\"college\" + 0.004*\"http\" + 0.003*\"literacy\" + 0.003*\"tales\" + 0.003*\"www\"\n",
      "INFO : topic #3 (0.100): 0.012*\"notation\" + 0.011*\"composite\" + 0.006*\"prime\" + 0.005*\"arithmetic\" + 0.005*\"symbols\" + 0.005*\"base\" + 0.004*\"table\" + 0.004*\"decimal\" + 0.004*\"sequence\" + 0.004*\"symbol\"\n",
      "INFO : topic #4 (0.100): 0.013*\"theorem\" + 0.008*\"proof\" + 0.008*\"algorithm\" + 0.007*\"algebra\" + 0.007*\"logic\" + 0.006*\"matrix\" + 0.005*\"groups\" + 0.005*\"finite\" + 0.005*\"graph\" + 0.004*\"conjecture\"\n",
      "INFO : topic #5 (0.100): 0.028*\"born\" + 0.017*\"usa\" + 0.008*\"germany\" + 0.008*\"france\" + 0.007*\"england\" + 0.006*\"latin\" + 0.006*\"britain\" + 0.005*\"million\" + 0.005*\"la\" + 0.004*\"russia\"\n",
      "INFO : topic #6 (0.100): 0.003*\"political\" + 0.003*\"media\" + 0.002*\"inf\" + 0.002*\"cultural\" + 0.002*\"comic\" + 0.002*\"comics\" + 0.002*\"letters\" + 0.002*\"dante\" + 0.002*\"chinese\" + 0.002*\"street\"\n",
      "INFO : topic #7 (0.100): 0.006*\"narrative\" + 0.005*\"genre\" + 0.004*\"reader\" + 0.003*\"person\" + 0.003*\"criticism\" + 0.003*\"action\" + 0.002*\"aristotle\" + 0.002*\"readers\" + 0.002*\"speech\" + 0.002*\"genres\"\n",
      "INFO : topic #8 (0.100): 0.007*\"film\" + 0.003*\"tv\" + 0.003*\"released\" + 0.003*\"publishes\" + 0.002*\"king\" + 0.002*\"father\" + 0.002*\"love\" + 0.002*\"black\" + 0.002*\"son\" + 0.002*\"episode\"\n",
      "INFO : topic #9 (0.100): 0.037*\"gospels\" + 0.026*\"greece\" + 0.018*\"gr\" + 0.014*\"epistles\" + 0.012*\"france\" + 0.012*\"conference\" + 0.011*\"acts\" + 0.011*\"monastery\" + 0.009*\"uk\" + 0.008*\"pauline\"\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.print_topics()  # print a few most important words for each LDA topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : collecting document frequencies\n",
      "INFO : PROGRESS: processing document #0\n",
      "INFO : calculating IDF weights for 3919 documents and 14700 features (724187 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using serial LSI version on this node\n",
      "INFO : updating model with new documents\n",
      "INFO : preparing a new chunk of documents\n",
      "INFO : using 100 extra samples and 2 power iterations\n",
      "INFO : 1st phase: constructing (14701, 103) action matrix\n",
      "INFO : orthonormalizing (14701, 103) action matrix\n",
      "INFO : 2nd phase: running dense svd on (103, 3919) matrix\n",
      "INFO : computing the final decomposition\n",
      "INFO : keeping 3 factors (discarding 84.997% of energy spectrum)\n",
      "INFO : processed documents up to #3919\n",
      "INFO : topic #0(6.822): 0.129*\"conference\" + 0.110*\"computational\" + 0.102*\"theorem\" + 0.095*\"algorithm\" + 0.092*\"programming\" + 0.090*\"award\" + 0.089*\"born\" + 0.074*\"proof\" + 0.073*\"usa\" + 0.068*\"logic\"\n",
      "INFO : topic #1(5.062): -0.335*\"born\" + -0.279*\"usa\" + 0.214*\"theorem\" + 0.160*\"algorithm\" + -0.141*\"conference\" + 0.128*\"proof\" + 0.103*\"algebra\" + 0.100*\"notation\" + 0.099*\"vector\" + -0.098*\"germany\"\n",
      "INFO : topic #2(4.607): -0.533*\"born\" + -0.471*\"usa\" + -0.140*\"germany\" + -0.124*\"theorem\" + -0.109*\"russia\" + -0.096*\"france\" + 0.090*\"narrative\" + -0.089*\"britain\" + -0.087*\"algorithm\" + -0.087*\"soviet\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.44 s\n"
     ]
    }
   ],
   "source": [
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_tfidf.mm\n",
      "INFO : saving sparse matrix to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_tfidf.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : saved 3919x14701 matrix, density=1.257% (724187/57613219)\n",
      "INFO : saving MmCorpus index to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_tfidf.mm.index\n",
      "INFO : storing corpus in Matrix Market format to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_lsa.mm\n",
      "INFO : saving sparse matrix to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_lsa.mm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.28 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : saved 3919x3 matrix, density=100.000% (11757/11757)\n",
      "INFO : saving MmCorpus index to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_lsa.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.66 s\n"
     ]
    }
   ],
   "source": [
    "# cache the transformed corpora to disk, for use in later notebooks\n",
    "wiki_tfidf_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_tfidf.mm'\n",
    "wiki_lsa_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_lsa.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_tfidf_path, tfidf_model[mm_corpus])\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_lsa_path, lsi_model[tfidf_model[mm_corpus]])\n",
    "# gensim.corpora.MmCorpus.serialize('./data/wiki_lda.mm', lda_model[mm_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.100): 0.008*\"models\" + 0.007*\"computational\" + 0.005*\"programming\" + 0.004*\"equations\" + 0.004*\"distribution\" + 0.004*\"engineering\" + 0.004*\"design\" + 0.004*\"modeling\" + 0.003*\"code\" + 0.003*\"energy\"\n",
      "INFO : topic #1 (0.100): 0.054*\"px\" + 0.012*\"triangle\" + 0.010*\"regular\" + 0.008*\"circle\" + 0.007*\"cell\" + 0.007*\"square\" + 0.006*\"symmetry\" + 0.006*\"angle\" + 0.006*\"truncated\" + 0.006*\"tiling\"\n",
      "INFO : topic #2 (0.100): 0.008*\"students\" + 0.006*\"award\" + 0.005*\"women\" + 0.005*\"prize\" + 0.005*\"conference\" + 0.004*\"college\" + 0.004*\"http\" + 0.003*\"literacy\" + 0.003*\"tales\" + 0.003*\"www\"\n",
      "INFO : topic #3 (0.100): 0.012*\"notation\" + 0.011*\"composite\" + 0.006*\"prime\" + 0.005*\"arithmetic\" + 0.005*\"symbols\" + 0.005*\"base\" + 0.004*\"table\" + 0.004*\"decimal\" + 0.004*\"sequence\" + 0.004*\"symbol\"\n",
      "INFO : topic #4 (0.100): 0.013*\"theorem\" + 0.008*\"proof\" + 0.008*\"algorithm\" + 0.007*\"algebra\" + 0.007*\"logic\" + 0.006*\"matrix\" + 0.005*\"groups\" + 0.005*\"finite\" + 0.005*\"graph\" + 0.004*\"conjecture\"\n",
      "INFO : topic #5 (0.100): 0.028*\"born\" + 0.017*\"usa\" + 0.008*\"germany\" + 0.008*\"france\" + 0.007*\"england\" + 0.006*\"latin\" + 0.006*\"britain\" + 0.005*\"million\" + 0.005*\"la\" + 0.004*\"russia\"\n",
      "INFO : topic #6 (0.100): 0.003*\"political\" + 0.003*\"media\" + 0.002*\"inf\" + 0.002*\"cultural\" + 0.002*\"comic\" + 0.002*\"comics\" + 0.002*\"letters\" + 0.002*\"dante\" + 0.002*\"chinese\" + 0.002*\"street\"\n",
      "INFO : topic #7 (0.100): 0.006*\"narrative\" + 0.005*\"genre\" + 0.004*\"reader\" + 0.003*\"person\" + 0.003*\"criticism\" + 0.003*\"action\" + 0.002*\"aristotle\" + 0.002*\"readers\" + 0.002*\"speech\" + 0.002*\"genres\"\n",
      "INFO : topic #8 (0.100): 0.007*\"film\" + 0.003*\"tv\" + 0.003*\"released\" + 0.003*\"publishes\" + 0.002*\"king\" + 0.002*\"father\" + 0.002*\"love\" + 0.002*\"black\" + 0.002*\"son\" + 0.002*\"episode\"\n",
      "INFO : topic #9 (0.100): 0.037*\"gospels\" + 0.026*\"greece\" + 0.018*\"gr\" + 0.014*\"epistles\" + 0.012*\"france\" + 0.012*\"conference\" + 0.011*\"acts\" + 0.011*\"monastery\" + 0.009*\"uk\" + 0.008*\"pauline\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.14286066754093796), (4, 0.85569766503756939)]\n",
      "[(1, 0.018565152002948199), (5, 0.24466592756443808), (6, 0.27926708632394187), (7, 0.33510336781368494), (8, 0.12169899118677541)]\n",
      "[(0, 0.1017609445581847), (2, 0.11228334793916089), (7, 0.76585487779527406), (8, 0.019575307072415293)]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "test1_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_convexhull.xml'\n",
    "test2_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Allegory.xml'\n",
    "test3_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Ethics.xml'\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_1 = [tokens for tokens in iter_wiki(test1_path)]\n",
    "#print(lda_model.print_topics(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1])[0]))\n",
    "\n",
    "#for x in heapq.nlargest(3, id2word_wiki.doc2bow(test_doc_1[0]), key=lambda x: x[1]):\n",
    "#    print(x[0], id2word_wiki[x[0]], x[1])\n",
    "\n",
    "#print(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1]))\n",
    "#print(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1])[0])\n",
    "\n",
    "print(lda_model[id2word_wiki.doc2bow(test_doc_1[0])])\n",
    "\n",
    "lda_model.print_topics()\n",
    "\n",
    "part1 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "#print(part1)\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_2 = [tokens for tokens in iter_wiki(test2_path)]\n",
    "part2 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "print(lda_model[id2word_wiki.doc2bow(test_doc_2[0])])\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_3 = [tokens for tokens in iter_wiki(test3_path)]\n",
    "part3 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "print(lda_model[id2word_wiki.doc2bow(test_doc_3[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00593754613919\n",
      "0.0\n",
      "0.643950721481\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part1 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "\n",
    "part2 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "part3 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0(6.822): 0.129*\"conference\" + 0.110*\"computational\" + 0.102*\"theorem\" + 0.095*\"algorithm\" + 0.092*\"programming\" + 0.090*\"award\" + 0.089*\"born\" + 0.074*\"proof\" + 0.073*\"usa\" + 0.068*\"logic\"\n",
      "INFO : topic #1(5.062): -0.335*\"born\" + -0.279*\"usa\" + 0.214*\"theorem\" + 0.160*\"algorithm\" + -0.141*\"conference\" + 0.128*\"proof\" + 0.103*\"algebra\" + 0.100*\"notation\" + 0.099*\"vector\" + -0.098*\"germany\"\n",
      "INFO : topic #2(4.607): -0.533*\"born\" + -0.471*\"usa\" + -0.140*\"germany\" + -0.124*\"theorem\" + -0.109*\"russia\" + -0.096*\"france\" + 0.090*\"narrative\" + -0.089*\"britain\" + -0.087*\"algorithm\" + -0.087*\"soviet\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 15.968758136814536), (1, -1.7760275323027801), (2, 3.2291864003224635)]\n",
      "[(0, 8.1312391723860191), (1, -2.5683564885463861), (2, 4.5365893948980007)]\n",
      "[(0, 1.3649376216191202), (1, 1.5611714875775313), (2, -0.64951863492836948)]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "test1_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Ethics.xml'\n",
    "test2_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Allegory.xml'\n",
    "test3_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_AI.xml'\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_1 = [tokens for tokens in iter_wiki(test1_path)]\n",
    "#print(lda_model.print_topics(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1])[0]))\n",
    "\n",
    "#for x in heapq.nlargest(3, id2word_wiki.doc2bow(test_doc_1[0]), key=lambda x: x[1]):\n",
    "#    print(x[0], id2word_wiki[x[0]], x[1])\n",
    "\n",
    "#print(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1]))\n",
    "#print(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1])[0])\n",
    "\n",
    "print(lsi_model[id2word_wiki.doc2bow(test_doc_1[0])])\n",
    "\n",
    "lsi_model.print_topics()\n",
    "\n",
    "part1 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "#print(part1)\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_2 = [tokens for tokens in iter_wiki(test2_path)]\n",
    "part2 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "print(lsi_model[id2word_wiki.doc2bow(test_doc_2[0])])\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_3 = [tokens for tokens in iter_wiki(test3_path)]\n",
    "part3 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "print(lsi_model[id2word_wiki.doc2bow(test_doc_3[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00390474844191\n",
      "0.475280566786\n",
      "0.0258858146634\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = test1_path\n",
    "text2 = test2_path\n",
    "\n",
    "vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "print ('Cosine:', cosine)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
