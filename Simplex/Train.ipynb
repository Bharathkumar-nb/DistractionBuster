{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "\n",
    "def head(stream, n=10):\n",
    "    \"\"\"Convenience fnc: return the first `n` elements of the stream, as plain list.\"\"\"\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.utils import smart_open, simple_preprocess\n",
    "from gensim.corpora.wikicorpus import _extract_pages, filter_wiki\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from xml.etree.cElementTree import iterparse\n",
    "\n",
    "def my_extract_pages(f):\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "    page_tag = \"rev\"\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag and elem.text != None:\n",
    "            text = elem.text\n",
    "            yield text\n",
    "            elem.clear()\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_wiki(dump_file):\n",
    "    \"\"\"Yield each article from the Wikipedia dump, as a `(title, tokens)` 2-tuple.\"\"\"\n",
    "    ignore_namespaces = 'Wikipedia Category File Portal Template MediaWiki User Help Book Draft'.split()\n",
    "    for text in my_extract_pages(smart_open(dump_file)):\n",
    "        text = filter_wiki(text)\n",
    "        tokens = tokenize(text)\n",
    "        if len(tokens) < 50:\n",
    "            continue  # ignore short articles and various meta-articles\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'ai', 'intelligence', 'exhibited', 'machines', 'science', 'field', 'ai', 'research']\n",
      "['association', 'computing', 'machinery', 'acm', 'international', 'learned', 'society', 'computing', 'founded', 'world']\n",
      "['user', 'interacts', 'application', 'software', 'typical', 'desktop', 'application', 'software', 'layer', 'interfaces']\n",
      "['link', 'programming', 'language', 'theory', 'link', 'computational', 'complexity', 'theory', 'link', 'graphics']\n",
      "['computational', 'linguistics', 'field', 'concerned', 'statistical', 'rule', 'based', 'modeling', 'natural', 'language']\n",
      "['language', 'hello', 'world', 'source', 'code', 'known', 'hello', 'world', 'snippet', 'seminal']\n",
      "['computational', 'chemistry', 'branch', 'chemistry', 'uses', 'simulation', 'assist', 'solving', 'chemical', 'problems']\n",
      "['diagram', 'complexity', 'classes', 'provided', 'np', 'existence', 'problems', 'np', 'outside', 'np']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : built Dictionary(125701 unique tokens: ['governor', 'photographic', 'arima', 'ramona', 'micromechanical']...) from 3919 documents (total 2112796 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.3 s\n",
      "Dictionary(125701 unique tokens: ['governor', 'photographic', 'arima', 'ramona', 'micromechanical']...)\n"
     ]
    }
   ],
   "source": [
    "doc_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\AllTopics.xml'\n",
    "stream = iter_wiki(doc_path)\n",
    "\n",
    "for tokens in itertools.islice(iter_wiki(doc_path), 8):\n",
    "    print (tokens[:10])\n",
    "doc_stream = (tokens for tokens in iter_wiki(doc_path))\n",
    "%time id2word_wiki = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 0 tokens: []...\n",
      "INFO : keeping 14701 tokens which were in no less than 10 and no more than 391 (=10.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(14701 unique tokens: ['governor', 'photographic', 'elementary', 'revered', 'clash']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14701 unique tokens: ['governor', 'photographic', 'elementary', 'revered', 'clash']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_wiki.filter_extremes(no_below=10, no_above=0.1)\n",
    "print(id2word_wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 3), (25, 2), (26, 1), (34, 1), (48, 1), (53, 1), (75, 8), (85, 1), (99, 3), (107, 6), (108, 1), (112, 1), (124, 1), (127, 1), (139, 4), (143, 1), (153, 1), (154, 4), (171, 1), (175, 1), (188, 1), (233, 8), (234, 1), (240, 2), (241, 1), (253, 1), (268, 2), (284, 3), (285, 1), (289, 1), (294, 1), (295, 3), (307, 1), (317, 2), (324, 4), (332, 4), (337, 1), (345, 2), (347, 1), (353, 2), (355, 4), (359, 1), (367, 1), (375, 3), (381, 2), (384, 1), (408, 1), (411, 2), (416, 2), (423, 1), (424, 1), (429, 1), (430, 107), (453, 1), (456, 2), (459, 1), (476, 1), (480, 6), (481, 3), (491, 1), (493, 2), (506, 1), (507, 2), (508, 2), (513, 1), (514, 2), (522, 1), (526, 3), (542, 1), (546, 1), (548, 1), (553, 1), (562, 2), (564, 1), (565, 2), (567, 1), (585, 1), (588, 1), (608, 2), (627, 2), (641, 3), (651, 1), (657, 14), (664, 1), (668, 3), (672, 2), (688, 1), (690, 5), (693, 1), (705, 4), (722, 2), (734, 3), (741, 1), (756, 1), (766, 4), (787, 1), (797, 1), (822, 1), (827, 1), (830, 1), (833, 1), (835, 1), (841, 1), (846, 37), (848, 7), (850, 4), (862, 1), (867, 1), (880, 1), (887, 2), (890, 1), (893, 13), (905, 1), (906, 1), (915, 2), (925, 2), (937, 1), (940, 1), (941, 1), (954, 1), (961, 2), (967, 1), (968, 1), (974, 4), (985, 1), (989, 3), (991, 2), (995, 1), (1011, 1), (1020, 1), (1022, 1), (1029, 1), (1033, 1), (1039, 2), (1051, 1), (1057, 10), (1060, 3), (1071, 8), (1079, 2), (1085, 1), (1094, 3), (1101, 1), (1114, 33), (1121, 1), (1128, 23), (1139, 1), (1145, 6), (1149, 1), (1167, 1), (1168, 1), (1170, 1), (1171, 1), (1177, 1), (1181, 3), (1183, 1), (1191, 2), (1199, 1), (1207, 1), (1223, 3), (1236, 1), (1245, 1), (1254, 1), (1257, 1), (1259, 1), (1277, 1), (1305, 2), (1320, 1), (1325, 3), (1341, 3), (1350, 3), (1360, 6), (1386, 1), (1404, 1), (1410, 1), (1414, 1), (1419, 3), (1420, 1), (1427, 4), (1438, 1), (1443, 4), (1452, 2), (1464, 1), (1471, 1), (1479, 2), (1503, 1), (1516, 3), (1524, 1), (1527, 2), (1534, 1), (1541, 2), (1544, 1), (1552, 2), (1554, 1), (1555, 2), (1558, 2), (1563, 18), (1567, 6), (1586, 5), (1588, 7), (1592, 4), (1615, 1), (1617, 1), (1618, 19), (1627, 2), (1628, 2), (1634, 2), (1636, 1), (1649, 1), (1652, 1), (1673, 2), (1693, 1), (1713, 3), (1728, 1), (1736, 1), (1760, 1), (1777, 1), (1780, 1), (1783, 1), (1785, 1), (1788, 1), (1798, 1), (1826, 1), (1840, 2), (1864, 2), (1866, 3), (1870, 2), (1875, 1), (1883, 2), (1888, 2), (1892, 1), (1894, 2), (1897, 2), (1911, 3), (1914, 1), (1927, 5), (1946, 1), (1953, 1), (1960, 1), (1963, 1), (1974, 2), (1983, 2), (1988, 1), (2012, 1), (2015, 4), (2031, 1), (2032, 2), (2039, 1), (2042, 1), (2063, 3), (2067, 2), (2079, 1), (2090, 1), (2117, 2), (2118, 1), (2163, 7), (2185, 4), (2186, 2), (2187, 1), (2189, 1), (2194, 4), (2204, 1), (2209, 1), (2214, 7), (2223, 1), (2235, 1), (2238, 2), (2251, 1), (2252, 1), (2258, 5), (2265, 2), (2290, 1), (2297, 1), (2302, 1), (2303, 4), (2309, 1), (2312, 13), (2329, 1), (2331, 2), (2337, 3), (2345, 2), (2349, 2), (2352, 12), (2361, 1), (2371, 1), (2375, 7), (2398, 2), (2399, 1), (2405, 1), (2409, 1), (2480, 1), (2494, 3), (2501, 1), (2505, 4), (2514, 4), (2525, 3), (2530, 1), (2531, 1), (2533, 1), (2535, 1), (2545, 1), (2563, 4), (2570, 1), (2595, 3), (2601, 23), (2606, 1), (2616, 1), (2624, 1), (2629, 1), (2631, 5), (2636, 1), (2642, 1), (2649, 2), (2651, 1), (2654, 1), (2668, 5), (2671, 4), (2678, 2), (2683, 1), (2686, 1), (2696, 1), (2704, 3), (2711, 4), (2713, 1), (2717, 3), (2722, 1), (2723, 1), (2726, 1), (2727, 1), (2733, 1), (2734, 2), (2740, 1), (2745, 4), (2747, 1), (2750, 1), (2767, 1), (2776, 1), (2780, 1), (2790, 2), (2796, 2), (2800, 1), (2808, 1), (2814, 7), (2822, 1), (2837, 1), (2857, 2), (2860, 3), (2866, 2), (2870, 10), (2871, 3), (2875, 1), (2880, 3), (2890, 1), (2903, 2), (2917, 1), (2925, 1), (2940, 2), (2945, 3), (2946, 3), (2954, 1), (2957, 1), (2958, 1), (2968, 1), (2970, 2), (2979, 9), (2981, 2), (2984, 3), (2986, 1), (2991, 1), (2999, 1), (3003, 2), (3026, 1), (3034, 4), (3035, 3), (3054, 4), (3058, 1), (3064, 1), (3072, 1), (3080, 1), (3082, 3), (3083, 1), (3094, 2), (3126, 1), (3132, 1), (3144, 4), (3146, 1), (3147, 1), (3149, 1), (3160, 3), (3173, 3), (3177, 1), (3191, 3), (3192, 2), (3202, 1), (3217, 1), (3220, 1), (3234, 1), (3235, 1), (3244, 1), (3246, 2), (3252, 1), (3262, 2), (3265, 2), (3268, 1), (3304, 1), (3307, 2), (3309, 2), (3328, 1), (3339, 4), (3345, 1), (3346, 1), (3367, 1), (3377, 1), (3398, 1), (3399, 2), (3400, 1), (3405, 2), (3409, 1), (3410, 1), (3418, 3), (3422, 2), (3435, 5), (3438, 1), (3444, 1), (3454, 2), (3457, 6), (3458, 1), (3473, 1), (3482, 1), (3484, 2), (3486, 2), (3495, 5), (3500, 2), (3513, 1), (3518, 1), (3526, 1), (3550, 1), (3554, 1), (3559, 3), (3562, 1), (3571, 1), (3585, 1), (3587, 11), (3594, 1), (3600, 2), (3617, 1), (3618, 2), (3630, 1), (3639, 4), (3652, 2), (3656, 4), (3666, 1), (3669, 4), (3693, 2), (3697, 4), (3703, 4), (3705, 2), (3711, 3), (3722, 3), (3724, 1), (3738, 2), (3760, 1), (3763, 2), (3765, 3), (3775, 1), (3779, 6), (3785, 1), (3789, 2), (3801, 1), (3817, 1), (3826, 1), (3829, 1), (3840, 2), (3846, 2), (3847, 1), (3851, 1), (3859, 1), (3864, 3), (3866, 1), (3867, 1), (3874, 1), (3879, 1), (3884, 3), (3907, 1), (3915, 1), (3919, 1), (3922, 1), (3928, 1), (3938, 1), (3949, 2), (3951, 1), (3952, 1), (3956, 2), (3974, 10), (3976, 1), (4007, 1), (4012, 1), (4014, 1), (4024, 1), (4028, 10), (4041, 6), (4046, 3), (4065, 3), (4081, 1), (4091, 3), (4099, 2), (4100, 1), (4102, 4), (4108, 1), (4118, 2), (4132, 1), (4146, 4), (4153, 1), (4159, 1), (4166, 1), (4171, 1), (4176, 2), (4184, 1), (4204, 2), (4210, 1), (4251, 1), (4263, 3), (4268, 1), (4282, 6), (4289, 1), (4291, 2), (4295, 2), (4308, 1), (4320, 1), (4336, 1), (4337, 1), (4341, 1), (4349, 2), (4364, 1), (4377, 2), (4381, 1), (4398, 1), (4399, 3), (4403, 1), (4419, 1), (4428, 2), (4437, 1), (4444, 1), (4452, 1), (4457, 1), (4467, 1), (4474, 1), (4485, 1), (4488, 1), (4494, 1), (4508, 1), (4525, 1), (4539, 1), (4585, 1), (4590, 5), (4612, 4), (4623, 4), (4636, 1), (4657, 3), (4659, 1), (4660, 6), (4686, 12), (4691, 1), (4694, 3), (4702, 1), (4729, 2), (4736, 1), (4740, 1), (4751, 1), (4755, 1), (4769, 1), (4770, 1), (4775, 1), (4782, 2), (4791, 1), (4794, 3), (4802, 1), (4805, 1), (4816, 1), (4817, 1), (4834, 1), (4840, 67), (4841, 2), (4859, 3), (4868, 1), (4876, 1), (4880, 1), (4897, 1), (4898, 1), (4917, 1), (4920, 1), (4921, 2), (4931, 1), (4937, 3), (4942, 2), (4946, 2), (4952, 3), (4965, 2), (4966, 2), (4987, 1), (4993, 3), (4999, 1), (5007, 1), (5011, 1), (5015, 4), (5018, 3), (5023, 2), (5036, 1), (5067, 1), (5070, 1), (5079, 1), (5082, 1), (5107, 2), (5110, 2), (5121, 2), (5127, 1), (5134, 1), (5151, 1), (5154, 1), (5156, 1), (5159, 2), (5168, 2), (5173, 1), (5185, 6), (5188, 2), (5191, 2), (5198, 81), (5212, 1), (5224, 2), (5244, 5), (5247, 1), (5251, 1), (5269, 2), (5270, 1), (5271, 3), (5275, 1), (5282, 1), (5292, 5), (5293, 1), (5305, 1), (5311, 1), (5312, 1), (5314, 1), (5323, 2), (5334, 1), (5336, 7), (5337, 1), (5354, 1), (5359, 1), (5364, 1), (5379, 1), (5381, 11), (5382, 2), (5406, 1), (5409, 1), (5422, 2), (5426, 1), (5434, 1), (5455, 1), (5469, 1), (5470, 1), (5483, 1), (5496, 3), (5498, 2), (5516, 1), (5537, 1), (5538, 1), (5539, 1), (5549, 1), (5572, 2), (5581, 1), (5586, 1), (5594, 1), (5595, 1), (5601, 1), (5609, 3), (5631, 1), (5640, 1), (5653, 1), (5657, 3), (5659, 1), (5668, 1), (5671, 1), (5675, 3), (5679, 1), (5690, 1), (5695, 5), (5704, 1), (5706, 1), (5716, 1), (5727, 2), (5730, 2), (5731, 2), (5739, 1), (5744, 1), (5753, 3), (5762, 1), (5768, 2), (5769, 2), (5799, 1), (5808, 1), (5814, 4), (5816, 1), (5829, 1), (5841, 1), (5849, 1), (5860, 5), (5878, 1), (5890, 1), (5897, 5), (5901, 1), (5905, 1), (5908, 1), (5915, 1), (5923, 1), (5924, 4), (5928, 1), (5929, 1), (5934, 1), (5942, 1), (5947, 3), (5950, 3), (5971, 1), (5972, 2), (5980, 3), (6008, 4), (6024, 3), (6030, 1), (6066, 14), (6067, 4), (6069, 1), (6095, 1), (6106, 2), (6122, 1), (6134, 1), (6147, 1), (6148, 1), (6149, 2), (6153, 1), (6189, 2), (6197, 1), (6199, 1), (6204, 1), (6213, 1), (6218, 3), (6220, 1), (6223, 1), (6227, 2), (6228, 4), (6229, 3), (6236, 1), (6249, 1), (6254, 2), (6260, 1), (6264, 3), (6294, 1), (6297, 5), (6308, 2), (6314, 1), (6322, 1), (6335, 1), (6342, 1), (6348, 2), (6365, 1), (6369, 1), (6392, 1), (6412, 1), (6420, 13), (6422, 1), (6424, 1), (6426, 15), (6429, 1), (6442, 1), (6448, 8), (6451, 1), (6457, 2), (6473, 1), (6478, 1), (6486, 1), (6489, 1), (6510, 1), (6515, 1), (6522, 1), (6530, 2), (6531, 1), (6532, 3), (6536, 1), (6579, 6), (6585, 1), (6594, 1), (6597, 1), (6602, 1), (6614, 1), (6640, 1), (6647, 2), (6651, 1), (6673, 2), (6677, 1), (6683, 2), (6702, 3), (6703, 1), (6704, 2), (6723, 1), (6726, 1), (6727, 4), (6728, 4), (6729, 2), (6735, 1), (6740, 1), (6749, 2), (6756, 1), (6761, 1), (6770, 1), (6774, 2), (6778, 3), (6779, 16), (6784, 1), (6791, 1), (6792, 2), (6796, 1), (6797, 4), (6799, 1), (6815, 2), (6816, 1), (6817, 2), (6840, 1), (6845, 2), (6858, 1), (6860, 1), (6869, 1), (6870, 1), (6871, 2), (6892, 3), (6893, 1), (6908, 1), (6909, 2), (6916, 1), (6918, 1), (6924, 1), (6927, 1), (6938, 1), (6942, 3), (6953, 1), (6955, 1), (6960, 4), (6964, 1), (6986, 1), (6997, 1), (7005, 1), (7016, 2), (7030, 1), (7044, 1), (7052, 1), (7055, 5), (7057, 1), (7075, 5), (7081, 2), (7086, 1), (7090, 2), (7099, 1), (7104, 2), (7107, 1), (7108, 1), (7113, 1), (7115, 2), (7117, 3), (7119, 1), (7126, 2), (7144, 1), (7160, 1), (7180, 1), (7185, 2), (7201, 1), (7204, 13), (7218, 1), (7226, 1), (7241, 1), (7244, 5), (7253, 1), (7257, 1), (7261, 3), (7265, 1), (7268, 1), (7269, 1), (7275, 1), (7277, 1), (7279, 2), (7281, 3), (7282, 3), (7283, 3), (7291, 1), (7303, 2), (7336, 29), (7342, 1), (7351, 2), (7358, 1), (7360, 2), (7397, 1), (7408, 2), (7432, 1), (7460, 1), (7468, 2), (7470, 2), (7477, 1), (7480, 1), (7481, 1), (7488, 1), (7489, 1), (7491, 1), (7500, 3), (7513, 1), (7518, 3), (7524, 1), (7530, 6), (7531, 2), (7539, 1), (7541, 2), (7565, 1), (7575, 2), (7603, 1), (7607, 4), (7644, 2), (7648, 1), (7676, 5), (7688, 5), (7689, 3), (7691, 1), (7694, 3), (7698, 2), (7701, 1), (7711, 2), (7717, 1), (7725, 1), (7739, 1), (7745, 1), (7752, 2), (7755, 49), (7780, 2), (7795, 2), (7802, 2), (7803, 1), (7805, 1), (7813, 1), (7817, 2), (7818, 3), (7828, 4), (7838, 1), (7846, 1), (7849, 3), (7850, 2), (7853, 1), (7861, 1), (7867, 1), (7872, 1), (7888, 1), (7890, 8), (7893, 1), (7895, 1), (7899, 1), (7905, 1), (7915, 13), (7916, 2), (7923, 1), (7933, 2), (7940, 1), (7943, 2), (7948, 1), (7952, 4), (7966, 1), (7994, 1), (7997, 4), (8007, 9), (8025, 5), (8041, 3), (8061, 1), (8071, 1), (8082, 1), (8083, 1), (8087, 1), (8091, 1), (8109, 2), (8122, 5), (8126, 1), (8129, 1), (8132, 2), (8138, 9), (8140, 1), (8142, 136), (8152, 1), (8153, 1), (8168, 1), (8170, 1), (8171, 1), (8189, 4), (8204, 1), (8219, 4), (8224, 1), (8239, 1), (8251, 4), (8255, 1), (8257, 1), (8259, 35), (8278, 2), (8287, 1), (8289, 1), (8296, 3), (8301, 1), (8306, 5), (8331, 2), (8336, 1), (8340, 2), (8343, 1), (8349, 1), (8355, 2), (8368, 1), (8376, 1), (8379, 3), (8388, 2), (8408, 1), (8424, 2), (8430, 1), (8432, 2), (8433, 3), (8435, 2), (8447, 1), (8449, 1), (8455, 2), (8457, 2), (8459, 1), (8463, 3), (8470, 14), (8488, 1), (8497, 1), (8499, 1), (8508, 1), (8512, 1), (8536, 5), (8539, 2), (8552, 1), (8555, 4), (8593, 4), (8604, 1), (8613, 2), (8617, 1), (8635, 1), (8647, 4), (8655, 1), (8659, 2), (8670, 1), (8682, 1), (8688, 5), (8689, 1), (8693, 1), (8696, 1), (8705, 1), (8714, 1), (8719, 1), (8725, 1), (8746, 1), (8758, 1), (8760, 1), (8776, 1), (8778, 1), (8790, 1), (8792, 3), (8814, 3), (8822, 1), (8825, 1), (8826, 3), (8827, 1), (8833, 3), (8836, 1), (8866, 1), (8868, 1), (8869, 1), (8870, 3), (8871, 2), (8878, 4), (8905, 1), (8917, 1), (8920, 1), (8923, 1), (8928, 1), (8930, 1), (8931, 2), (8935, 1), (8937, 1), (8951, 1), (8952, 1), (8954, 1), (8963, 4), (8983, 1), (8988, 1), (8995, 18), (9001, 4), (9036, 4), (9041, 1), (9056, 1), (9067, 1), (9089, 4), (9095, 1), (9113, 2), (9124, 2), (9145, 8), (9150, 1), (9180, 1), (9181, 3), (9184, 4), (9205, 1), (9212, 2), (9213, 3), (9222, 2), (9237, 1), (9240, 2), (9248, 1), (9250, 1), (9251, 1), (9256, 1), (9258, 1), (9264, 2), (9272, 2), (9275, 1), (9279, 1), (9282, 2), (9283, 9), (9292, 1), (9299, 4), (9317, 1), (9325, 1), (9336, 1), (9345, 1), (9347, 1), (9360, 5), (9372, 3), (9374, 1), (9378, 1), (9383, 4), (9391, 1), (9396, 1), (9401, 2), (9408, 2), (9416, 1), (9430, 1), (9433, 4), (9437, 4), (9442, 1), (9455, 1), (9456, 1), (9457, 1), (9463, 1), (9478, 2), (9483, 1), (9490, 3), (9495, 2), (9497, 2), (9507, 18), (9520, 1), (9529, 1), (9531, 3), (9538, 1), (9545, 1), (9550, 1), (9567, 4), (9599, 1), (9601, 1), (9602, 2), (9619, 1), (9636, 1), (9644, 2), (9648, 1), (9656, 1), (9670, 1), (9686, 11), (9698, 1), (9713, 1), (9714, 2), (9716, 2), (9719, 1), (9722, 1), (9727, 2), (9739, 1), (9752, 2), (9767, 1), (9776, 11), (9791, 4), (9794, 1), (9797, 4), (9809, 2), (9813, 1), (9823, 2), (9836, 3), (9847, 3), (9863, 1), (9902, 1), (9907, 1), (9909, 1), (9936, 3), (9944, 3), (9959, 1), (9964, 1), (9965, 3), (9981, 1), (9985, 4), (10002, 3), (10013, 1), (10015, 1), (10047, 1), (10077, 1), (10099, 5), (10111, 1), (10119, 1), (10124, 2), (10135, 1), (10159, 1), (10178, 2), (10190, 1), (10193, 1), (10203, 1), (10206, 1), (10221, 1), (10226, 1), (10230, 1), (10245, 3), (10259, 1), (10279, 1), (10284, 1), (10287, 1), (10291, 1), (10293, 6), (10294, 2), (10297, 2), (10311, 2), (10320, 1), (10326, 1), (10327, 1), (10367, 1), (10373, 1), (10390, 1), (10394, 1), (10407, 8), (10415, 1), (10421, 1), (10429, 1), (10432, 1), (10445, 1), (10453, 7), (10460, 3), (10476, 1), (10484, 1), (10515, 2), (10541, 2), (10564, 2), (10569, 1), (10576, 13), (10584, 2), (10591, 1), (10592, 2), (10593, 1), (10606, 5), (10620, 3), (10654, 6), (10661, 2), (10664, 1), (10668, 1), (10676, 1), (10677, 1), (10684, 1), (10692, 1), (10696, 1), (10716, 1), (10717, 3), (10718, 3), (10728, 1), (10737, 1), (10746, 1), (10751, 21), (10755, 1), (10767, 1), (10770, 1), (10773, 1), (10785, 3), (10798, 2), (10800, 1), (10804, 4), (10810, 3), (10815, 2), (10833, 1), (10834, 1), (10838, 1), (10856, 1), (10872, 2), (10887, 2), (10891, 1), (10894, 1), (10901, 6), (10909, 3), (10911, 2), (10913, 1), (10914, 3), (10927, 1), (10936, 7), (10939, 1), (10945, 1), (10950, 1), (10952, 3), (10953, 2), (10954, 4), (10971, 7), (10974, 1), (10976, 1), (10983, 1), (10989, 1), (10990, 1), (10997, 1), (11006, 1), (11019, 29), (11020, 1), (11032, 4), (11043, 1), (11066, 2), (11082, 2), (11088, 5), (11095, 2), (11101, 4), (11112, 1), (11115, 1), (11127, 1), (11129, 3), (11131, 2), (11134, 5), (11149, 3), (11153, 1), (11158, 1), (11165, 1), (11168, 1), (11171, 1), (11173, 1), (11176, 1), (11199, 1), (11205, 1), (11215, 4), (11227, 1), (11228, 1), (11230, 1), (11241, 2), (11247, 1), (11253, 1), (11267, 1), (11272, 1), (11292, 1), (11299, 1), (11305, 1), (11306, 1), (11307, 1), (11314, 4), (11323, 4), (11343, 2), (11344, 1), (11366, 1), (11401, 1), (11404, 2), (11415, 1), (11427, 2), (11429, 1), (11465, 1), (11473, 1), (11475, 1), (11484, 1), (11505, 1), (11509, 1), (11511, 3), (11514, 1), (11525, 1), (11531, 2), (11534, 1), (11550, 1), (11568, 1), (11576, 1), (11584, 1), (11599, 1), (11603, 1), (11605, 4), (11607, 2), (11616, 8), (11618, 1), (11626, 1), (11630, 3), (11634, 1), (11660, 1), (11679, 2), (11686, 1), (11687, 1), (11698, 1), (11713, 1), (11714, 2), (11716, 1), (11720, 2), (11732, 1), (11738, 1), (11741, 3), (11746, 2), (11750, 1), (11777, 1), (11791, 2), (11794, 1), (11817, 1), (11818, 3), (11830, 19), (11849, 1), (11854, 2), (11875, 1), (11900, 2), (11925, 1), (11927, 2), (11928, 1), (11934, 1), (11942, 1), (11948, 1), (11950, 1), (11952, 5), (11954, 3), (11959, 2), (11962, 1), (11966, 1), (11978, 1), (11980, 2), (11981, 1), (11987, 2), (12019, 2), (12021, 18), (12030, 1), (12046, 1), (12051, 2), (12060, 1), (12066, 1), (12083, 1), (12085, 1), (12093, 2), (12096, 1), (12101, 12), (12103, 1), (12106, 1), (12108, 1), (12126, 1), (12141, 1), (12145, 3), (12159, 1), (12167, 2), (12173, 4), (12192, 2), (12198, 4), (12206, 1), (12235, 1), (12238, 3), (12287, 1), (12307, 1), (12314, 1), (12332, 4), (12335, 1), (12342, 1), (12346, 10), (12351, 1), (12372, 1), (12385, 3), (12388, 1), (12395, 2), (12429, 3), (12437, 2), (12445, 2), (12450, 2), (12453, 1), (12457, 1), (12468, 1), (12473, 2), (12483, 5), (12489, 1), (12500, 3), (12508, 8), (12514, 2), (12517, 4), (12524, 2), (12526, 1), (12532, 1), (12543, 4), (12548, 2), (12556, 3), (12567, 1), (12576, 7), (12582, 1), (12583, 1), (12590, 26), (12592, 1), (12594, 1), (12597, 1), (12601, 1), (12603, 1), (12605, 13), (12608, 1), (12609, 1), (12610, 1), (12612, 1), (12636, 1), (12642, 4), (12648, 4), (12651, 1), (12660, 1), (12668, 1), (12670, 1), (12691, 2), (12696, 1), (12697, 5), (12727, 2), (12746, 1), (12753, 1), (12759, 1), (12766, 5), (12770, 1), (12775, 14), (12779, 2), (12790, 1), (12813, 1), (12843, 1), (12848, 1), (12855, 1), (12886, 1), (12889, 1), (12905, 1), (12915, 1), (12920, 1), (12922, 1), (12933, 1), (12957, 2), (12958, 1), (12959, 1), (12961, 1), (12973, 1), (12983, 1), (12984, 3), (12989, 1), (13010, 2), (13017, 1), (13019, 1), (13030, 1), (13045, 1), (13047, 1), (13050, 1), (13062, 2), (13064, 1), (13067, 4), (13070, 1), (13076, 1), (13092, 1), (13097, 2), (13104, 8), (13113, 1), (13148, 1), (13158, 2), (13160, 1), (13170, 1), (13176, 3), (13178, 2), (13179, 1), (13192, 5), (13195, 1), (13198, 1), (13204, 2), (13207, 1), (13211, 1), (13214, 2), (13218, 1), (13219, 1), (13221, 5), (13229, 1), (13235, 6), (13242, 6), (13244, 1), (13245, 1), (13252, 1), (13254, 1), (13255, 1), (13258, 1), (13280, 2), (13287, 2), (13295, 4), (13308, 1), (13314, 1), (13322, 1), (13326, 2), (13342, 4), (13348, 1), (13350, 1), (13354, 1), (13362, 1), (13371, 4), (13376, 3), (13381, 3), (13382, 1), (13391, 7), (13397, 1), (13400, 1), (13411, 1), (13428, 1), (13434, 1), (13440, 1), (13444, 1), (13472, 5), (13478, 2), (13485, 1), (13490, 3), (13495, 1), (13510, 1), (13525, 1), (13544, 1), (13546, 1), (13547, 1), (13553, 1), (13558, 5), (13563, 2), (13585, 1), (13588, 1), (13589, 3), (13606, 1), (13620, 1), (13640, 1), (13643, 1), (13696, 1), (13697, 3), (13699, 2), (13718, 1), (13722, 4), (13724, 2), (13726, 1), (13741, 1), (13745, 1), (13767, 1), (13786, 1), (13788, 1), (13792, 2), (13796, 4), (13851, 1), (13859, 7), (13879, 5), (13880, 1), (13884, 1), (13929, 1), (13931, 3), (13943, 1), (13956, 2), (13960, 1), (13961, 1), (13973, 1), (13978, 1), (13979, 1), (13980, 1), (13989, 3), (13993, 1), (14002, 1), (14006, 1), (14010, 2), (14013, 1), (14014, 1), (14018, 1), (14024, 1), (14034, 4), (14042, 3), (14047, 1), (14086, 2), (14088, 1), (14104, 1), (14110, 1), (14123, 1), (14135, 2), (14139, 2), (14156, 1), (14160, 1), (14170, 1), (14171, 8), (14180, 1), (14181, 1), (14183, 1), (14186, 1), (14192, 1), (14199, 1), (14200, 12), (14202, 2), (14223, 1), (14242, 1), (14245, 1), (14249, 1), (14272, 1), (14273, 1), (14276, 1), (14279, 1), (14293, 1), (14295, 1), (14301, 1), (14313, 1), (14331, 2), (14337, 1), (14343, 4), (14350, 1), (14358, 1), (14385, 4), (14386, 1), (14394, 1), (14396, 1), (14397, 2), (14399, 1), (14405, 1), (14416, 1), (14423, 1), (14425, 11), (14426, 1), (14427, 1), (14432, 2), (14434, 1), (14444, 1), (14451, 2), (14462, 1), (14468, 1), (14487, 3), (14494, 1), (14497, 1), (14502, 1), (14507, 1), (14509, 2), (14512, 1), (14525, 1), (14529, 1), (14537, 1), (14549, 1), (14565, 1), (14568, 1), (14571, 1), (14579, 1), (14581, 7), (14586, 1), (14644, 1), (14676, 1), (14680, 1), (14682, 1), (14683, 1), (14691, 3)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class WikiCorpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        \"\"\"\n",
    "        Parse the first `clip_docs` Wikipedia documents from file `dump_file`.\n",
    "        Yield each document in turn, as a list of tokens (unicode strings).\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for tokens in itertools.islice(iter_wiki(self.dump_file), self.clip_docs):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs\n",
    "\n",
    "# create a stream of bag-of-words vectors\n",
    "wiki_corpus = WikiCorpus(doc_path, id2word_wiki)\n",
    "vector = next(iter(wiki_corpus))\n",
    "print(vector)  # print the first vector in the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai 136\n",
      "averaging\n",
      "[(8142, 136), (430, 107), (5198, 81)]\n"
     ]
    }
   ],
   "source": [
    "# what is the most common word in that first article?\n",
    "most_index, most_count = max(vector, key=lambda x: x[1])\n",
    "print(id2word_wiki[most_index], most_count)\n",
    "\n",
    "print(id2word_wiki[68])\n",
    "\n",
    "import heapq\n",
    "print(heapq.nlargest(3, vector, key=lambda x: x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : saving sparse matrix to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : saved 3919x14701 matrix, density=1.257% (724187/57613219)\n",
      "INFO : saving MmCorpus index to D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "wiki_bow_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_bow_path, wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loaded corpus index from D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm.index\n",
      "INFO : initializing corpus reader from D:\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_bow.mm\n",
      "INFO : accepted corpus with 3919 documents, 14701 features, 724187 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(3919 documents, 14701 features, 724187 non-zero entries)\n",
      "3919\n"
     ]
    }
   ],
   "source": [
    "mm_corpus = gensim.corpora.MmCorpus(wiki_bow_path)\n",
    "print(mm_corpus)\n",
    "\n",
    "print(len([ x for x in iter(mm_corpus)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "INFO : using symmetric eta at 6.802258349772124e-05\n",
      "INFO : using serial LDA version on this node\n",
      "INFO : running online LDA training, 4 topics, 5 passes over the supplied corpus of 3919 documents, updating model once every 2000 documents, evaluating perplexity every 3919 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "INFO : PROGRESS: pass 0, at document #2000/3919\n",
      "INFO : optimized alpha [0.37842610491898376, 0.37857957629467054, 0.34583772796098211, 0.37048910359942355]\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.378): 0.004*\"conference\" + 0.003*\"computational\" + 0.003*\"film\" + 0.003*\"algorithm\" + 0.002*\"algorithms\" + 0.002*\"design\" + 0.002*\"web\" + 0.002*\"genre\" + 0.002*\"programming\" + 0.001*\"models\"\n",
      "INFO : topic #1 (0.379): 0.003*\"film\" + 0.003*\"tales\" + 0.003*\"award\" + 0.002*\"computational\" + 0.002*\"prize\" + 0.002*\"engineering\" + 0.002*\"conference\" + 0.002*\"narrative\" + 0.002*\"king\" + 0.002*\"genre\"\n",
      "INFO : topic #2 (0.346): 0.004*\"film\" + 0.002*\"narrative\" + 0.002*\"genre\" + 0.002*\"students\" + 0.002*\"code\" + 0.002*\"released\" + 0.002*\"person\" + 0.001*\"literacy\" + 0.001*\"poet\" + 0.001*\"canto\"\n",
      "INFO : topic #3 (0.370): 0.004*\"computational\" + 0.004*\"conference\" + 0.002*\"programming\" + 0.002*\"million\" + 0.002*\"award\" + 0.002*\"algorithm\" + 0.001*\"acm\" + 0.001*\"students\" + 0.001*\"film\" + 0.001*\"algorithms\"\n",
      "INFO : topic diff=1.069803, rho=1.000000\n",
      "INFO : -9.301 per-word bound, 630.7 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 0, at document #3919/3919\n",
      "INFO : optimized alpha [0.34707098998026004, 0.29270472895440597, 0.26766500055871378, 0.295115141997983]\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.347): 0.006*\"theorem\" + 0.004*\"notation\" + 0.003*\"proof\" + 0.003*\"algorithm\" + 0.003*\"matrix\" + 0.003*\"px\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"composite\" + 0.002*\"models\"\n",
      "INFO : topic #1 (0.293): 0.007*\"px\" + 0.007*\"gospels\" + 0.004*\"greece\" + 0.004*\"gr\" + 0.003*\"france\" + 0.002*\"women\" + 0.002*\"uk\" + 0.002*\"acts\" + 0.002*\"monastery\" + 0.002*\"award\"\n",
      "INFO : topic #2 (0.268): 0.003*\"film\" + 0.002*\"genre\" + 0.002*\"tv\" + 0.002*\"narrative\" + 0.002*\"museum\" + 0.002*\"publishes\" + 0.001*\"love\" + 0.001*\"theatre\" + 0.001*\"al\" + 0.001*\"music\"\n",
      "INFO : topic #3 (0.295): 0.013*\"born\" + 0.009*\"usa\" + 0.004*\"germany\" + 0.003*\"france\" + 0.003*\"england\" + 0.003*\"million\" + 0.003*\"britain\" + 0.002*\"russia\" + 0.002*\"italy\" + 0.002*\"conference\"\n",
      "INFO : topic diff=0.680966, rho=0.707107\n",
      "INFO : PROGRESS: pass 1, at document #2000/3919\n",
      "INFO : optimized alpha [0.17378121380633671, 0.14233557289883686, 0.1915286325053191, 0.1919862128565156]\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.174): 0.005*\"algorithm\" + 0.004*\"theorem\" + 0.003*\"computational\" + 0.003*\"models\" + 0.003*\"logic\" + 0.002*\"proof\" + 0.002*\"notation\" + 0.002*\"algorithms\" + 0.002*\"matrix\" + 0.002*\"vector\"\n",
      "INFO : topic #1 (0.142): 0.004*\"px\" + 0.004*\"gospels\" + 0.003*\"tales\" + 0.003*\"greece\" + 0.002*\"france\" + 0.002*\"award\" + 0.002*\"prize\" + 0.002*\"women\" + 0.002*\"inf\" + 0.002*\"music\"\n",
      "INFO : topic #2 (0.192): 0.005*\"film\" + 0.003*\"genre\" + 0.002*\"narrative\" + 0.002*\"released\" + 0.001*\"person\" + 0.001*\"love\" + 0.001*\"king\" + 0.001*\"television\" + 0.001*\"action\" + 0.001*\"reader\"\n",
      "INFO : topic #3 (0.192): 0.008*\"born\" + 0.006*\"conference\" + 0.006*\"usa\" + 0.003*\"students\" + 0.003*\"computational\" + 0.003*\"million\" + 0.003*\"award\" + 0.003*\"germany\" + 0.002*\"programming\" + 0.002*\"acm\"\n",
      "INFO : topic diff=0.355894, rho=0.502551\n",
      "INFO : -8.583 per-word bound, 383.4 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 1, at document #3919/3919\n",
      "INFO : optimized alpha [0.15057874564881849, 0.10052559403562189, 0.12988091692050352, 0.12630586180261152]\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.151): 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"vector\" + 0.003*\"logic\" + 0.003*\"matrix\" + 0.003*\"algebra\" + 0.003*\"models\" + 0.003*\"composite\"\n",
      "INFO : topic #1 (0.101): 0.012*\"px\" + 0.011*\"gospels\" + 0.007*\"greece\" + 0.005*\"gr\" + 0.004*\"france\" + 0.003*\"epistles\" + 0.003*\"acts\" + 0.003*\"monastery\" + 0.002*\"tales\" + 0.002*\"latin\"\n",
      "INFO : topic #2 (0.130): 0.004*\"film\" + 0.003*\"genre\" + 0.002*\"narrative\" + 0.002*\"tv\" + 0.002*\"love\" + 0.001*\"king\" + 0.001*\"publishes\" + 0.001*\"theatre\" + 0.001*\"television\" + 0.001*\"person\"\n",
      "INFO : topic #3 (0.126): 0.015*\"born\" + 0.010*\"usa\" + 0.005*\"germany\" + 0.004*\"conference\" + 0.004*\"france\" + 0.004*\"students\" + 0.003*\"england\" + 0.003*\"million\" + 0.003*\"britain\" + 0.002*\"award\"\n",
      "INFO : topic diff=0.360047, rho=0.502551\n",
      "INFO : PROGRESS: pass 2, at document #2000/3919\n",
      "INFO : optimized alpha [0.1237885513848128, 0.079225551093627444, 0.11714843630599761, 0.11656645304092568]\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.124): 0.005*\"algorithm\" + 0.005*\"theorem\" + 0.004*\"computational\" + 0.003*\"models\" + 0.003*\"proof\" + 0.003*\"logic\" + 0.003*\"notation\" + 0.002*\"algorithms\" + 0.002*\"vector\" + 0.002*\"matrix\"\n",
      "INFO : topic #1 (0.079): 0.007*\"px\" + 0.006*\"gospels\" + 0.005*\"greece\" + 0.004*\"tales\" + 0.003*\"latin\" + 0.003*\"france\" + 0.003*\"greek\" + 0.003*\"gr\" + 0.003*\"music\" + 0.003*\"inf\"\n",
      "INFO : topic #2 (0.117): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"released\" + 0.002*\"love\" + 0.002*\"king\" + 0.002*\"person\" + 0.001*\"television\" + 0.001*\"fantasy\" + 0.001*\"criticism\"\n",
      "INFO : topic #3 (0.117): 0.009*\"born\" + 0.008*\"conference\" + 0.006*\"usa\" + 0.004*\"students\" + 0.004*\"award\" + 0.003*\"germany\" + 0.003*\"institute\" + 0.003*\"million\" + 0.003*\"acm\" + 0.003*\"programming\"\n",
      "INFO : topic diff=0.307030, rho=0.449036\n",
      "INFO : -8.493 per-word bound, 360.4 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 2, at document #3919/3919\n",
      "INFO : optimized alpha [0.11847092341834123, 0.070714861287418315, 0.10153561981980802, 0.098377621566277801]\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.118): 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"vector\" + 0.003*\"logic\" + 0.003*\"matrix\" + 0.003*\"models\" + 0.003*\"equations\"\n",
      "INFO : topic #1 (0.071): 0.014*\"px\" + 0.012*\"gospels\" + 0.009*\"greece\" + 0.006*\"gr\" + 0.004*\"france\" + 0.004*\"epistles\" + 0.004*\"latin\" + 0.004*\"acts\" + 0.003*\"monastery\" + 0.003*\"greek\"\n",
      "INFO : topic #2 (0.102): 0.004*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"tv\" + 0.002*\"love\" + 0.002*\"king\" + 0.001*\"television\" + 0.001*\"person\" + 0.001*\"theatre\" + 0.001*\"released\"\n",
      "INFO : topic #3 (0.098): 0.015*\"born\" + 0.010*\"usa\" + 0.005*\"conference\" + 0.005*\"germany\" + 0.004*\"students\" + 0.004*\"france\" + 0.003*\"england\" + 0.003*\"award\" + 0.003*\"million\" + 0.003*\"institute\"\n",
      "INFO : topic diff=0.306520, rho=0.449036\n",
      "INFO : PROGRESS: pass 3, at document #2000/3919\n",
      "INFO : optimized alpha [0.10665684890566314, 0.061753700666579245, 0.097577761672033914, 0.09809808015025559]\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.107): 0.005*\"algorithm\" + 0.005*\"theorem\" + 0.004*\"computational\" + 0.003*\"models\" + 0.003*\"proof\" + 0.003*\"notation\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"algebra\" + 0.002*\"algorithms\"\n",
      "INFO : topic #1 (0.062): 0.008*\"px\" + 0.007*\"gospels\" + 0.006*\"greece\" + 0.004*\"latin\" + 0.004*\"tales\" + 0.004*\"greek\" + 0.003*\"france\" + 0.003*\"gr\" + 0.003*\"music\" + 0.003*\"ancient\"\n",
      "INFO : topic #2 (0.098): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"released\" + 0.002*\"love\" + 0.002*\"king\" + 0.002*\"person\" + 0.001*\"television\" + 0.001*\"fantasy\" + 0.001*\"criticism\"\n",
      "INFO : topic #3 (0.098): 0.009*\"born\" + 0.008*\"conference\" + 0.006*\"usa\" + 0.005*\"students\" + 0.004*\"award\" + 0.003*\"institute\" + 0.003*\"germany\" + 0.003*\"prize\" + 0.003*\"million\" + 0.003*\"acm\"\n",
      "INFO : topic diff=0.264514, rho=0.409633\n",
      "INFO : -8.464 per-word bound, 353.1 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 3, at document #3919/3919\n",
      "INFO : optimized alpha [0.10599468910584267, 0.059027798640256451, 0.09012355522386499, 0.088787030658239577]\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.106): 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"matrix\" + 0.003*\"models\" + 0.003*\"equations\"\n",
      "INFO : topic #1 (0.059): 0.015*\"px\" + 0.012*\"gospels\" + 0.009*\"greece\" + 0.006*\"gr\" + 0.005*\"latin\" + 0.004*\"france\" + 0.004*\"epistles\" + 0.004*\"greek\" + 0.004*\"acts\" + 0.003*\"monastery\"\n",
      "INFO : topic #2 (0.090): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"tv\" + 0.002*\"love\" + 0.002*\"king\" + 0.001*\"person\" + 0.001*\"television\" + 0.001*\"released\" + 0.001*\"game\"\n",
      "INFO : topic #3 (0.089): 0.015*\"born\" + 0.010*\"usa\" + 0.006*\"conference\" + 0.005*\"students\" + 0.005*\"germany\" + 0.004*\"france\" + 0.004*\"award\" + 0.003*\"england\" + 0.003*\"million\" + 0.003*\"institute\"\n",
      "INFO : topic diff=0.259938, rho=0.409633\n",
      "INFO : PROGRESS: pass 4, at document #2000/3919\n",
      "INFO : optimized alpha [0.099473796286361063, 0.054084794972109745, 0.088972380856421021, 0.09108000417019245]\n",
      "INFO : merging changes from 2000 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.099): 0.005*\"algorithm\" + 0.005*\"theorem\" + 0.004*\"computational\" + 0.003*\"models\" + 0.003*\"proof\" + 0.003*\"notation\" + 0.003*\"logic\" + 0.003*\"algebra\" + 0.003*\"vector\" + 0.002*\"matrix\"\n",
      "INFO : topic #1 (0.054): 0.009*\"px\" + 0.007*\"gospels\" + 0.006*\"greece\" + 0.005*\"latin\" + 0.004*\"greek\" + 0.004*\"tales\" + 0.004*\"france\" + 0.003*\"gr\" + 0.003*\"poet\" + 0.003*\"ancient\"\n",
      "INFO : topic #2 (0.089): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"released\" + 0.002*\"king\" + 0.002*\"love\" + 0.002*\"person\" + 0.001*\"television\" + 0.001*\"fantasy\" + 0.001*\"criticism\"\n",
      "INFO : topic #3 (0.091): 0.009*\"born\" + 0.008*\"conference\" + 0.007*\"usa\" + 0.005*\"students\" + 0.005*\"award\" + 0.003*\"prize\" + 0.003*\"institute\" + 0.003*\"germany\" + 0.003*\"programming\" + 0.003*\"million\"\n",
      "INFO : topic diff=0.223687, rho=0.379063\n",
      "INFO : -8.450 per-word bound, 349.8 perplexity estimate based on a held-out corpus of 1919 documents with 662535 words\n",
      "INFO : PROGRESS: pass 4, at document #3919/3919\n",
      "INFO : optimized alpha [0.10030334671222621, 0.053326519237949781, 0.084735922021470789, 0.085021000934613067]\n",
      "INFO : merging changes from 1919 documents into a model of 3919 documents\n",
      "INFO : topic #0 (0.100): 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"models\" + 0.003*\"matrix\" + 0.003*\"equations\"\n",
      "INFO : topic #1 (0.053): 0.015*\"px\" + 0.012*\"gospels\" + 0.009*\"greece\" + 0.005*\"gr\" + 0.005*\"latin\" + 0.005*\"france\" + 0.004*\"epistles\" + 0.004*\"greek\" + 0.004*\"acts\" + 0.003*\"monastery\"\n",
      "INFO : topic #2 (0.085): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"tv\" + 0.002*\"love\" + 0.002*\"king\" + 0.001*\"person\" + 0.001*\"television\" + 0.001*\"released\" + 0.001*\"game\"\n",
      "INFO : topic #3 (0.085): 0.014*\"born\" + 0.010*\"usa\" + 0.006*\"conference\" + 0.005*\"students\" + 0.005*\"germany\" + 0.004*\"award\" + 0.004*\"france\" + 0.003*\"million\" + 0.003*\"england\" + 0.003*\"institute\"\n",
      "INFO : topic diff=0.218733, rho=0.379063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "clipped_corpus = gensim.utils.ClippedCorpus(mm_corpus, 4000)  # use fewer documents during training, LDA is slow\n",
    "# ClippedCorpus new in gensim 0.10.1\n",
    "# copy&paste it from https://github.com/piskvorky/gensim/blob/0.10.1/gensim/utils.py#L467 if necessary (or upgrade your gensim)\n",
    "%time lda_model = gensim.models.LdaModel(clipped_corpus, num_topics=4, id2word=id2word_wiki, passes=5, alpha='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.100): 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"models\" + 0.003*\"matrix\" + 0.003*\"equations\"\n",
      "INFO : topic #1 (0.053): 0.015*\"px\" + 0.012*\"gospels\" + 0.009*\"greece\" + 0.005*\"gr\" + 0.005*\"latin\" + 0.005*\"france\" + 0.004*\"epistles\" + 0.004*\"greek\" + 0.004*\"acts\" + 0.003*\"monastery\"\n",
      "INFO : topic #2 (0.085): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"tv\" + 0.002*\"love\" + 0.002*\"king\" + 0.001*\"person\" + 0.001*\"television\" + 0.001*\"released\" + 0.001*\"game\"\n",
      "INFO : topic #3 (0.085): 0.014*\"born\" + 0.010*\"usa\" + 0.006*\"conference\" + 0.005*\"students\" + 0.005*\"germany\" + 0.004*\"award\" + 0.004*\"france\" + 0.003*\"million\" + 0.003*\"england\" + 0.003*\"institute\"\n"
     ]
    }
   ],
   "source": [
    "_ = lda_model.print_topics()  # print a few most important words for each LDA topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tfidf_model = gensim.models.TfidfModel(mm_corpus, id2word=id2word_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time lsi_model = gensim.models.LsiModel(tfidf_model[mm_corpus], id2word=id2word_wiki, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cache the transformed corpora to disk, for use in later notebooks\n",
    "wiki_tfidf_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_tfidf.mm'\n",
    "wiki_lsa_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\wiki_lsa.mm'\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_tfidf_path, tfidf_model[mm_corpus])\n",
    "%time gensim.corpora.MmCorpus.serialize(wiki_lsa_path, lsi_model[tfidf_model[mm_corpus]])\n",
    "# gensim.corpora.MmCorpus.serialize('./data/wiki_lda.mm', lda_model[mm_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #0 (0.100): 0.006*\"theorem\" + 0.004*\"algorithm\" + 0.004*\"notation\" + 0.004*\"proof\" + 0.003*\"algebra\" + 0.003*\"logic\" + 0.003*\"vector\" + 0.003*\"models\" + 0.003*\"matrix\" + 0.003*\"equations\"\n",
      "INFO : topic #1 (0.053): 0.015*\"px\" + 0.012*\"gospels\" + 0.009*\"greece\" + 0.005*\"gr\" + 0.005*\"latin\" + 0.005*\"france\" + 0.004*\"epistles\" + 0.004*\"greek\" + 0.004*\"acts\" + 0.003*\"monastery\"\n",
      "INFO : topic #2 (0.085): 0.005*\"film\" + 0.003*\"genre\" + 0.003*\"narrative\" + 0.002*\"tv\" + 0.002*\"love\" + 0.002*\"king\" + 0.001*\"person\" + 0.001*\"television\" + 0.001*\"released\" + 0.001*\"game\"\n",
      "INFO : topic #3 (0.085): 0.014*\"born\" + 0.010*\"usa\" + 0.006*\"conference\" + 0.005*\"students\" + 0.005*\"germany\" + 0.004*\"award\" + 0.004*\"france\" + 0.003*\"million\" + 0.003*\"england\" + 0.003*\"institute\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.14421276005823253), (2, 0.53135314206025475), (3, 0.32438737447065991)]\n",
      "[(1, 0.36331121809631484), (2, 0.63352242745552212)]\n",
      "[(0, 0.99779825993637672)]\n"
     ]
    }
   ],
   "source": [
    "# evaluate on 1k documents **not** used in LDA training\n",
    "test1_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Ethics.xml'\n",
    "test2_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_Allegory.xml'\n",
    "test3_path = 'D:\\\\Study\\Winter-2017\\Machine Learning\\Project\\DistractionBuster\\Simplex\\Dump\\Test_doc_AI.xml'\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_1 = [tokens for tokens in iter_wiki(test1_path)]\n",
    "#print(lda_model.print_topics(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1])[0]))\n",
    "\n",
    "#for x in heapq.nlargest(3, id2word_wiki.doc2bow(test_doc_1[0]), key=lambda x: x[1]):\n",
    "#    print(x[0], id2word_wiki[x[0]], x[1])\n",
    "\n",
    "#print(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1]))\n",
    "#print(max(lda_model[id2word_wiki.doc2bow(test_doc_1[0])], key=lambda item: item[1])[0])\n",
    "\n",
    "print(lda_model[id2word_wiki.doc2bow(test_doc_1[0])])\n",
    "\n",
    "lda_model.print_topics()\n",
    "\n",
    "part1 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "#print(part1)\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_2 = [tokens for tokens in iter_wiki(test2_path)]\n",
    "part2 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "print(lda_model[id2word_wiki.doc2bow(test_doc_2[0])])\n",
    "\n",
    "# doc_stream = [tokens for tokens in iter_wiki(test1_path))  # generator\n",
    "test_doc_3 = [tokens for tokens in iter_wiki(test3_path)]\n",
    "part3 = [lda_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "print(lda_model[id2word_wiki.doc2bow(test_doc_3[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721322526286\n",
      "0.225673774229\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part1 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_1]\n",
    "\n",
    "part2 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_2]\n",
    "\n",
    "part3 = [lsi_model[id2word_wiki.doc2bow(tokens)] for tokens in test_doc_3]\n",
    "\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part2)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part1, part3)]))\n",
    "print(np.mean([gensim.matutils.cossim(p1, p2) for p1, p2 in zip(part2, part3)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = test1_path\n",
    "text2 = test2_path\n",
    "\n",
    "vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "print ('Cosine:', cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
